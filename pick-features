#!/usr/bin/python3

import sys
sys.path[:0] = '/home/dima/projects/GL_image_display',

r'''xxx'''

import sys
import argparse
import re
import os

def parse_args():

    def positive_float(string):
        try:
            value = float(string)
        except:
            raise argparse.ArgumentTypeError("argument MUST be a positive floating-point number. Got '{}'".format(string))
        if value <= 0:
            raise argparse.ArgumentTypeError("argument MUST be a positive floating-point number. Got '{}'".format(string))
        return value
    def positive_int(string):
        try:
            value = int(string)
        except:
            raise argparse.ArgumentTypeError("argument MUST be a positive integer. Got '{}'".format(string))
        if value <= 0 or abs(value-float(string)) > 1e-6:
            raise argparse.ArgumentTypeError("argument MUST be a positive integer. Got '{}'".format(string))
        return value


    parser = \
        argparse.ArgumentParser(description = __doc__,
                                formatter_class=argparse.RawDescriptionHelpFormatter)


    ######## geometry and rectification system parameters
    parser.add_argument('--az-fov-deg',
                        type=float,
                        help='''The field of view in the azimuth direction, in
                        degrees. There's no auto-detection at this time, so this
                        argument is required''')
    parser.add_argument('--el-fov-deg',
                        type=float,
                        help='''The field of view in the elevation direction, in
                        degrees. There's no auto-detection at this time, so this
                        argument is required''')
    parser.add_argument('--az0-deg',
                        default = None,
                        type=float,
                        help='''The azimuth center of the rectified images. "0"
                        means "the horizontal center of the rectified system is
                        the mean forward direction of the two cameras projected
                        to lie perpendicular to the baseline". If omitted, we
                        align the center of the rectified system with the center
                        of the two cameras' views''')
    parser.add_argument('--el0-deg',
                        default = 0,
                        type=float,
                        help='''The elevation center of the rectified system.
                        "0" means "the vertical center of the rectified system
                        lies along the mean forward direction of the two
                        cameras" Defaults to 0.''')
    parser.add_argument('--pixels-per-deg',
                        help='''The resolution of the rectified images. This is
                        either a whitespace-less, comma-separated list of two
                        values (az,el) or a single value to be applied to both
                        axes. If a resolution of >0 is requested, the value is
                        used as is. If a resolution of <0 is requested, we use
                        this as a scale factor on the resolution of the input
                        image. For instance, to downsample by a factor of 2,
                        pass -0.5. By default, we use -1 for both axes: the
                        resolution of the input image at the center of the
                        rectified system.''')
    parser.add_argument('--rectification',
                        choices=('LENSMODEL_PINHOLE', 'LENSMODEL_LATLON'),
                        default = 'LENSMODEL_LATLON',
                        help='''The lens model to use for rectification.
                        Currently two models are supported: LENSMODEL_LATLON
                        (the default) and LENSMODEL_PINHOLE. Pinhole stereo
                        works badly for wide lenses and suffers from varying
                        angular resolution across the image. LENSMODEL_LATLON
                        rectification uses a transverse equirectangular
                        projection, and does not suffer from these effects. It
                        is thus the recommended model''')

    ######## image pre-filtering
    parser.add_argument('--clahe',
                        action='store_true',
                        help='''If given, apply CLAHE equalization to the images
                        prior to the stereo matching''')

    ######## stereo processing
    parser.add_argument('--disparity-range',
                        type=int,
                        nargs=2,
                        default=(0,100),
                        help='''The disparity limits to use in the search, in
                        pixels. Two integers are expected: MIN_DISPARITY
                        MAX_DISPARITY. Completely arbitrarily, we default to
                        MIN_DISPARITY=0 and MAX_DISPARITY=100''')
    parser.add_argument('--valid-intrinsics-region',
                        action='store_true',
                        help='''If given, annotate the image with its
                        valid-intrinsics region. This will end up in the
                        rectified images, and make it clear where successful
                        matching shouldn't be expected''')

    parser.add_argument('--sgbm-block-size',
                        type=int,
                        default = 5,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, 5 is used''')
    parser.add_argument('--sgbm-p1',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-p2',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-disp12-max-diff',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-pre-filter-cap',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-uniqueness-ratio',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-speckle-window-size',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-speckle-range',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-mode',
                        choices=('SGBM','HH','HH4','SGBM_3WAY'),
                        help='''A parameter for the OpenCV SGBM matcher. Must be
                        one of ('SGBM','HH','HH4','SGBM_3WAY'). If omitted, the
                        OpenCV default (SGBM) is used''')

    parser.add_argument('--baseline-nominal',
                        type=float,
                        required=True,
                        help='''Baseline in my nominal stereo-pair geometry''')
    parser.add_argument('--range-nominal',
                        type=float,
                        default=10.,
                        help='''Initial guess for the range of all picked
                        points. Defaults to 10m''')
    parser.add_argument('--template-size',
                        type=positive_int,
                        nargs=2,
                        default = (13,13),
                        help='''The size of the template used for feature
                        matching, in pixel coordinates of the second image. Two
                        arguments are required: width height. This is passed
                        directly to mrcal.match_feature(). We default to
                        13x13''')
    parser.add_argument('--search-radius',
                        type=positive_int,
                        default = 20,
                        help='''How far the feature-matching routine should
                        search, in pixel coordinates of the second image. This
                        should be larger if the nominal range estimate is poor,
                        especially, at near ranges. This is passed directly to
                        mrcal.match_feature(). We default to 20 pixels''')

    parser.add_argument('models',
                        type=str,
                        nargs = 2,
                        help='''Camera models representing cameras used to
                        capture the images. Intrinsics only are used. A nominal
                        stereo geometry with --baseline-nominal is assumed''')
    parser.add_argument('images',
                        type=str,
                        nargs=2,
                        help='''The images to use for the matching''')

    args = parser.parse_args()


    if args.pixels_per_deg is None:
        args.pixels_per_deg = (-1, -1)
    else:
        try:
            l = [float(x) for x in args.pixels_per_deg.split(',')]
            if len(l) < 1 or len(l) > 2:
                raise
            for x in l:
                if x == 0:
                    raise
            args.pixels_per_deg = l
        except:
            print("""Argument-parsing error:
  --pixels_per_deg requires RESX,RESY or RESXY, where RES... is a value <0 or >0""",
                  file=sys.stderr)
            sys.exit(1)

    if (args.az_fov_deg is None or \
        args.el_fov_deg is None ):
        print("""Argument-parsing error:
  --az-fov-deg and --el-fov-deg are required""",
              file=sys.stderr)
        sys.exit(1)

    return args

args = parse_args()

# arg-parsing is done before the imports so that --help works without building
# stuff, so that I can generate the manpages and README






import numpy as np
import numpysane as nps
import cv2
import glob
import mrcal

# if args.sgbm_mode is not None:
#     if   args.sgbm_mode == 'SGBM':      args.sgbm_mode = cv2.StereoSGBM_MODE_SGBM
#     elif args.sgbm_mode == 'HH':        args.sgbm_mode = cv2.StereoSGBM_MODE_HH
#     elif args.sgbm_mode == 'HH4':       args.sgbm_mode = cv2.StereoSGBM_MODE_HH4
#     elif args.sgbm_mode == 'SGBM_3WAY': args.sgbm_mode = cv2.StereoSGBM_MODE_SGBM_3WAY
#     else:
#         raise Exception("arg-parsing error. This is a bug. Please report")

if len(args.pixels_per_deg) == 2:
    pixels_per_deg_az,pixels_per_deg_el = args.pixels_per_deg
else:
    pixels_per_deg_az = pixels_per_deg_el = args.pixels_per_deg[0]

models = [mrcal.cameramodel(m) for m in args.models]

models[0].extrinsics_rt_fromref(np.array((0,0,0,                      0,0,0), dtype=float))
models[1].extrinsics_rt_fromref(np.array((0,0,0, -args.baseline_nominal,0,0), dtype=float))


Rt01 = mrcal.compose_Rt( models[0].extrinsics_Rt_fromref(),
                         models[1].extrinsics_Rt_toref() )
Rt10 = mrcal.invert_Rt(Rt01)

models_rectified = \
    mrcal.rectified_system(models,
                           az_fov_deg          = args.az_fov_deg,
                           el_fov_deg          = args.el_fov_deg,
                           el0_deg             = args.el0_deg,
                           az0_deg             = args.az0_deg,
                           pixels_per_deg_az   = pixels_per_deg_az,
                           pixels_per_deg_el   = pixels_per_deg_el,
                           rectification_model = args.rectification)

print("maybe I don't need --baseline-nominal: the transformed images are the same")
rectification_maps = mrcal.rectification_maps(models, models_rectified)

if args.clahe:
    clahe = cv2.createCLAHE()
    clahe.setClipLimit(8)


images = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in args.images]
if images[0] is None:
    print(f"Couldn't read image '{images[0]}'", file=sys.stderr)
    sys.exit(1)
if images[1] is None:
    print(f"Couldn't read image '{images[1]}'", file=sys.stderr)
    sys.exit(1)


# This doesn't really matter: I don't use the input imagersize. But a
# mismatch suggests the user probably messed up, and it would save them time
# to yell at them
imagersize_image = np.array((images[0].shape[1], images[0].shape[0]))
imagersize_model = models[0].imagersize()
if np.any(imagersize_image - imagersize_model):
    raise Exception(f"Image '{args.images[0]}' dimensions {imagersize_image} don't match the model '{args.models[0]}' dimensions {imagersize_model}")
imagersize_image = np.array((images[1].shape[1], images[1].shape[0]))
imagersize_model = models[1].imagersize()
if np.any(imagersize_image - imagersize_model):
    raise Exception(f"Image '{args.images[1]}' dimensions {imagersize_image} don't match the model '{args.models[1]}' dimensions {imagersize_model}")

if args.clahe:
    images = [ clahe.apply(image) for image in images ]

if args.valid_intrinsics_region:
    for i in range(2):
        mrcal.annotate_image__valid_intrinsics_region(images[i], models[i])

images_rectified = [mrcal.transform_image(images[i],
                                          rectification_maps[i]) \
                    for i in range(2)]


# # Done with all the preliminaries. Run the stereo matching
# disp_min,disp_max = args.disparity_range

# # This is a hard-coded property of the OpenCV StereoSGBM implementation
# disparity_scale = 16

# # round to nearest multiple of disparity_scale. The OpenCV StereoSGBM
# # implementation requires this
# disp_max = disparity_scale*round(disp_max/disparity_scale)

# # I only add non-default args. StereoSGBM_create() doesn't like being given
# # None args
# kwargs = dict()
# if args.sgbm_p1 is not None:
#     kwargs['P1']                = args.sgbm_p1
# if args.sgbm_p2 is not None:
#     kwargs['P2']                = args.sgbm_p2
# if args.sgbm_disp12_max_diff is not None:
#     kwargs['disp12MaxDiff']     = args.sgbm_disp12_max_diff
# if args.sgbm_uniqueness_ratio is not None:
#     kwargs['uniquenessRatio']   = args.sgbm_uniqueness_ratio
# if args.sgbm_speckle_window_size is not None:
#     kwargs['speckleWindowSize'] = args.sgbm_speckle_window_size
# if args.sgbm_speckle_range is not None:
#     kwargs['speckleRange']      = args.sgbm_speckle_range
# if args.sgbm_mode is not None:
#     kwargs['mode']              = args.sgbm_mode
# stereo = \
#     cv2.StereoSGBM_create(minDisparity      = disp_min,
#                           numDisparities    = disp_max,
#                           # blocksize is required, so I always pass it.
#                           # There's a default set in the argument parser, no
#                           # this is never None
#                           blockSize         = args.sgbm_block_size,
#                           **kwargs)
# disparity = stereo.compute(*images_rectified)
# disparity_colored = mrcal.apply_color_map(disparity,
#                                           0, disp_max*disparity_scale)

UI_usage_message = r'''Usage:

Left mouse button click/drag: pan
Mouse wheel up/down/left/right: pan
Ctrl-mouse wheel up/down: zoom
'u': reset view: zoom out, pan to the center

Right mouse button click: examine stereo at pixel
TAB: transpose windows
'''

from fltk               import *
from Fl_Gl_Image_Widget import *

highlighted_point_radius = \
    dict(q01_hypothesis = 8,
         q01_stored     = 10,
         q01_selected   = 11)
highlighted_point_color = \
    dict(q01_hypothesis = np.array((1,0,0), dtype=np.float32),
         q01_stored     = np.array((0,1,0), dtype=np.float32),
         q01_selected   = np.array((0,1,0), dtype=np.float32))
# Each is an array of shape (...,Nimages=2,xy=2)
points_state = dict( q01_hypothesis = None,
                     q01_stored     = np.zeros((0,2,2), dtype=float),
                     q01_selected   = None)



def get_q01_nominal(q_rectified, is_from_image0):
    if is_from_image0:
        q0_rectified = q_rectified
        v0 = mrcal.unproject(q0_rectified, *models_rectified[0].intrinsics(),
                             normalize = True)
        p0 = v0 * args.range_nominal
        p1 = mrcal.transform_point_Rt(Rt10, p0)
        q1_rectified = mrcal.project(p1, *models_rectified[1].intrinsics())
    else:
        q1_rectified = q_rectified
        v1 = mrcal.unproject(q1_rectified, *models_rectified[1].intrinsics(),
                             normalize = True)
        p1 = v1 * args.range_nominal
        p0 = mrcal.transform_point_Rt(Rt01, p1)
        q0_rectified = mrcal.project(p0, *models_rectified[0].intrinsics())

    return nps.cat(q0_rectified, q1_rectified)


def set_all_overlay_lines():
    def set_overlay_lines_widget(# output
                                 lines,
                                 # input
                                 widget,
                                 q01, idx, radius, color):
        if q01 is not None and q01.size != 0:
            q = q01[..., idx,:]

            for r in (radius, 0.5):
                # q now has shape (..., 2)
                rx = np.array((r,0), dtype=np.float32)
                ry = np.array((0,r), dtype=np.float32)

                # shape (..., Nsegments_in_square=4, Npoints_in_line_segment=2, xy=2)
                p = np.zeros(q.shape[:-1] + (4,2,2), dtype=np.float32)
                p[..., 0,0,:] = q-rx-ry
                p[..., 0,1,:] = q-rx+ry
                p[..., 1,0,:] = q+rx-ry
                p[..., 1,1,:] = q+rx+ry
                p[..., 2,0,:] = q-rx-ry
                p[..., 2,1,:] = q+rx-ry
                p[..., 3,0,:] = q-rx+ry
                p[..., 3,1,:] = q+rx+ry
                # p has shape (4*..., 2,2)
                p = nps.clump(p, n=p.ndim-2)

                lines.append(dict(points = p,
                                  color_rgb = color ))

    lines = [[], []]

    for what in points_state.keys():
        set_overlay_lines_widget(lines[0],
                                 widget_image0,
                                 points_state[what],
                                 0,
                                 highlighted_point_radius[what],
                                 highlighted_point_color [what])
        set_overlay_lines_widget(lines[1],
                                 widget_image1,
                                 points_state[what],
                                 1,
                                 highlighted_point_radius[what],
                                 highlighted_point_color [what])

    widget_image0.set_lines(lines[0])
    widget_image1.set_lines(lines[1])


class Fl_Gl_Image_Widget_Derived(Fl_Gl_Image_Widget):

    def set_panzoom(self,
                    x_centerpixel, y_centerpixel,
                    visible_width_pixels):
        r'''Pan/zoom the image

        This is an override of the function to do this: any request to
        pan/zoom the widget will come here first. I dispatch any
        pan/zoom commands to all the widgets, so that they all zoom in
        unison. visible_width_pixels < 0 means: this is the redirected
        call. Just call the base class

        '''
        if visible_width_pixels < 0:
            return super().set_panzoom(x_centerpixel, y_centerpixel,
                                       -visible_width_pixels)
        if visible_width_pixels is np.nan:
            return super().set_panzoom(x_centerpixel, y_centerpixel,
                                       visible_width_pixels)

        # All the widgets should zoom together. I perform the requested
        # operation, and then ask all the widgets to zoom-only to this setting
        super().set_panzoom(x_centerpixel, y_centerpixel,
                            visible_width_pixels)
        return \
            all( w.set_panzoom(np.nan, np.nan,
                               -visible_width_pixels) \
                 for w in (widget_image0, widget_image1) )

    def handle(self, event):
        global points_state

        if event == FL_ENTER:
            return 1
        if event == FL_LEAVE:
            widget_status.value("")
            return 1
        if event == FL_MOVE:
            try:
                q = self.map_pixel_image_from_viewport( (Fl.event_x(),Fl.event_y()), )
                this = "Image 0" if self is widget_image0 else "Image 1"
                widget_status.value(f"{this}: {q[0]:.2f},{q[1]:.2f}")
            except:
                widget_status.value("")
            return 0

        if event == FL_PUSH:

            if Fl.event_button() != FL_RIGHT_MOUSE:
                return super().handle(event)

            if self is not widget_image0 and \
               self is not widget_image1:
                return super().handle(event)

            try:
                q_rectified = \
                    np.array( self.map_pixel_image_from_viewport( (Fl.event_x(),Fl.event_y()), ),
                              dtype=float )
            except:
                widget_info.value(UI_usage_message + "\n" + \
                                    "Error converting pixel coordinates")
                points_state['q01_hypothesis'] = None
                set_all_overlay_lines()
                return super().handle(event)

            if self is widget_image0: W,H = images_rectified[0].shape[:2]
            else:                     W,H = images_rectified[1].shape[:2]


            if not (q_rectified[0] >= -0.5 and q_rectified[0] <= W-0.5 and \
                    q_rectified[1] >= -0.5 and q_rectified[1] <= H-0.5):
                widget_info.value(UI_usage_message + "\n" + \
                                    "Out of bounds")
                points_state['q01_hypothesis'] = None
                set_all_overlay_lines()
                return super().handle(event)

            q01 = None
            if self is widget_image0:
                # shape (2,2): (leftright, qxy)
                points_state['q01_hypothesis'] = get_q01_nominal(q_rectified, True)

                match_feature_out = \
                    mrcal.match_feature(images_rectified[0], images_rectified[1],
                                        q0               = points_state['q01_hypothesis'][0],
                                        q1_estimate      = points_state['q01_hypothesis'][1],
                                        search_radius1   = args.search_radius,
                                        template_size1   = args.template_size)

                q1, match_feature_diagnostics = match_feature_out[:2]

                # report match_feature_diagnostics['matchoutput_optimum_subpixel']
                if q1 is not None:
                    q01 = nps.cat(points_state['q01_hypothesis'][0],
                                  q1)

            else:
                points_state['q01_hypothesis'] = get_q01_nominal(q_rectified, False)

                match_feature_out = \
                    mrcal.match_feature(images_rectified[1], images_rectified[0],
                                        q0               = points_state['q01_hypothesis'][1],
                                        q1_estimate      = points_state['q01_hypothesis'][0],
                                        search_radius1   = args.search_radius,
                                        template_size1   = args.template_size)

                q1, match_feature_diagnostics = match_feature_out[:2]

                # report match_feature_diagnostics['matchoutput_optimum_subpixel']
                if q1 is not None:
                    q01 = nps.cat(q1,
                                  points_state['q01_hypothesis'][1])

            if q01 is not None:
                # adding a new row
                points_state['q01_stored'] = \
                    nps.glue( points_state['q01_stored'],
                              q01,
                              axis=-3)
                Nrows = points_state['q01_stored'].shape[0]
                widget_table.rows( Nrows )
                widget_table.select_row(Nrows-1,
                                        event_from_widget_image0 = self is widget_image0)
                self.redraw()

            set_all_overlay_lines()

            return 0

        if event == FL_KEYDOWN:
            if Fl.event_key() == fltk.FL_Delete:
                # delete selected feature
                Nrows = points_state['q01_stored'].shape[0]
                def selected_row():
                    # This is crazy. Is this really right?
                    for i in range(Nrows):
                        if widget_table.row_selected(i):
                            return i
                    return None
                iselected = selected_row()
                if iselected is not None:
                    widget_table.select_row(iselected,
                                            flag = 0)
                    points_state['q01_stored'] = \
                        nps.glue( points_state['q01_stored'][:iselected],
                                  points_state['q01_stored'][iselected+1:],
                                  axis = -3 )
                    widget_table.rows( Nrows-1 )
                    set_all_overlay_lines()
                    self.redraw()

                return 1

        return super().handle(event)


class Fl_Table_Derived(Fl_Table_Row):

    def __init__(self, x, y, w, h, *args):
        Fl_Table_Row.__init__(self, x, y, w, h, *args)

        self.col_labels = \
            [ "x0",
              "y0",
              "x1",
              "y1",
              "Nominal range",
              "Correlation",
              "Fit error" ]

        self.type(fltk.Fl_Table_Row.SELECT_SINGLE)
        self.rows(0)
        self.cols(len(self.col_labels))
        self.col_header(1)
        self.col_resize(1)

        self.end()

    def draw_cell(self, context, row, col, x, y, w, h):

        if context == self.CONTEXT_STARTPAGE:
            fl_font(FL_HELVETICA, 12)
            return

        if context == self.CONTEXT_COL_HEADER:
            text = self.col_labels[col]

            fl_push_clip(x, y, w, h)
            fl_draw_box(FL_THIN_UP_BOX, x, y, w, h, self.row_header_color())
            fl_color(FL_BLACK)
            fl_draw(text, x, y, w, h, FL_ALIGN_CENTER)
            fl_pop_clip()
            return

        if context == self.CONTEXT_CELL:
            if col < 4:
                iimage = col // 2
                ixy    = col %  2
                text = f"{points_state['q01_stored'][row,iimage,ixy]:.2f}"
            else:
                text = '-'

            fl_push_clip(x, y, w, h)
            # background color
            fl_color(self.selection_color() if self.row_selected(row) else FL_WHITE)
            fl_rectf(x, y, w, h)

            # text
            fl_color(FL_BLACK)
            fl_draw(text, x, y, w, h, FL_ALIGN_CENTER)

            # border
            fl_color(FL_LIGHT2)
            fl_rect(x, y, w, h)
            fl_pop_clip()

            return

        return

    def select_row(self, row, flag=1, event_from_widget_image0 = None):
        ret = super().select_row(row,flag)

        if self.row_selected(row):
            points_state['q01_selected'] = points_state['q01_stored'][-1,...]

            # I pan to show the match. Don't pan the window I just clicked on
            if event_from_widget_image0 is False:
                widget_image0.set_panzoom(*points_state['q01_selected'][0,...], np.nan)
            if event_from_widget_image0 is True:
                widget_image1.set_panzoom(*points_state['q01_selected'][1,...], np.nan)
        else:
            points_state['q01_selected'] = None

        return ret





window = Fl_Window(800, 620, "mrcal feature picker")
body   = Fl_Group(0,0,800,600)

widget_image0 = Fl_Gl_Image_Widget_Derived(0,    0, 400,300)
widget_image1 = Fl_Gl_Image_Widget_Derived(400,  0, 400,300)
widget_table  = Fl_Table_Derived(          0,  300, 400,300)
widget_info   = Fl_Multiline_Output(       400,300, 400,300)
body.end()

widget_status = Fl_Output(0,600,800,20)

widget_info.value(UI_usage_message)

window.resizable(body)
window.end()
window.show()

widget_image0. \
  update_image(decimation_level = 0,
               image_data       = images_rectified[0])
widget_image1. \
  update_image(decimation_level = 0,
               image_data       = images_rectified[1])

Fl.run()

sys.exit(0)





r'''

show two images

click one image

- draw corresponding pixel in the other image
- search, draw matched pixel
- User can
  - accept match
  - reject match
  - click to init search from another guess
  -

'''
