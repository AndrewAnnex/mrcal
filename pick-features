#!/usr/bin/python3

import sys
sys.path[:0] = '/home/dima/projects/GL_image_display',

r'''xxx'''

import sys
import argparse
import re
import os

def parse_args():

    def positive_float(string):
        try:
            value = float(string)
        except:
            raise argparse.ArgumentTypeError("argument MUST be a positive floating-point number. Got '{}'".format(string))
        if value <= 0:
            raise argparse.ArgumentTypeError("argument MUST be a positive floating-point number. Got '{}'".format(string))
        return value
    def positive_int(string):
        try:
            value = int(string)
        except:
            raise argparse.ArgumentTypeError("argument MUST be a positive integer. Got '{}'".format(string))
        if value <= 0 or abs(value-float(string)) > 1e-6:
            raise argparse.ArgumentTypeError("argument MUST be a positive integer. Got '{}'".format(string))
        return value


    parser = \
        argparse.ArgumentParser(description = __doc__,
                                formatter_class=argparse.RawDescriptionHelpFormatter)


    ######## geometry and rectification system parameters
    parser.add_argument('--az-fov-deg',
                        type=float,
                        help='''The field of view in the azimuth direction, in
                        degrees. There's no auto-detection at this time, so this
                        argument is required''')
    parser.add_argument('--el-fov-deg',
                        type=float,
                        help='''The field of view in the elevation direction, in
                        degrees. There's no auto-detection at this time, so this
                        argument is required''')
    parser.add_argument('--az0-deg',
                        default = None,
                        type=float,
                        help='''The azimuth center of the rectified images. "0"
                        means "the horizontal center of the rectified system is
                        the mean forward direction of the two cameras projected
                        to lie perpendicular to the baseline". If omitted, we
                        align the center of the rectified system with the center
                        of the two cameras' views''')
    parser.add_argument('--el0-deg',
                        default = 0,
                        type=float,
                        help='''The elevation center of the rectified system.
                        "0" means "the vertical center of the rectified system
                        lies along the mean forward direction of the two
                        cameras" Defaults to 0.''')
    parser.add_argument('--pixels-per-deg',
                        help='''The resolution of the rectified images. This is
                        either a whitespace-less, comma-separated list of two
                        values (az,el) or a single value to be applied to both
                        axes. If a resolution of >0 is requested, the value is
                        used as is. If a resolution of <0 is requested, we use
                        this as a scale factor on the resolution of the input
                        image. For instance, to downsample by a factor of 2,
                        pass -0.5. By default, we use -1 for both axes: the
                        resolution of the input image at the center of the
                        rectified system.''')
    parser.add_argument('--rectification',
                        choices=('LENSMODEL_PINHOLE', 'LENSMODEL_LATLON'),
                        default = 'LENSMODEL_LATLON',
                        help='''The lens model to use for rectification.
                        Currently two models are supported: LENSMODEL_LATLON
                        (the default) and LENSMODEL_PINHOLE. Pinhole stereo
                        works badly for wide lenses and suffers from varying
                        angular resolution across the image. LENSMODEL_LATLON
                        rectification uses a transverse equirectangular
                        projection, and does not suffer from these effects. It
                        is thus the recommended model''')

    ######## image pre-filtering
    parser.add_argument('--clahe',
                        action='store_true',
                        help='''If given, apply CLAHE equalization to the images
                        prior to the stereo matching''')

    ######## stereo processing
    parser.add_argument('--disparity-range',
                        type=int,
                        nargs=2,
                        default=(0,100),
                        help='''The disparity limits to use in the search, in
                        pixels. Two integers are expected: MIN_DISPARITY
                        MAX_DISPARITY. Completely arbitrarily, we default to
                        MIN_DISPARITY=0 and MAX_DISPARITY=100''')
    parser.add_argument('--valid-intrinsics-region',
                        action='store_true',
                        help='''If given, annotate the image with its
                        valid-intrinsics region. This will end up in the
                        rectified images, and make it clear where successful
                        matching shouldn't be expected''')

    parser.add_argument('--sgbm-block-size',
                        type=int,
                        default = 5,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, 5 is used''')
    parser.add_argument('--sgbm-p1',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-p2',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-disp12-max-diff',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-pre-filter-cap',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-uniqueness-ratio',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-speckle-window-size',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-speckle-range',
                        type=int,
                        help='''A parameter for the OpenCV SGBM matcher. If
                        omitted, the OpenCV default is used''')
    parser.add_argument('--sgbm-mode',
                        choices=('SGBM','HH','HH4','SGBM_3WAY'),
                        help='''A parameter for the OpenCV SGBM matcher. Must be
                        one of ('SGBM','HH','HH4','SGBM_3WAY'). If omitted, the
                        OpenCV default (SGBM) is used''')

    parser.add_argument('--baseline-nominal',
                        type=float,
                        required=True,
                        help='''Baseline in my nominal stereo-pair geometry''')
    parser.add_argument('--range-nominal',
                        type=float,
                        default=10.,
                        help='''Initial guess for the range of all picked
                        points. Defaults to 10m''')
    parser.add_argument('--template-size',
                        type=positive_int,
                        nargs=2,
                        default = (13,13),
                        help='''The size of the template used for feature
                        matching, in pixel coordinates of the second image. Two
                        arguments are required: width height. This is passed
                        directly to mrcal.match_feature(). We default to
                        13x13''')
    parser.add_argument('--search-radius',
                        type=positive_int,
                        default = 20,
                        help='''How far the feature-matching routine should
                        search, in pixel coordinates of the second image. This
                        should be larger if the nominal range estimate is poor,
                        especially, at near ranges. This is passed directly to
                        mrcal.match_feature(). We default to 20 pixels''')

    parser.add_argument('models',
                        type=str,
                        nargs = 2,
                        help='''Camera models representing cameras used to
                        capture the images. Intrinsics only are used. A nominal
                        stereo geometry with --baseline-nominal is assumed''')
    parser.add_argument('images',
                        type=str,
                        nargs=2,
                        help='''The images to use for the matching''')

    args = parser.parse_args()


    if args.pixels_per_deg is None:
        args.pixels_per_deg = (-1, -1)
    else:
        try:
            l = [float(x) for x in args.pixels_per_deg.split(',')]
            if len(l) < 1 or len(l) > 2:
                raise
            for x in l:
                if x == 0:
                    raise
            args.pixels_per_deg = l
        except:
            print("""Argument-parsing error:
  --pixels_per_deg requires RESX,RESY or RESXY, where RES... is a value <0 or >0""",
                  file=sys.stderr)
            sys.exit(1)

    if (args.az_fov_deg is None or \
        args.el_fov_deg is None ):
        print("""Argument-parsing error:
  --az-fov-deg and --el-fov-deg are required""",
              file=sys.stderr)
        sys.exit(1)

    return args

args = parse_args()

# arg-parsing is done before the imports so that --help works without building
# stuff, so that I can generate the manpages and README






import numpy as np
import numpysane as nps
import cv2
import glob
import mrcal

# if args.sgbm_mode is not None:
#     if   args.sgbm_mode == 'SGBM':      args.sgbm_mode = cv2.StereoSGBM_MODE_SGBM
#     elif args.sgbm_mode == 'HH':        args.sgbm_mode = cv2.StereoSGBM_MODE_HH
#     elif args.sgbm_mode == 'HH4':       args.sgbm_mode = cv2.StereoSGBM_MODE_HH4
#     elif args.sgbm_mode == 'SGBM_3WAY': args.sgbm_mode = cv2.StereoSGBM_MODE_SGBM_3WAY
#     else:
#         raise Exception("arg-parsing error. This is a bug. Please report")

if len(args.pixels_per_deg) == 2:
    pixels_per_deg_az,pixels_per_deg_el = args.pixels_per_deg
else:
    pixels_per_deg_az = pixels_per_deg_el = args.pixels_per_deg[0]

models = [mrcal.cameramodel(m) for m in args.models]

models[0].extrinsics_rt_fromref(np.array((0,0,0,                      0,0,0), dtype=float))
models[1].extrinsics_rt_fromref(np.array((0,0,0, -args.baseline_nominal,0,0), dtype=float))


Rt01 = mrcal.compose_Rt( models[0].extrinsics_Rt_fromref(),
                         models[1].extrinsics_Rt_toref() )
Rt10 = mrcal.invert_Rt(Rt01)

models_rectified = \
    mrcal.rectified_system(models,
                           az_fov_deg          = args.az_fov_deg,
                           el_fov_deg          = args.el_fov_deg,
                           el0_deg             = args.el0_deg,
                           az0_deg             = args.az0_deg,
                           pixels_per_deg_az   = pixels_per_deg_az,
                           pixels_per_deg_el   = pixels_per_deg_el,
                           rectification_model = args.rectification)

print("maybe I don't need --baseline-nominal: the transformed images are the same")
rectification_maps = mrcal.rectification_maps(models, models_rectified)

if args.clahe:
    clahe = cv2.createCLAHE()
    clahe.setClipLimit(8)


images = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in args.images]
if images[0] is None:
    print(f"Couldn't read image '{images[0]}'", file=sys.stderr)
    sys.exit(1)
if images[1] is None:
    print(f"Couldn't read image '{images[1]}'", file=sys.stderr)
    sys.exit(1)


# This doesn't really matter: I don't use the input imagersize. But a
# mismatch suggests the user probably messed up, and it would save them time
# to yell at them
imagersize_image = np.array((images[0].shape[1], images[0].shape[0]))
imagersize_model = models[0].imagersize()
if np.any(imagersize_image - imagersize_model):
    raise Exception(f"Image '{args.images[0]}' dimensions {imagersize_image} don't match the model '{args.models[0]}' dimensions {imagersize_model}")
imagersize_image = np.array((images[1].shape[1], images[1].shape[0]))
imagersize_model = models[1].imagersize()
if np.any(imagersize_image - imagersize_model):
    raise Exception(f"Image '{args.images[1]}' dimensions {imagersize_image} don't match the model '{args.models[1]}' dimensions {imagersize_model}")

if args.clahe:
    images = [ clahe.apply(image) for image in images ]

if args.valid_intrinsics_region:
    for i in range(2):
        mrcal.annotate_image__valid_intrinsics_region(images[i], models[i])

images_rectified = [mrcal.transform_image(images[i],
                                          rectification_maps[i]) \
                    for i in range(2)]


# # Done with all the preliminaries. Run the stereo matching
# disp_min,disp_max = args.disparity_range

# # This is a hard-coded property of the OpenCV StereoSGBM implementation
# disparity_scale = 16

# # round to nearest multiple of disparity_scale. The OpenCV StereoSGBM
# # implementation requires this
# disp_max = disparity_scale*round(disp_max/disparity_scale)

# # I only add non-default args. StereoSGBM_create() doesn't like being given
# # None args
# kwargs = dict()
# if args.sgbm_p1 is not None:
#     kwargs['P1']                = args.sgbm_p1
# if args.sgbm_p2 is not None:
#     kwargs['P2']                = args.sgbm_p2
# if args.sgbm_disp12_max_diff is not None:
#     kwargs['disp12MaxDiff']     = args.sgbm_disp12_max_diff
# if args.sgbm_uniqueness_ratio is not None:
#     kwargs['uniquenessRatio']   = args.sgbm_uniqueness_ratio
# if args.sgbm_speckle_window_size is not None:
#     kwargs['speckleWindowSize'] = args.sgbm_speckle_window_size
# if args.sgbm_speckle_range is not None:
#     kwargs['speckleRange']      = args.sgbm_speckle_range
# if args.sgbm_mode is not None:
#     kwargs['mode']              = args.sgbm_mode
# stereo = \
#     cv2.StereoSGBM_create(minDisparity      = disp_min,
#                           numDisparities    = disp_max,
#                           # blocksize is required, so I always pass it.
#                           # There's a default set in the argument parser, no
#                           # this is never None
#                           blockSize         = args.sgbm_block_size,
#                           **kwargs)
# disparity = stereo.compute(*images_rectified)
# disparity_colored = mrcal.apply_color_map(disparity,
#                                           0, disp_max*disparity_scale)

UI_usage_message = r'''Usage:

Left mouse button click/drag: pan
Mouse wheel up/down/left/right: pan
Ctrl-mouse wheel up/down: zoom
'u': reset view: zoom out, pan to the center

Right mouse button click: examine stereo at pixel
TAB: transpose windows
'''

from fltk               import *
from Fl_Gl_Image_Widget import *

highlighted_point_radius = \
    dict(q01_rectified_hypothesis = 10,
         q01_rectified_match      = 8)
highlighted_point_color = \
    dict(q01_rectified_hypothesis = np.array((1,0,0), dtype=np.float32),
         q01_rectified_match      = np.array((0,1,0), dtype=np.float32))
# shape (2,2): cat(q0,q1)
points_to_highlight = dict( q01_rectified_hypothesis = None,
                            q01_rectified_match      = None)



def get_q01_nominal(q_rectified, is_from_image0):
    if is_from_image0:
        q0_rectified = q_rectified
        v0 = mrcal.unproject(q0_rectified, *models_rectified[0].intrinsics(),
                             normalize = True)
        p0 = v0 * args.range_nominal
        p1 = mrcal.transform_point_Rt(Rt10, p0)
        q1_rectified = mrcal.project(p1, *models_rectified[1].intrinsics())
    else:
        q1_rectified = q_rectified
        v1 = mrcal.unproject(q1_rectified, *models_rectified[1].intrinsics(),
                             normalize = True)
        p1 = v1 * args.range_nominal
        p0 = mrcal.transform_point_Rt(Rt01, p1)
        q0_rectified = mrcal.project(p0, *models_rectified[0].intrinsics())

    return nps.cat(q0_rectified, q1_rectified)


def set_all_overlay_lines():
    def set_overlay_lines_widget(# output
                                 lines,
                                 # input
                                 widget,
                                 q01, idx, radius, color):
        if q01 is not None and q01.size != 0:
            q = q01[..., idx,:]

            # q now has shape (..., 2)
            rx = np.array((radius,0), dtype=np.float32)
            ry = np.array((0,radius), dtype=np.float32)

            # shape (..., Nsegments_in_square=4, Npoints_in_line_segment=2, xy=2)
            p = np.zeros(q.shape[:-1] + (4,2,2), dtype=np.float32)
            p[..., 0,0,:] = q-rx-ry
            p[..., 0,1,:] = q-rx+ry
            p[..., 1,0,:] = q+rx-ry
            p[..., 1,1,:] = q+rx+ry
            p[..., 2,0,:] = q-rx-ry
            p[..., 2,1,:] = q+rx-ry
            p[..., 3,0,:] = q-rx+ry
            p[..., 3,1,:] = q+rx+ry
            # p has shape (4*..., 2,2)
            p = nps.clump(p, n=p.ndim-2)

            lines.append(dict(points = p,
                              color_rgb = color ))

    lines = [[], []]

    for what in points_to_highlight.keys():
        set_overlay_lines_widget(lines[0],
                                 widget_image0,
                                 points_to_highlight[what],
                                 0,
                                 highlighted_point_radius[what],
                                 highlighted_point_color [what])
        set_overlay_lines_widget(lines[1],
                                 widget_image1,
                                 points_to_highlight[what],
                                 1,
                                 highlighted_point_radius[what],
                                 highlighted_point_color [what])
    widget_image0.set_lines(lines[0])
    widget_image1.set_lines(lines[1])


class Fl_Gl_Image_Widget_Derived(Fl_Gl_Image_Widget):

    def set_panzoom(self,
                    x_centerpixel, y_centerpixel,
                    visible_width_pixels):
        r'''Pan/zoom the image

        This is an override of the function to do this: any request to
        pan/zoom the widget will come here first. I dispatch any
        pan/zoom commands to all the widgets, so that they all work in
        unison. visible_width_pixels < 0 means: this is the redirected
        call. Just call the base class

        '''
        if visible_width_pixels < 0:
            return super().set_panzoom(x_centerpixel, y_centerpixel,
                                       -visible_width_pixels)

        # All the widgets should pan/zoom together
        return \
            all( w.set_panzoom(x_centerpixel, y_centerpixel,
                               -visible_width_pixels) \
                 for w in (widget_image0, widget_image1) )

    def handle(self, event):
        global points_to_highlight

        if event == FL_PUSH:

            if Fl.event_button() != FL_RIGHT_MOUSE:
                return super().handle(event)

            if self is not widget_image0 and \
               self is not widget_image1:
                return super().handle(event)

            try:
                q_rectified = \
                    np.array( self.map_pixel_image_from_viewport( (Fl.event_x(),Fl.event_y()), ),
                              dtype=float )
            except:
                widget_status.value(UI_usage_message + "\n" + \
                                    "Error converting pixel coordinates")
                points_to_highlight['q01_rectified_hypothesis'] = None
                set_all_overlay_lines()
                return super().handle(event)

            if self is widget_image0: W,H = images_rectified[0].shape[:2]
            else:                     W,H = images_rectified[1].shape[:2]


            if not (q_rectified[0] >= -0.5 and q_rectified[0] <= W-0.5 and \
                    q_rectified[1] >= -0.5 and q_rectified[1] <= H-0.5):
                widget_status.value(UI_usage_message + "\n" + \
                                    "Out of bounds")
                points_to_highlight['q01_rectified_hypothesis'] = None
                set_all_overlay_lines()
                return super().handle(event)

            if self is widget_image0:
                # shape (2,2): (leftright, qxy)
                points_to_highlight['q01_rectified_hypothesis'] = get_q01_nominal(q_rectified, True)

                match_feature_out = \
                    mrcal.match_feature(images_rectified[0], images_rectified[1],
                                        q0               = points_to_highlight['q01_rectified_hypothesis'][0],
                                        q1_estimate      = points_to_highlight['q01_rectified_hypothesis'][1],
                                        search_radius1   = args.search_radius,
                                        template_size1   = args.template_size)

                q1, match_feature_diagnostics = match_feature_out[:2]

                # report match_feature_diagnostics['matchoutput_optimum_subpixel']
                if q1 is not None:
                    points_to_highlight['q01_rectified_match'] = \
                        nps.cat(points_to_highlight['q01_rectified_hypothesis'][0],
                                q1)
                else:
                    points_to_highlight['q01_rectified_match'] = None

            else:
                points_to_highlight['q01_rectified_hypothesis'] = get_q01_nominal(q_rectified, False)

                match_feature_out = \
                    mrcal.match_feature(images_rectified[1], images_rectified[0],
                                        q0               = points_to_highlight['q01_rectified_hypothesis'][1],
                                        q1_estimate      = points_to_highlight['q01_rectified_hypothesis'][0],
                                        search_radius1   = args.search_radius,
                                        template_size1   = args.template_size)

                q1, match_feature_diagnostics = match_feature_out[:2]

                # report match_feature_diagnostics['matchoutput_optimum_subpixel']
                if q1 is not None:
                    points_to_highlight['q01_rectified_match'] = \
                        nps.cat(q1,
                                points_to_highlight['q01_rectified_hypothesis'][1])
                else:
                    points_to_highlight['q01_rectified_match'] = None

            set_all_overlay_lines()

            return 0

        if event == FL_KEYDOWN:
            if Fl.event_key() == fltk.FL_Tab:
                pass
            pass
        return super().handle(event)

window           = Fl_Window(800, 600, "mrcal feature picker")
widget_image0    = Fl_Gl_Image_Widget_Derived(0,    0, 400,300)
widget_image1    = Fl_Gl_Image_Widget_Derived(400,  0, 400,300)
widget_status    = Fl_Multiline_Output(       400,300, 400,300)

widget_status.value(UI_usage_message)

window.resizable(window)
window.end()
window.show()

import numpy as np
widget_image0. \
  update_image(decimation_level = 0,
               image_data       = images_rectified[0])
widget_image1. \
  update_image(decimation_level = 0,
               image_data       = images_rectified[1])
Fl.run()

sys.exit(0)





r'''

show two images

click one image

- draw corresponding pixel in the other image
- search, draw matched pixel
- User can
  - accept match
  - reject match
  - click to init search from another guess
  - 

'''
