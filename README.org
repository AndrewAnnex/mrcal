* SYNOPSIS

#+BEGIN_EXAMPLE
$ mrcal-calibrate-cameras --focal 2000
                    --outdir /tmp --object-spacing 0.01
                    --object-width-n 10 '/tmp/left*.png' '/tmp/right*.png'

... lots of output as the solve runs ...
Wrote /tmp/camera0-0.cameramodel
Wrote /tmp/camera0-1.cameramodel
#+END_EXAMPLE

* SUMMARY

Mrcal is a generic toolkit to solve calibration and SFM-like problems.
Functionality related to these problems is exposed as a set of python libraries.
Both CAHVOR and OpenCV distortion models are fully supported; CAHVORE is
partially supported. The toolkit includes:

- Some libraries:
  - =libmrcal=: A flexible solver core written in C and providing a C API that
    solves the underlying optimization problem
  - =mrcal=: a Python library that contains (among other things) an interface
    to this core. Other things provided by the =mrcal= Python library:
    - functions to read/write/manipulate camera models
    - functions to manipulate 3D poses
    - functions to (un)project and (un)distort data

- Some tools:
  - =mrcal-calibrate-cameras=: calibrates N cameras
  - =mrcal-convert-distortion=: fits one distortion model to another
  - =mrcal-show-distortion=: visualize the distortion effects of a specific
    model
  - =mrcal-show-intrinsics-uncertainty=: visualize the uncertainty of intrinsics
    due to noise in the calibration inputs
  - =mrcal-show-intrinsics-diff=: visualize the difference between the
    intrinsics of a number of models
  - =mrcal-undistort-image=: Given image(s) and a distortion model, produces a
    new set of images that have removed the distortion
  - =mrcal-redistort-points=: Given two distortion models and a set of points,
    maps them from one distortion model to the other
  - =mrcal-graft-cameramodel=: Combines the intrinsics of one cameramodel with
    the extrinsics of another

These libraries and tools make it easy to both produce calibrations in many ways
and to manipulate them by moving stuff around, grafting various
intrinsics/extrinsics, etc.

* DESCRIPTION

** Fundamental assumptions

Some notational conventions are used throught the code and implementation, and
they're explicitly described here.

*** Coordinate system conventions

No convention is assumed for the world coordinate system. The canonical camera
coordinate system has =x,y= as with pixel coordinates in an image: =x= is to the
"right" and =y= is "down". =z= is then "forward" to complete the right-handed
system of coordinates

*** Transformation conventions

When describing a transformation (or its rotation/translation components) that
maps a point represented in coordinate system A to its representation in
coordinate system B, =T_AB= is the notation used. These chain together nicely,
so if we know the transformation between =A= and =B= and between =B= and =C=, we
can transform a point represented in =C= to =A=: =x_A = T_AB T_BC x_C=

*** Pose representations

Various parts of the toolkit have preferred representations of pose, and =mrcal=
has functions to convert between them. Available representations are:

- =Rt=: a (4,3) numpy array with a (3,3) rotation matrix concatenated with a
  (1,3) translation vector

- =rt=: a (6,) numpy array with a (3,) vector representing a Rodrigues rotation
  concatenated with another (3,) vector, representing a rotation.

Each of these represents a transformation =rotate(x) + t=

A Rodrigues rotation vector =r= represents a rotation of =length(r)= radians
around an axis in the direction =r=. Converting between =R= and =r= is done via
the [[https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula][Rodrigues rotation formula]]. This is implemented in OpenCV Python function
=cv2.Rodrigues=. For translating /poses/, not just rotations, use
=mrcal.Rt_from_rt()= and =mrcal.rt_from_Rt()=.

** Camera model file formats

Reading/writing cameramodels is done in Python with the =mrcal.cameramodel=
class. This class supports two different file formats:

- =.cameramodel=: the preferred format. This is a simple text representation
  that has clear sections for the distortion model, pinhole intrinsics,
  distortion coefficients, and an extrinsic pose. The pose is represented as
  =rt_fromref=: an =rt= transformation /from/ the reference coordinate system
  /to/ the coordinate system of this camera. The class provides methods to get
  the transformation in any form, but =rt_fromref= is the internal
  representation

- =.cahvor=: the legacy format. This exists for compatibility with existing JPL
  tools. There's no other reason to use this format

** Distortion models

Distortion models are specified as elements of =enum distortion_model_t= (in C)
or, as strings that match the entries of that enum (in Python). Currently I
support all CAHVOR flavors and all models implemented in OpenCV. A limitation is
that the solver core does not support CAHVORE, so use the OpenCV models if a
high-distoriton model is required. Currently the supported models are:

- =DISTORTION_NONE=
- =DISTORTION_OPENCV4=
- =DISTORTION_OPENCV5=
- =DISTORTION_OPENCV8=
- =DISTORTION_OPENCV12= (if we have OpenCV >= 3.0.0)
- =DISTORTION_OPENCV14= (if we have OpenCV >= 3.1.0)
- =DISTORTION_CAHVOR=
- =DISTORTION_CAHVORE=

** Calibration object

When running a camera calibration, we use camera observations of a calibration
object (usually a chessboard). These images must be converted to a set of pixels
where chessboard corners were observed. =mrcal= is a purely geometrical toolkit,
so this vision problem is handled by another library: [[https://github.com/dkogan/mrgingham/][=mrgingham=]]. See its
documentation for more details.

* MANPAGES
** mrcal-calibrate-cameras
#+BEGIN_EXAMPLE
NAME
    mrcal-calibrate-cameras - Calibrate some synchronized cameras

SYNOPSIS
      $ mrcal-calibrate-cameras
          --corners-cache corners.vnl
          --focal 1700 --object-spacing 0.01 --object-width-n 10
          --out /tmp
          --distortion-model DISTORTION_OPENCV8
          --observed-pixel-uncertainty 0.5
          'left*.png' 'right*.png'

        ... lots of output as the solve runs ...
        Done!
        RMS reprojection error: 1.9 pixels
        Worst reprojection error: 7.8 pixels
        Noutliers: 319 out of 17100 total points: 1.9% of the data

        Wrote /tmp/camera0-0.cahvor
        Wrote /tmp/camera0-1.cahvor
        Wrote /tmp/camera0-0.cameramodel
        Wrote /tmp/camera0-1.cameramodel

DESCRIPTION
    This tool uses the generic mrcal platform to solve a common specific
    problem of N-camera calibration using observations of a chessboard.

    TUTORIAL

    If all you want to do is run a calibration, read this section first.

    You need to get observations of a grid of points. This tool doesn't
    dictate exactly how these observations are obtained, but the recommended
    way to do that is to use mrgingham (http://github.com/dkogan/mrgingham).
    This documentation assumes that's what is being done.

    See the mrgingham documentation for a .pdf of a chessboard pattern. This
    pattern should be printed (at some size; see below) and mounted onto a
    RIGID and FLAT surface to produce the calibration object. The most
    useful observations are close-ups: views that cover as much of the
    imager as possible. Thus you generally a large printout of the
    chessboard pattern. If you're calibrating a wide lens then this is
    especially true: the wider the lens, the larger an object needs to be in
    order to cover the field of view.

    Now that we have a calibration object, this object needs to be shown to
    the camera(s) to produce the images that mrgingham will use to find the
    corner coordinates, which mrcal will then use in its computations.

    It is important that the images contain clear corners. If the image is
    badly overexposed, the white chessboard squares will bleed into each
    other, the adjoining black squares will no longer touch each other in
    the image, and there would be no corner to detect. Conversely, if the
    image is badly underexposed, the black squares will bleed into each
    other, which would also destroy the corner. mrgingham tries to handle a
    variety of lighting conditions, including varying illuination across the
    image, but the corners must exist in the image in some form. A
    fundamental design decision in mrgingham is to only output chessboards
    that we are very confident in, and a consequence of this is that
    mrgingham requires the WHOLE chessboard to be visible in order to
    produce any results. Thus it requires a bit of effort to produce any
    data at the edges and in the corners of the imager: if even a small
    number of the chessboard corners are out of bounds, mrgingham will not
    detect the chessboard at all. A live preview of the calibration images
    being gathered is thus essential to aid the user in obtaining good data.
    Another requirement due to the design of mrgingham is that the board
    should be held with a flat edge parallel to the camera xz plane
    (parallel to the ground, usually). mrgingham looks for vertical and
    horizontal sequences of corners, but if the board is rotated in this
    way, then none of these sequences are "horizontal" or "vertical", but
    they're all "diagonal", which isn't what mrgingham is looking for.

    The most useful observations to gather are

    - close-ups: the chessboard should fill the whole frame as much as
    possible

    - oblique views: tilt the board forward/back and left/right. I generally
    tilt by more than 45 degrees. At a certain point the corners become
    indistinct and mrgingham starts having trouble, but depending on the
    lens, that point could come with quite a bit of tilt.

    - If you are calibrating multiple cameras, and they are synchronized,
    you can calibrate them all at the same time, and obtain intrinsics AND
    extrinsics. In that case you want frames where multiple cameras see the
    calibration object at the same time. Depending on the geometry, it may
    be impossible to place a calibration object in a location where it's
    seen by all the cameras, AND where it's a close-up for all the cameras
    at the same time. In that case, get close-ups for each camera
    individually, and get observations common to multiple cameras, that
    aren't necessarily close-ups. The former will serve to define your
    camera intrinsics, and the latter will serve to define your extrinsics
    (geometry).

    A dataset composed primarily of tilted closeups will produce good
    results. It is better to have more data rather than less. mrgingham will
    throw away frames where no chessboard can be found, so it is perfectly
    reasonable to grab too many images with the expectation that they won't
    all end up being used in the computation.

    I usually aim for about 100 usable frames, but you can often get away
    with far fewer. The mrcal confidence feedback (see below) will tell you
    if you need more data.

    Once we have gathered input images, we can run the calibration
    procedure:

      mrcal-calibrate-cameras
        --corners-cache corners.vnl
        -j 10
        --focal 2000
        --object-spacing 0.1
        --object-width-n 10
        --out /tmp
        --distortion-model DISTORTION_OPENCV8
        --observed-pixel-uncertainty 1.0
        --explore
        'frame*-camera0.png' 'frame*-camera1.png' 'frame*-camera2.png'

    You would adjust all the arguments for your specific case.

    The first argument says that the chessboard corner coordinates live in a
    file called "corners.vnl". If this file exists, we'll use that data. If
    that file does not exist (which is what will happen the first time),
    mrgingham will be invoked to compute the corners from the images, and
    the results will be written to that file. So the same command is used to
    both compute the corners initially, and to reuse the pre-computed
    corners with subsequent runs.

    '-j 10' says to spread the mrgingham computation across 10 CPU cores.
    This command controls mrgingham only; if 'corners.vnl' exists, this
    option does nothing.

    '--focal 2000' says that the initial estimate for the camera focal
    lengths is 2000 pixels. This doesn't need to be precise at all, but do
    try to get this roughly correct if possible. Simple geometry says that

      focal_length = imager_width / ( 2 tan (field_of_view_horizontal / 2) )

    --object-spacing is the width of each square in your chessboard. This
    depends on the specific chessboard object you are using.
    --object-width-n is the corner count of the calibration object.
    Currently mrgingham more or less assumes that this is 10.

    --out specifies the directory where the output models will be written

    --distortion-model specifies which distortion model we're using for the
    cameras. At this time all OpenCV distortion models are supported, in
    addition to DISTORTION_CAHVOR. The CAHVOR model is there for legacy
    compatibility only. If you're not going to be using these models in a
    system that only supports CAHVOR, there's little reason to use it. If
    you use a model that is too lean (DISTORTION_NONE or DISTORTION_OPENCV4
    maybe), the model will not fit the data, especially at the edges; the
    tool will tell you this. If you use a model that is too rich (something
    crazy like DISTORTION_OPENCV14), then you will need much more data than
    you normally would. Most lenses I've seen work well with
    DISTORTION_OPENCV4 or DISTORTION_OPENCV5 or DISTORTION_OPENCV8; wider
    lenses need richer models.

    '--observed-pixel-uncertainty 1.0' says that the x,y corner coordinates
    reported by mrgingham are distributed normally, independently, and with
    the standard deviation as given in this argument. There's a tool to
    compute this value empirically, but it needs more validation. For now
    pick a value that seems reasonable. 1.0 pixels or less usually makes
    sense.

    --explore says that after the models are computed, a REPL should be open
    so that the user can look at various metrics describing the output; more
    on this later.

    After all the options, globs describing the images are passed in. Note
    that these are GLOBS, not FILENAMES. So you need to quote or escape each
    glob to prevent the shell from expanding it. You want one glob per
    camera; in the above example we have 3 cameras. The program will look
    for all files matching the globs, and filenames with identical matched
    strings are assumed to have been gathered at the same instant in time.
    I.e. if in the above example we found frame003-camera0.png and
    frame003-camera1.png, we will assume that these two images were
    time-synchronized. If your capture system doesn't have fully-functional
    frame syncronization, you should run a series of monocular calibrations.
    Otherwise the models won't fit well (high reprojection errors and/or
    high outlier counts) and you might see a frame with systematic
    reprojection errors where one supposedly-synchronized camera's
    observation pulls the solution in one direction, and another camera's
    observation pulls it in another.

    When you run the program as given above, the tool will spend a bit of
    time computing (usually 10-20 seconds is enough, but this is highly
    dependent on the specific problem, the amount of data, and the
    computational hardware). When finished, it will write the resulting
    models to disk, and open a REPL (if --explore was given). Models are
    written in both .cahvor and .cameramodel file formats. Both contain the
    same information, but .cameramodel is far more sensible. The .cahvor
    file format exists for legacy compatibility only. Use this one one only
    if you'll be using these models in some cahvor-only tool; in this case
    you'll probably want to choose the DISTORTION_CAHVOR model as well. The
    resulting filenames are "camera-N.cameramodel" where N is the index of
    the camera, starting at 0. The models contain the intrinsics and
    extrinsics, with camera-0 sitting at the reference coordinate system.

    When the solve is completed, you'll see a summary such as this one:

        RMS reprojection error: 0.3 pixels
        Worst reprojection error: 4.0 pixels
        Noutliers: 7 out of 9100 total points: 0.1% of the data

    The reprojection errors should look reasonable given your
    --observed-pixel-uncertainty. Since any outliers will be thrown out, the
    reported reprojection errors will be reasonable.

    Higher outlier counts are indicative of some/all of these:

    - Errors in the input data, such as incorrectly-detected chessboard
    corners, or unsynchronized cameras

    - Badly-fitting distortion model

    A distortion model that doesn't fit isn't a problem in itself. The
    results will simply not be reliable everywhere in the imager, as
    indicated by the uncertainty and residual metrics (see below)

    With --explore you get a REPL, and a message that points out some useful
    functions. Generally you want to start with

        show_residuals_observation_worst(0)

    This will show you the worst-fitting chessboard observation with its
    observed and predicted corners, as an error vector. The reprojection
    errors are given by a colored dot. Corners thrown out as outliers will
    be missing their colored dot. You want to make sure that this is
    reasonable. Incorrectly-detected corners will be visible: they will be
    outliers or they will have a high error. The errors should be higher
    towards the edge of the imager, especially with a wider lens. A richer
    better-fitting model would reduce those errors. Past that, there should
    be no pattern to the errors. If the camera synchronization was broken,
    you'll see a bias in the error vectors, to compensate for the motion of
    the chessboard.

    Next do this for each camera in your calibration set (i_camera is an
    index counting up from 0):

        show_residuals('regional', i_camera)

    Each of these will pop up 3 plots describing your distribution of
    errors. You get

    - a plot showing the mean reprojection error across the imager - a plot
    showing the standard deviation of reprojection errors across the imager
    - a plot showing the number of data points across the imager AFTER the
    outlier rejection

    The intrinsics are reliable in areas that have

    - a low mean error relative to --observed-pixel-uncertainty - a standard
    deviation roughly similar to --observed-pixel-uncertainty - have some
    data available

    If you have too little data, you will be overfitting, so you'd be
    expalining the signal AND the noise, and your reprojection errors will
    be too low. With enough input data you'll be explaining the signal only:
    the noise is random and with enough samples our model can't explain it.
    Another factor that controls this is the model we're fitting. If we fit
    a richer model (DISTORTION_OPENCV8 vs DISTORTION_OPENCV4 for instance),
    the extra parameters will allow us to fit the data better, and to
    produce lower errors in more areas of the imager.

    These are very rough guidelines; I haven't written the logic to
    automatically interpret these yet. A common feature that these plots
    bring to light is a poorly-fitting model at the edges of the imager. In
    that case you'll see higher errors with a wider distribution towards the
    edge.

    Finally run this:

        show_intrinsics_uncertainty()

    This will pop up a plot of projection uncertainties for each camera. The
    uncertainties are shown as a color-map along with contours. These are
    the expected value of projection based on noise in input corner
    observations. The noise is assumed to be independent, 0-mean gaussian
    with a standard deviation of --observed-pixel-uncertainty. You will see
    low uncertainties in the center of the imager (this is the default focus
    point; a different one can be picked). As you move away from the center,
    you'll see higher errors. You should decide how much error is
    acceptable, and determine the usable area of the imager based on this.
    These uncertainty metrics are complementary to the residual metrics
    described above. If you have too little data, the residuals will be low,
    but the uncertainties will be very high. The more data you gather, the
    lower the uncertainties. A richer distortion model lowers the residuals,
    but raises the uncertainties. So with a richer model you need to get
    more data to get to the same acceptable uncertainty level. The
    uncertainties are all determined relative to some focus point. If you
    care about the calibration accuracy in a particular area of the imager,
    do something like this instead:

        show_intrinsics_uncertainty( focus_center = np.array((1000,2000))) )

OPTIONS
  POSITIONAL ARGUMENTS
      images                A glob-per-camera for the images. Include a glob for
                            each camera. It is assumed that the image filenames in
                            each glob are of of the form xxxNNNyyy where xxx and
                            yyy are common to all images in the set, and NNN
                            varies. This NNN is a frame number, and identical
                            frame numbers across different globs signify a time-
                            synchronized observation. I.e. you can pass
                            'left*.jpg' and 'right*.jpg' to find images
                            'left0.jpg', 'left1.jpg', ..., 'right0.jpg',
                            'right1.jpg', ...

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --focal FOCAL         Initial estimate of the focal length, in pixels
      --imagersize IMAGERSIZE IMAGERSIZE
                            Size of the imager. This is only required if we pass
                            --corners-cache AND if none of the image files on disk
                            actually exist
      --outdir OUTDIR       Directory for the output camera models
      --object-spacing OBJECT_SPACING
                            Width of each square in the calibration board, in
                            meters
      --object-width-n OBJECT_WIDTH_N
                            How many points the calibration board has per side
      --distortion-model DISTORTION_MODEL
                            Which distortion model we're using. By default I use
                            DISTORTION_OPENCV4
      --roi ROI ROI ROI ROI
                            Region of interest of the calibration. This is the
                            area in the imager we're interested in. Errors in
                            observations outside this area will be attenuated
                            significantly. If we want to use all the data evenly,
                            omit this argument. Otherwise pass 4 values for each
                            --roi: (x_center,y_center,x_radius,y_radius). The
                            region is an axis-aligned ellipsoid. If passing in ANY
                            roi, you MUST pass in the ROI for EACH camera; a
                            separate '--roi' for each one.
      --incremental         If passed, we incrementally increase ROI and
                            distortion model complexity across multiple solves. In
                            this mode the requested ROI is a target, and the
                            requested distortion model is the upper bound. If we
                            can get away with a simpler one, we use that.
      --num-cross-validation-splits NUM_CROSS_VALIDATION_SPLITS
                            If passed, we cross-validate the results with this
                            many splits. This only makes sense as an integer >1.
                            THIS IS EXPERIMENTAL.
      --jobs JOBS, -j JOBS  How much parallelization we want. Like GNU make.
                            Affects only the chessboard corner finder. If we are
                            reading a cache file, this does nothing
      --corners-cache CORNERS_CACHE
                            Path to read corner-finder data from or (if path does
                            not exist) to write data to
      --muse-extrinsics     Apply MUSE's non-identity rotation for camera0
      --skip-regularization
                            By default we apply regularization to the solver. This
                            option turns that off
      --skip-outlier-rejection
                            By default we throw out outliers. This option turns
                            that off
      --verbose-solver      By default the final stage of the solver doesn't say
                            much. This option turns on verbosity to get lots of
                            diagnostics.
      --explore             After the solve open an interactive shell to examine
                            the solution
      --observed-pixel-uncertainty OBSERVED_PIXEL_UNCERTAINTY
                            The standard deviation of x and y pixel coordinates of
                            the input observations. The distribution of the inputs
                            is assumed to be gaussian, with the standard deviation
                            specified by this argument. Note: this is the x and y
                            standard deviation, treated independently. If each of
                            these is s, then the LENGTH of the deviation of each
                            pixel is a Rayleigh distribution with expected value
                            s*sqrt(pi/2) ~ s*1.25
      --cull-points-left-of CULL_POINTS_LEFT_OF
                            For testing. Throw out all observations with x < the
                            given value
      --cull-points-rad-off-center CULL_POINTS_RAD_OFF_CENTER
                            For testing. Throw out all observations with
                            dist_from_center > the given value
      --cull-random-observations-ratio CULL_RANDOM_OBSERVATIONS_RATIO
                            For testing. Throw out a random number of board
                            observations. The ratio of observations is given as
                            the argument. 1.0 = throw out ALL the observations;
                            0.0 = throw out NONE of the observations


#+END_EXAMPLE
** mrcal-convert-distortion
#+BEGIN_EXAMPLE
NAME
    mrcal-convert-distortion - Converts a camera model from one distortion
    model to another

SYNOPSIS
      $ mrcal-convert-distortion
          --viz --to DISTORTION_OPENCV4 left.cameramodel
          > left.opencv4.cameramodel

      ... lots of output as the solve runs ...
      libdogleg at dogleg.c:1064: success! took 10 iterations
      RMS error of the solution: 3.40256580058 pixels.

      ... a plot pops up showing the vector field of the difference ...

DESCRIPTION
    DESCRIPTION

    This is a tool to convert a given camera model from one distortion model
    to another. The input and output models have identical extrinsics and an
    identical intrinsic core (focal lengths, center pixel coords). The ONLY
    differing part is the distortion coefficients.

    While the distortion models all exist to solve the same problem, the
    different representations don't map to one another perfectly, so this
    tool seeks to find the best fit only. It does this by sampling a number
    of points in the imager, converting them to observation vectors in the
    camera coordinate system (using the given camera model), and then
    fitting a new camera model (with a different distortions) that matches
    the observation vectors to the source imager coordinates.

    Note that the distortion model implementations are usually optimized in
    the 'undistort' direction, not the 'distort' direction, so the step of
    converting the target imager coordinates to observation vectors can be
    slow. This is highly dependent on the camera model specifically. CAHVORE
    especially is glacial. This can be mitigated somewhat by a better
    implementation, but in the meantime, please be patient.

    Camera models have originally been computed by a calibration procedure
    that takes as input a number of point observations, and the resulting
    models are only valid in an area where those observations were
    available; it's an extrapolation everywhere else. This is generally OK,
    and we try to cover the whole imager when calibrating cameras. Models
    with high distortions (CAHVORE, OPENCV >= 8) generally have
    quickly-increasing effects towards the edges of the imager, and the
    distortions represented by these models at the extreme edges of the
    imager are often not reliable, since the initial calibration data is
    rarely available at the extreme edges. Thus using points at the extreme
    edges to fit another model is often counterproductive, and I provide the
    --where and --radius commandline options for this case. We use data in a
    circular region of the imager. This region is centered on the point
    given by --where (or at the center of the imager, if omitted). The
    radius of this region is given by --radius. If '--radius 0' is given, I
    use ALL the data. A radius<0 can be used to set the size of the no-data
    margin at the corners; in this case I'll use sqrt(width^2 + height^2) -
    abs(radius)

OPTIONS
  POSITIONAL ARGUMENTS
      cameramodel          Input camera model. Assumed to be mrcal native, Unless
                           the name is xxx.cahvor, in which case the cahvor format
                           is assumed

  OPTIONAL ARGUMENTS
      -h, --help           show this help message and exit
      --to TO              The target distortion model
      --verbose            Report the solver details
      --viz                Visualize the difference
      --where WHERE WHERE  I use a subset of the imager to compute the fit. The
                           active region is a circle centered on this point. If
                           omitted, we will focus on the center of the imager
      --radius RADIUS      I use a subset of the imager to compute the fit. The
                           active region is a circle with a radius given by this
                           paramter. If radius == 0, I'll use the whole imager for
                           the fit. If radius < 0, this parameter specifies the
                           width of the region at the corners that I should
                           ignore: I will use sqrt(width^2 + height^2) -
                           abs(radius). This is valid ONLY if we're focussing at
                           the center of the imager
      --writecahvor        If given, we write the output in a cahvor file format


#+END_EXAMPLE
** mrcal-show-distortion
#+BEGIN_EXAMPLE
NAME
    mrcal-show-distortion - Renders a vector field to visualize the effect
    of a model

SYNOPSIS
      $ mrcal-show-distortion --vectorfield left.cameramodel
      ... a plot pops up showing the distortion vector field

DESCRIPTION
    This allows us to visually see what a distortion model does. Depending
    on the model, the vectors could be very large or very small, and we can
    scale them by passing '--scale s'. By default we sample in a 60x40 grid,
    but this spacing can be controlled by passing '--gridn w h'.

    By default we render a heat map of the distortion. We can also see the
    vectorfield by passing in --vectorfield. Or we can see the radial
    distortion curve by passing --radial

OPTIONS
  POSITIONAL ARGUMENTS
      model                 Input camera model. Assumed to be mrcal native, unless
                            the name is xxx.cahvor, in which case the cahvor
                            format is assumed

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --gridn GRIDN GRIDN   How densely we should sample the imager. By default we
                            report a 60x40 grid
      --scale SCALE         Scale the vectors by this factor. Default is 1.0
                            (report the truth), but this is often too small to see
      --radial              Show the radial distortion scale factor instead of a
                            colormap/vectorfield
      --vectorfield         Plot the diff as a vector field instead of as a heat
                            map. The vector field contains more information
                            (magnitude AND direction), but is less clear at a
                            glance
      --cbmax CBMAX         Maximum range of the colorbar
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --hardcopy HARDCOPY   Write the output to disk, instead of making an
                            interactive plot
      --extraset EXTRASET   Extra 'set' directives to gnuplot. Can be given
                            multiple times


#+END_EXAMPLE
** mrcal-show-intrinsics-uncertainty
#+BEGIN_EXAMPLE
NAME
    mrcal-show-intrinsics-uncertainty - Renders the expected projection
    error due to calibration uncertainty

SYNOPSIS
      $ mrcal-show-intrinsics-uncertainty left.cameramodel
      ... a plot pops up showing the projection uncertainty of the intrinsics in
      ... this model

DESCRIPTION
    A calibration process produces the best-fitting camera parameters
    (intrinsics and extrinsics) and a inv(JtJ) matrix representing the
    uncertainty in these parameters. When we use the intrinsics to project
    3D points into the image plane, this intrinsics uncertainty creates an
    uncertainty in the resulting projection point. This tool plots the
    expected value of this projection error across the imager. Areas with a
    high expected projection error are unreliable for further work.

    Only mrcal-native .cameramodel files are supported because .cahvor files
    don't store the required data

OPTIONS
  POSITIONAL ARGUMENTS
      model                 Input camera model. Assumed to be mrcal native

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --gridn GRIDN GRIDN   How densely we should sample the imager. By default we
                            report a 60x40 grid
      --where WHERE WHERE   Center of the region of interest. Uncertainty is a
                            relative concept, so I focus on a particular area. I
                            compute an implied rotation to make that area as
                            certain as possible. The center of this focus area is
                            given by this argument. If omitted, we will focus on
                            the center of the imager
      --radius RADIUS       Radius of the region of interest. If ==0, we do NOT
                            fit an implied rotation at all. If omitted or <0, we
                            use a "reasonable value: min(width,height)/6. To fit
                            with data across the WHOLE imager: pass in a very
                            large radius
      --outlierness         Report an outlierness-based uncertainty, not an input-
                            noise-based one
      --cbmax CBMAX         Maximum range of the colorbar
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --hardcopy HARDCOPY   Write the output to disk, instead of an interactive
                            plot
      --extraset EXTRASET   Extra 'set' directives to gnuplot. Can be given
                            multiple times


#+END_EXAMPLE
** mrcal-show-intrinsics-diff
#+BEGIN_EXAMPLE
NAME
    mrcal-show-intrinsics-diff - Renders a difference in projection between
    two models

SYNOPSIS
      $ mrcal-show-intrinsics-diff before.cameramodel after.cameramodel
      ... a plot pops up showing how two models differ in their projections

DESCRIPTION
    If we're given exactly 2 models then I can either show a vector field of
    a heat map of the differences. I N > 2 then a vector field isn't
    possible and we show a heat map of the standard deviation of the
    differences. Note that for N=2 the difference shows in a-b, which is NOT
    the standard deviation (that is (a-b)/2). I use the standard deviation
    for N > 2

    This routine fits the implied camera rotation to align the models as
    much as possible. This is required because a camera pitch/yaw motion
    looks a lot like a shift in the camera optical axis (cx,cy). So I could
    be comparing two sets of intrinsics that both represent the same lens
    faithfully, but imply different rotations: the rotation would be
    compensated for by a shift in cx,cy. If I compare the two sets of
    intrinsics by IGNORING rotations, I would get a large diff because of
    the cx,cy difference.

    In all 3 cases I try to find the largest matching region around the area
    of interest. So the recommentation is to specify where['center'], but to
    omit where['radius'].

OPTIONS
  POSITIONAL ARGUMENTS
      models                Camera models to diff

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --gridn GRIDN GRIDN   How densely we should sample the imager. By default we
                            report a 60x40 grid
      --where WHERE WHERE   Center of the region of interest for this diff. It is
                            usually impossible for the models to match everywhere,
                            but focusing on a particular area can work better. The
                            implied rotation will be fit to match as large as
                            possible an area centered on this argument. If
                            omitted, we will focus on the center of the imager
      --radius RADIUS       Radius of the region of interest. If ==0, we do NOT
                            fit an implied rotation at all. If omitted or <0, we
                            use a "reasonable value: min(width,height)/6. To fit
                            with data across the WHOLE imager: pass in a very
                            large radius
      --cbmax CBMAX         Maximum range of the colorbar
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --vectorfield         Plot the diff as a vector field instead of as a heat
                            map. The vector field contains more information
                            (magnitude AND direction), but is less clear at a
                            glance
      --hardcopy HARDCOPY   Write the output to disk, instead of making an
                            interactive plot
      --extraset EXTRASET   Extra 'set' directives to gnuplot. Can be given
                            multiple times


#+END_EXAMPLE
** mrcal-undistort-image
#+BEGIN_EXAMPLE
NAME
    mrcal-undistort-image - Un-distorts image(s)

SYNOPSIS
      $ mrcal-undistort-image --model left.cameramodel im1.png im2.png
      ... corresponding pinhole mrcal-native model
      Wrote im1-undistorted.png
      Wrote im2-undistorted.png

DESCRIPTION
    Given a single camera model (cahvor or mrcal-native) and some number of
    images, this tool un-distorts each image and writes the result to disk.
    For each image named xxxx.yyy, the new image filename is
    xxxx-undistorted.yyy. This tool refuses to overwrite anything, and will
    barf if a target file already exists. A corresponding pinhole camera
    model is also generated, and written to stdout.

    Note that currently the corresponding pinhole model uses the same focal
    length, center pixel values as the original, but no distortions. Thus
    the undistorted images might cut out chunks of the original, or leave
    empty borders on the edges.

OPTIONS
  POSITIONAL ARGUMENTS
      image                 Images to undistort

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --model MODEL         Input camera model. Assumed to be mrcal native, Unless
                            the name is xxx.cahvor, in which case the cahvor
                            format is assumed
      --fit FIT             If given, we'll scale the focal length of the pinhole
                            model to fit some of the original image into the
                            output. This is one of "corners", "centers-
                            horizontal", "centers-vertical". If omitted, we keep
                            the focal lengths the same
      --scale SCALE         If given, we scale the size of the pinhole image by
                            this factor. By default the scale is 1.0, i.e. the
                            undistorted and distorted images have the same size
      --force, -f           By default I don't overwrite existing files. Pass
                            --force to overwrite them without complaint
      --jobs JOBS, -j JOBS  parallelize the processing JOBS-ways. This is like GNU
                            make, except you're required to explicitly specify a
                            job count.


#+END_EXAMPLE
** mrcal-redistort-points
#+BEGIN_EXAMPLE
NAME
    mrcal-redistort-points - Converts distorted points from one model to
    another

SYNOPSIS
      $ mrcal-redistort-points
          --from pinhole.cameramodel
          --to fisheye.cameramodel
          < input.vnl > output.vnl

DESCRIPTION
    This tool takes a set of pixel observations corresponding to one camera
    model, and converts them to corresponding observations in another model.
    This is useful in conjunction with the 'undistort-points' tool. An
    envisioned usage:

    - undistort-points --model fisheye.cameramodel input.png This produces
    an undistorted image and a corresponding pinhole camera model.

    - Run some sort of feature-detection thing on the input_undistorted.png
    thing we just made. This feature-detection thing can make geometric
    assumptions that wouldn't hold in the distorted image

    - redistort-points to convert the pixel coords we got from the feature
    detector back into the space of the original image

    The input data comes in on standard input, and the output data goes out
    on standard output. Both are vnlog data: human-readable text with 2
    columns: x and y pixel coord. Comments are allowed, and start with the
    '#' character.

OPTIONS
  OPTIONAL ARGUMENTS
      -h, --help   show this help message and exit
      --from FROM  Camera model for the INPUT points. Assumed to be mrcal native,
                   Unless the name is xxx.cahvor, in which case the cahvor format
                   is assumed
      --to TO      Camera model for the OUTPUT points. Assumed to be mrcal native,
                   Unless the name is xxx.cahvor, in which case the cahvor format
                   is assumed


#+END_EXAMPLE
** mrcal-graft-cameramodel
#+BEGIN_EXAMPLE
NAME
    mrcal-graft-cameramodel - Combines the intrinsics of one cameramodel
    with the extrinsics of another

SYNOPSIS
      $ mrcal-graft-cameramodel
          intrinsics.cameramodel
          extrinsics.cameramodel
          > joint.cameramodel
      Merged intrinsics from 'intrinsics.cameramodel' with extrinsics from
      'exrinsics.cameramodel'

DESCRIPTION
    This tool combines intrinsics and extrinsics from different sources into
    a single model. The output is written to standard out in mrcal-native
    format

OPTIONS
  POSITIONAL ARGUMENTS
      intrinsics  Input camera model for the intrinsics. Assumed to be mrcal
                  native, Unless the name is xxx.cahvor, in which case the cahvor
                  format is assumed
      extrinsics  Input camera model for the extrinsics. Assumed to be mrcal
                  native, Unless the name is xxx.cahvor, in which case the cahvor
                  format is assumed

  OPTIONAL ARGUMENTS
      -h, --help  show this help message and exit


#+END_EXAMPLE

* REPOSITORY

https://github.jpl.nasa.gov/maritime-robotics/mrcal/

* AUTHOR

Dima Kogan (=Dmitriy.Kogan@jpl.nasa.gov=)

* LICENSE AND COPYRIGHT

All of this is currently proprietary. Do not distribute outside of JPL

Copyright 2016-2018 California Institute of Technology
