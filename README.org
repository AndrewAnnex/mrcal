* SYNOPSIS

#+BEGIN_EXAMPLE
$ mrcal-calibrate-cameras --focal 2000
      --outdir /tmp --object-spacing 0.01
      --object-width-n 10 '/tmp/left*.png' '/tmp/right*.png'

... lots of output as the solve runs ...
Wrote /tmp/camera0-0.cameramodel
Wrote /tmp/camera0-1.cameramodel
#+END_EXAMPLE

* SUMMARY

=mrcal= is a generic toolkit to solve calibration and SFM-like problems.
Functionality related to these problems is exposed as a set of python libraries.
Both CAHVOR and OpenCV lens models are fully supported; CAHVORE is partially
supported. The toolkit includes:

- Some libraries:
  - =libmrcal=: A flexible solver core written in C and providing a C API that
    solves the underlying optimization problem
  - =mrcal=: a Python library that contains (among other things) an interface
    to this core. Other things provided by the =mrcal= Python library:
    - functions to read/write/manipulate camera models
    - functions to manipulate 3D poses
    - functions to (un)project and (un)distort data

- Some tools:
  - =mrcal-calibrate-cameras=: calibrates N cameras
  - =mrcal-convert-lensmodel=: fits one lens model to another
  - =mrcal-show-distortion-off-pinhole=: visualize the deviation of a specific
    lens model from a pinhole model
  - =mrcal-show-splined-model-surface=: visualize the surface and knots used in
    the specification of splined models
  - =mrcal-show-projection-uncertainty=: visualize the uncertainty of intrinsics
    due to noise in the calibration inputs
  - =mrcal-show-projection-diff=: visualize the difference between the
    intrinsics of a number of models
  - =mrcal-reproject-points=: Given two lens models and a set of points,
    maps them from one lens model to the other
  - =mrcal-reproject-image=: Given image(s) and lens model(s), produces a new
    set of images that observe the same scene but with the other model. Several
    flavors of functionality are included here, such as undistortion-to-pinhole,
    re-rotation, and remapping to infinity.
  - =mrcal-graft-models=: Combines the intrinsics of one cameramodel with the
    extrinsics of another
  - =mrcal-to-cahvor=: Converts a model stored in the native =.cameramodel= file
    format to the =.cahvor= format. This exists for compatibility only, and does
    not touch the data: the lens distortion may or may not use the CAHVOR
    lens model
  - =mrcal-to-cameramodel=: Converts a model stored in the legacy =.cahvor= file
    format to the =.cameramodel= format. This exists for compatibility only, and
    does not touch the data: the lens distortion may or may not use the CAHVOR
    lens model
  - =mrcal-show-geometry=: Shows a visual representation of the geometry
    represented by some camera models on disk, and optionally, the
    chessboard observations used to compute that geometry
  - =mrcal-show-valid-intrinsics-regions=: Visualizes the region where a model's
    intrinsics are valid
  - =mrcal-is-within-valid-intrinsics-region=: Augments a vnlog of pixel
    coordinates with a column indicating whether or not each point lies within
    the valid-intrinsics region

These libraries and tools make it easy to both produce calibrations in many ways
and to manipulate them by moving stuff around, grafting various
intrinsics/extrinsics, etc.

* DESCRIPTION

** Fundamental assumptions

Some notational conventions are used throught the code and implementation, and
they're explicitly described here.

*** Coordinate system conventions

No convention is assumed for the world coordinate system. The canonical camera
coordinate system has =x,y= as with pixel coordinates in an image: =x= is to the
"right" and =y= is "down". =z= is then "forward" to complete the right-handed
system of coordinates

*** Transformation conventions

When describing a transformation (or its rotation/translation components) that
maps a point represented in coordinate system A to its representation in
coordinate system B, =T_AB= is the notation used. These chain together nicely,
so if we know the transformation between =A= and =B= and between =B= and =C=, we
can transform a point represented in =C= to =A=: =x_A = T_AB T_BC x_C=.

*** Pose representations

Various parts of the toolkit have preferred representations of pose, and =mrcal=
has functions to convert between them. Available representations are:

- =Rt=: a (4,3) numpy array with a (3,3) rotation matrix concatenated with a
  (1,3) translation vector

- =rt=: a (6,) numpy array with a (3,) vector representing a Rodrigues rotation
  concatenated with another (3,) vector, representing a rotation.

Each of these represents a transformation =rotate(x) + t=.

Since a pose represents a transformation between two coordinate systems, the
toolkit generally refers to a pose as something like =Rt_AB=, which is an
=Rt=-represented transformation to convert a point from a representation in the
coordinate system =B= to a representation in coordinate system =A=. =B=

A Rodrigues rotation vector =r= represents a rotation of =length(r)= radians
around an axis in the direction =r=. Converting between =R= and =r= is done via
the [[https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula][Rodrigues rotation formula]]. This is done with =mrcal.r_from_R() and
mrcal.R_from_r()=. For translating /poses/, not just rotations, use
=mrcal.Rt_from_rt()= and =mrcal.rt_from_Rt()=.

** Camera model file formats

Reading/writing cameramodels is done in Python with the =mrcal.cameramodel=
class. This class supports two different file formats:

- =.cameramodel=: the preferred format. This is a simple text representation
  that has clear sections for the lens model, pinhole intrinsics,
  distortion coefficients, and an extrinsic pose. The pose is represented as
  =rt_fromref=: an =rt= transformation /from/ the reference coordinate system
  /to/ the coordinate system of this camera. The class provides methods to get
  the transformation in any form, but =rt_fromref= is the internal
  representation

- =.cahvor=: the legacy format. This exists for compatibility with existing JPL
  tools. There's no other reason to use this format

The file format is just a way to store data: any lens model can be stored
in any file format. Currently some things aren't representable in a =.cahvor=
file (uncertainty stuff), but only mrcal tools know what to do with that data,
and mrcal supports =.cameramodel= files.

** Lens models

Lens models are specified as elements of =enum lensmodel_t= (in C) or, as
strings that match the entries of that enum (in Python). Currently I support all
CAHVOR flavors and all models implemented in OpenCV and a pure stereographic
model and a /very/ rich splined stereographic model. CAHVORE isn't supported in
the solver. Some models have configuration parameters defined in the model
string. These define details of the model, and are not subject to optimization.
Currently the supported models are:

- =LENSMODEL_PINHOLE=
- =LENSMODEL_STEREOGRAPHIC=
- =LENSMODEL_SPLINED_STEREOGRAPHIC_...= (this model has configuration
  parameters)
- =LENSMODEL_OPENCV4=
- =LENSMODEL_OPENCV5=
- =LENSMODEL_OPENCV8=
- =LENSMODEL_OPENCV12= (if we have OpenCV >= 3.0.0)
- =LENSMODEL_CAHVOR=
- =LENSMODEL_CAHVORE=

** Calibration object

When running a camera calibration, we use camera observations of a calibration
object (usually a chessboard). These images must be converted to a set of pixels
where chessboard corners were observed. =mrcal= is a purely geometrical toolkit,
so this vision problem is handled by another library: [[https://github.com/dkogan/mrgingham/][=mrgingham=]]. See its
documentation for more details.

* MANPAGES
** mrcal-calibrate-cameras
#+BEGIN_EXAMPLE
NAME
    mrcal-calibrate-cameras - Calibrate some synchronized, stationary
    cameras

SYNOPSIS
      $ mrcal-calibrate-cameras
          --corners-cache corners.vnl
          --focal 1700 --object-spacing 0.01 --object-width-n 10
          --outdir /tmp
          --lensmodel LENSMODEL_OPENCV8
          --observed-pixel-uncertainty 0.5
          --pairs
          'left*.png' 'right*.png'

        ... lots of output as the solve runs ...
        Done!
        RMS reprojection error: 1.9 pixels
        Worst reprojection error: 7.8 pixels
        Noutliers: 319 out of 17100 total points: 1.9% of the data

        Wrote /tmp/camera0-0.cameramodel
        Wrote /tmp/camera0-1.cameramodel

DESCRIPTION
    This tool uses the generic mrcal platform to solve a common specific
    problem of N-camera calibration using observations of a chessboard.

    TUTORIAL

    If all you want to do is run a calibration, read this section first.

    You need to get observations of a grid of points. This tool doesn't
    dictate exactly how these observations are obtained, but the recommended
    way to do that is to use mrgingham (http://github.com/dkogan/mrgingham).
    This documentation assumes that's what is being done.

    See the mrgingham documentation for a .pdf of a chessboard pattern. This
    pattern should be printed (at some size; see below) and mounted onto a
    RIGID and FLAT surface to produce the calibration object. The most
    useful observations are close-ups: views that cover as much of the
    imager as possible. Thus you generally a large printout of the
    chessboard pattern. If you're calibrating a wide lens then this is
    especially true: the wider the lens, the larger an object needs to be in
    order to cover the field of view.

    Now that we have a calibration object, this object needs to be shown to
    the camera(s) to produce the images that mrgingham will use to find the
    corner coordinates, which mrcal will then use in its computations.

    It is important that the images contain clear corners. If the image is
    badly overexposed, the white chessboard squares will bleed into each
    other, the adjoining black squares will no longer touch each other in
    the image, and there would be no corner to detect. Conversely, if the
    image is badly underexposed, the black squares will bleed into each
    other, which would also destroy the corner. mrgingham tries to handle a
    variety of lighting conditions, including varying illumination across
    the image, but the corners must exist in the image in some form. A
    fundamental design decision in mrgingham is to only output chessboards
    that we are very confident in, and a consequence of this is that
    mrgingham requires the WHOLE chessboard to be visible in order to
    produce any results. Thus it requires a bit of effort to produce any
    data at the edges and in the corners of the imager: if even a small
    number of the chessboard corners are out of bounds, mrgingham will not
    detect the chessboard at all. A live preview of the calibration images
    being gathered is thus essential to aid the user in obtaining good data.
    Another requirement due to the design of mrgingham is that the board
    should be held with a flat edge parallel to the camera xz plane
    (parallel to the ground, usually). mrgingham looks for vertical and
    horizontal sequences of corners, but if the board is rotated diagonally,
    then none of these sequences are "horizontal" or "vertical", but they're
    all "diagonal", which isn't what mrgingham is looking for.

    The most useful observations to gather are

    - close-ups: the chessboard should fill the whole frame as much as
    possible

    - oblique views: tilt the board forward/back and left/right. I generally
    tilt by more than 45 degrees. At a certain point the corners become
    indistinct and mrgingham starts having trouble, but depending on the
    lens, that point could come with quite a bit of tilt.

    - If you are calibrating multiple cameras, and they are synchronized,
    you can calibrate them all at the same time, and obtain intrinsics AND
    extrinsics. In that case you want frames where multiple cameras see the
    calibration object at the same time. Depending on the geometry, it may
    be impossible to place a calibration object in a location where it's
    seen by all the cameras, AND where it's a close-up for all the cameras
    at the same time. In that case, get close-ups for each camera
    individually, and get observations common to multiple cameras, that
    aren't necessarily close-ups. The former will serve to define your
    camera intrinsics, and the latter will serve to define your extrinsics
    (geometry).

    A dataset composed primarily of tilted closeups will produce good
    results. It is better to have more data rather than less. mrgingham will
    throw away frames where no chessboard can be found, so it is perfectly
    reasonable to grab too many images with the expectation that they won't
    all end up being used in the computation.

    I usually aim for about 100 usable frames, but you can often get away
    with far fewer. The mrcal confidence feedback (see below) will tell you
    if you need more data.

    Once we have gathered input images, we can run the calibration
    procedure:

      mrcal-calibrate-cameras
        --corners-cache corners.vnl
        -j 10
        --focal 2000
        --object-spacing 0.1
        --object-width-n 10
        --outdir /tmp
        --lensmodel LENSMODEL_OPENCV8
        --observed-pixel-uncertainty 1.0
        --explore
        'frame*-camera0.png' 'frame*-camera1.png' 'frame*-camera2.png'

    You would adjust all the arguments for your specific case.

    The first argument says that the chessboard corner coordinates live in a
    file called "corners.vnl". If this file exists, we'll use that data. If
    that file does not exist (which is what will happen the first time),
    mrgingham will be invoked to compute the corners from the images, and
    the results will be written to that file. So the same command is used to
    both compute the corners initially, and to reuse the pre-computed
    corners with subsequent runs.

    '-j 10' says to spread the mrgingham computation across 10 CPU cores.
    This command controls mrgingham only; if 'corners.vnl' already exists,
    this option does nothing.

    '--focal 2000' says that the initial estimate for the camera focal
    lengths is 2000 pixels. This doesn't need to be precise at all, but do
    try to get this roughly correct if possible. Simple geometry says that

      focal_length = imager_width / ( 2 tan (field_of_view_horizontal / 2) )

    --object-spacing is the width of each square in your chessboard. This
    depends on the specific chessboard object you are using.
    --object-width-n is the corner count of the calibration object.
    Currently mrgingham more or less assumes that this is 10.

    --outdir specifies the directory where the output models will be written

    --lensmodel specifies which lens model we're using for the cameras. At
    this time all OpenCV lens models are supported, in addition to
    LENSMODEL_CAHVOR. The CAHVOR model is there for legacy compatibility
    only. If you're not going to be using these models in a system that only
    supports CAHVOR, there's little reason to use it. If you use a model
    that is too lean (LENSMODEL_PINHOLE or LENSMODEL_OPENCV4 maybe), the
    model will not fit the data, especially at the edges; the tool will tell
    you this. If you use a model that is too rich (something crazy like
    LENSMODEL_OPENCV12), then you will need much more data than you normally
    would. Most lenses I've seen work well with LENSMODEL_OPENCV4 or
    LENSMODEL_OPENCV5 or LENSMODEL_OPENCV8; wider lenses need richer models.

    '--observed-pixel-uncertainty 1.0' says that the x,y corner coordinates
    reported by mrgingham are distributed normally, independently, and with
    the standard deviation as given in this argument. There's a tool to
    compute this value empirically, but it needs more validation. For now
    pick a value that seems reasonable. 1.0 pixels or less usually makes
    sense.

    --explore says that after the models are computed, a REPL should be open
    so that the user can look at various metrics describing the output; more
    on this later.

    After all the options, globs describing the images are passed in. Note
    that these are GLOBS, not FILENAMES. So you need to quote or escape each
    glob to prevent the shell from expanding it. You want one glob per
    camera; in the above example we have 3 cameras. The program will look
    for all files matching the globs, and filenames with identical matched
    strings are assumed to have been gathered at the same instant in time.
    I.e. if in the above example we found frame003-camera0.png and
    frame003-camera1.png, we will assume that these two images were
    time-synchronized. If your capture system doesn't have fully-functional
    frame syncronization, you should run a series of monocular calibrations.
    Otherwise the models won't fit well (high reprojection errors and/or
    high outlier counts) and you might see a frame with systematic
    reprojection errors where one supposedly-synchronized camera's
    observation pulls the solution in one direction, and another camera's
    observation pulls it in another.

    When you run the program as given above, the tool will spend a bit of
    time computing (usually 10-20 seconds is enough, but this is highly
    dependent on the specific problem, the amount of data, and the
    computational hardware). When finished, it will write the resulting
    models to disk, and open a REPL (if --explore was given). The resulting
    filenames are "camera-N.cameramodel" where N is the index of the camera,
    starting at 0. The models contain the intrinsics and extrinsics, with
    camera-0 sitting at the reference coordinate system.

    When the solve is completed, you'll see a summary such as this one:

        RMS reprojection error: 0.3 pixels
        Worst reprojection error: 4.0 pixels
        Noutliers: 7 out of 9100 total points: 0.1% of the data

    The reprojection errors should look reasonable given your
    --observed-pixel-uncertainty. Since any outliers will be thrown out, the
    reported reprojection errors will be reasonable.

    Higher outlier counts are indicative of some/all of these:

    - Errors in the input data, such as incorrectly-detected chessboard
    corners, or unsynchronized cameras

    - Badly-fitting lens model

    A lens model that doesn't fit isn't a problem in itself. The results
    will simply not be reliable everywhere in the imager, as indicated by
    the uncertainty and residual metrics (see below)

    With --explore you get a REPL, and a message that points out some useful
    functions. Generally you want to start with

        show_residuals_observation_worst(0)

    This will show you the worst-fitting chessboard observation with its
    observed and predicted corners, as an error vector. The reprojection
    errors are given by a colored dot. Corners thrown out as outliers will
    be missing their colored dot. You want to make sure that this is
    reasonable. Incorrectly-detected corners will be visible: they will be
    outliers or they will have a high error. The errors should be higher
    towards the edge of the imager, especially with a wider lens. A richer
    better-fitting model would reduce those errors. Past that, there should
    be no pattern to the errors. If the camera synchronization was broken,
    you'll see a bias in the error vectors, to compensate for the motion of
    the chessboard.

    Next do this for each camera in your calibration set (icam is an index
    counting up from 0):

        show_residuals_regional(icam)

    Each of these will pop up 3 plots describing your distribution of
    errors. You get

    - a plot showing the mean reprojection error across the imager - a plot
    showing the standard deviation of reprojection errors across the imager
    - a plot showing the number of data points across the imager AFTER the
    outlier rejection

    The intrinsics are reliable in areas that have

    - a low mean error relative to --observed-pixel-uncertainty - a standard
    deviation roughly similar to --observed-pixel-uncertainty - have some
    data available

    If you have too little data, you will be overfitting, so you'd be
    expalining the signal AND the noise, and your reprojection errors will
    be too low. With enough input data you'll be explaining the signal only:
    the noise is random and with enough samples our model can't explain it.
    Another factor that controls this is the model we're fitting. If we fit
    a richer model (LENSMODEL_OPENCV8 vs LENSMODEL_OPENCV4 for instance),
    the extra parameters will allow us to fit the data better, and to
    produce lower errors in more areas of the imager.

    These are very rough guidelines; I haven't written the logic to
    automatically interpret these yet. A common feature that these plots
    bring to light is a poorly-fitting model at the edges of the imager. In
    that case you'll see higher errors with a wider distribution towards the
    edge.

    Finally run this:

        show_projection_uncertainty()

    This will pop up a plot of projection uncertainties for each camera. The
    uncertainties are shown as a color-map along with contours. These are
    the expected value of projection based on noise in input corner
    observations. The noise is assumed to be independent, 0-mean gaussian
    with a standard deviation of --observed-pixel-uncertainty. You will see
    low uncertainties in the center of the imager (this is the default focus
    point; a different one can be picked). As you move away from the center,
    you'll see higher errors. You should decide how much error is
    acceptable, and determine the usable area of the imager based on this.
    These uncertainty metrics are complementary to the residual metrics
    described above. If you have too little data, the residuals will be low,
    but the uncertainties will be very high. The more data you gather, the
    lower the uncertainties. A richer lens model lowers the residuals, but
    raises the uncertainties. So with a richer model you need to get more
    data to get to the same acceptable uncertainty level.

OPTIONS
  POSITIONAL ARGUMENTS
      images                A glob-per-camera for the images. Include a glob for
                            each camera. It is assumed that the image filenames in
                            each glob are of of the form xxxNNNyyy where xxx and
                            yyy are common to all images in the set, and NNN
                            varies. This NNN is a frame number, and identical
                            frame numbers across different globs signify a time-
                            synchronized observation. I.e. you can pass
                            'left*.jpg' and 'right*.jpg' to find images
                            'left0.jpg', 'left1.jpg', ..., 'right0.jpg',
                            'right1.jpg', ...

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --focal FOCAL         Initial estimate of the focal length, in pixels.
                            Required unless --seed is given
      --imagersize IMAGERSIZE IMAGERSIZE
                            Size of the imager. This is only required if we pass
                            --corners-cache AND if none of the image files on disk
                            actually exist and if we don't have a --seed. If we do
                            have a --seed, the --imagersize values must match the
                            --seed exactly
      --outdir OUTDIR       Directory for the output camera models
      --object-spacing OBJECT_SPACING
                            Width of each square in the calibration board, in
                            meters
      --object-width-n OBJECT_WIDTH_N
                            How many points the calibration board has per
                            horizontal side. If omitted we default to 10
      --object-height-n OBJECT_HEIGHT_N
                            How many points the calibration board has per vertical
                            side. If omitted, we assume a square object, setting
                            height=width
      --lensmodel LENSMODEL
                            Which lens model we're using. This is required unless
                            we have a --seed
      --seed SEED           A comma-separated whitespace-less list of camera model
                            globs to use as a seed for the intrinsics and
                            extrinsics. The number of models must match the number
                            of cameras exactly. Expanded globs are sorted
                            alphanumerically. This is useful to bootstrap the
                            solve or to validate an existing set of models, or to
                            recompute just the extrinsics or just the intrinsics
                            of a solve. If omitted, we estimate a seed. Exclusive
                            with --focal. If given, --imagersize is omitted or it
                            must match EXACTLY with whatever is in the --seed
                            models
      --jobs JOBS, -j JOBS  How much parallelization we want. Like GNU make.
                            Affects only the chessboard corner finder. If we are
                            reading a cache file, this does nothing
      --corners-cache CORNERS_CACHE
                            Path to read corner-finder data from or (if path does
                            not exist) to write data to
      --pairs               By default, we are calibrating a set of N independent
                            cameras. If we actually have a number of stereo pairs,
                            pass this argument. It changes the filename format of
                            the models written to disk (cameraPAIR-
                            INDEXINPAIR.cameramodel), and will report some
                            uncertainties about geometry inside each pair.
                            Consecutive cameras in the given list are paired up,
                            and an even number of cameras is required
      --skip-regularization
                            By default we apply regularization in the solver in
                            the final optimization. This discourages obviously-
                            wrong solutions, but can introduce a bias. With this
                            option, regularization isn't applied
      --skip-outlier-rejection
                            By default we throw out outliers. This option turns
                            that off
      --skip-extrinsics-solve
                            Keep the seeded extrinsics, if given. Allowed only if
                            --seed
      --skip-intrinsics-solve
                            Keep the seeded intrinsics, if given. Allowed only if
                            --seed
      --skip-calobject-warp-solve
                            By default we assume the calibration target is
                            slightly deformed, and we compute this deformation. If
                            we want to assume that it is flat, pass this option.
      --unweighted-corners  By default we weight each corner error contribution
                            using the uncertainty from the corner detector. If we
                            want to ignore this information, and weigh them all
                            equally, pass --unweighted-corners.
      --verbose-solver      By default the final stage of the solver doesn't say
                            much. This option turns on verbosity to get lots of
                            diagnostics.
      --explore             After the solve open an interactive shell to examine
                            the solution
      --observed-pixel-uncertainty OBSERVED_PIXEL_UNCERTAINTY
                            The standard deviation of x and y pixel coordinates of
                            the input observations. The distribution of the inputs
                            is assumed to be gaussian, with the standard deviation
                            specified by this argument. Note: this is the x and y
                            standard deviation, treated independently. If each of
                            these is s, then the LENGTH of the deviation of each
                            pixel is a Rayleigh distribution with expected value
                            s*sqrt(pi/2) ~ s*1.25


#+END_EXAMPLE
** mrcal-convert-lensmodel
#+BEGIN_EXAMPLE
NAME
    mrcal-convert-lensmodel - Converts a camera model from one lens model to
    another

SYNOPSIS
      $ mrcal-convert-lensmodel
          --viz LENSMODEL_OPENCV4 left.cameramodel
          > left.opencv4.cameramodel

      ... lots of output as the solve runs ...
      RMS error of the solution: 3.40256580058 pixels.

      ... a plot pops up showing the differences ...

DESCRIPTION
    Given a camera model, this tool computes another model that represents
    the same lens, but using a different lens model. While lens models all
    exist to solve the same problem, the different representations don't map
    to one another perfectly, and this tool seeks to find the best-fitting
    parameters of the target lens model. Two different methods are
    implemented:

    1. If the given cameramodel file contains optimization_inputs, then we
    have all the data that was used to compute this model in the first
    place, and we can re-run the original optimization, using the new lens
    model. This is the default behavior. If the input model doesn't have
    optimization_inputs, an error will result, and the other method must be
    selected by passing --sampled

    2. We can sample lots of points on the imager, unproject them to
    observation vectors in the camera coordinate system, and then fit a new
    camera model that reprojects these vectors as closely to the original
    pixel coordinates as possible. Select this mode by passing --sampled.

    The first method is preferred. Since camera models (lens parameters AND
    geometry) are computed off real pixel observations, the confidence of
    the final projections varies greatly, depending on the location of the
    points being projected. The first method uses the original data, so it
    implicitly respects these uncertainties 100%: low-data areas in the
    original model will also be low-data areas in the new model. The second
    method, however, doesn't have this information: it doesn't know which
    parts of the imager are reliable and which aren't, so the results won't
    be as good.

    As always, the intrinsics have some baked-in geometry information. Both
    methods optimize intrinsics AND extrinsics, and output cameramodels with
    updated versions of both. If --sampled, then we can request that only
    the intrinsics be optimized by passing --intrinsics-only. Also, if
    --sampled then we fit the extrinsics off 3D points, not just observation
    directions. The distance from the camera to the fitting points is set by
    --distance. Set this to the distance where you expect the intrinsics to
    have the most accuracy. This is only neede dif --sampled, since we have
    all this information otherwise.

    If --sampled, we need to consider that the model we're trying to fit may
    not fit the original model in all parts of the imager. Usually this is a
    factor when converting wide-angle cameramodels to use a leaner model: a
    decent fit will be possible at the center, with more and more divergence
    as we move towards the edges. We handle this with the --where and
    --radius options to allow the user to choose the area of the imager that
    is used for the fit. This region is centered on the point given by
    --where (or at the center of the imager, if omitted). The radius of this
    region is given by --radius. If '--radius 0' then I use ALL the data. A
    radius<0 can be used to set the size of the no-data margin at the
    corners; in this case I'll use

        r = sqrt(width^2 + height^2)/2. - abs(radius)

    There's a balance to strike here. A larger radius means that we'll try
    to fit as well as we can in a larger area. This might mean that we won't
    fit well anywhere, but we won't do terribly anywhere, either. A smaller
    area means that we give up on the outer regions entirely (resulting in
    very poor fits there), but we'll be able to fit much better in the areas
    that remain. Generally empirical testing is required to find a good
    compromise: pass --viz to see the resulting differences. Note that
    --radius and --where applies only if we're optimizing sampled
    reprojections; if we're using the original optimization inputs, the
    options are illegal.

OPTIONS
  POSITIONAL ARGUMENTS
      to                    The target lens model
      model                 Input camera model. If "-' is given, we read standard
                            input

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --sampled             Instead of solving the original calibration problem
                            using the new lens model, use sampled imager points.
                            This produces biased results, but can be used even if
                            the original optimization_inputs aren't available
      --gridn GRIDN GRIDN   Used if --sampled. How densely we should sample the
                            imager. By default we use a 60x40 grid
      --distance DISTANCE   Used (and required) if --sampled. A sampled solve fits
                            the intrinsics and extrinsics to match up
                            reprojections of a grid of observed pixels. The points
                            being projected are set a particular distance (set by
                            this argument) from the camera. Set this to the
                            distance that is expected to be most confident for the
                            given cameramodel
      --intrinsics-only     Used if --sampled. By default I optimize the
                            intrinsics and extrinsics to find the closest
                            reprojection. If for whatever reason we know that the
                            camera coordinate system was already right, or we need
                            to keep the original extrinsics, pass --intrinsics-
                            only. The resulting extrinsics will be the same, but
                            the fit will not be as good. In many cases, optimizing
                            extrinsics is required to get a usable fit, so
                            --intrinsics-only may not be an option if accurate
                            results are required.
      --where WHERE WHERE   Used if --sampled. I use a subset of the imager to
                            compute the fit. The active region is a circle
                            centered on this point. If omitted, we will focus on
                            the center of the imager
      --radius RADIUS       Used if --sampled. I use a subset of the imager to
                            compute the fit. The active region is a circle with a
                            radius given by this parameter. If radius == 0, I'll
                            use the whole imager for the fit. If radius < 0, this
                            parameter specifies the width of the region at the
                            corners that I should ignore: I will use sqrt(width^2
                            + height^2)/2. - abs(radius). This is valid ONLY if
                            we're focusing at the center of the imager. By default
                            I ignore a large-ish chunk area at the corners.
      --viz                 Visualize the differences between the input and output
                            models
      --num-trials NUM_TRIALS
                            If given, run the solve more than once. Useful in case
                            random initialization produces noticeably different
                            results. By default we run just one trial, which
                            hopefully should be enough


#+END_EXAMPLE
** mrcal-show-distortion-off-pinhole
#+BEGIN_EXAMPLE
NAME
    mrcal-show-distortion-off-pinhole - Visualize the behavior or a lens
    model

SYNOPSIS
      $ mrcal-show-distortion-off-pinhole --vectorfield left.cameramodel

      ... a plot pops up showing the vector field of the difference from a pinhole
      projection

DESCRIPTION
    This tool is used to examine how a lens model behaves. Depending on the
    model, the vectors could be very large or very small, and we can scale
    them by passing '--scale s'. By default we sample in a 60x40 grid, but
    this spacing can be controlled by passing '--gridn w h'.

    By default we render a heat map of the lens effects. We can also see the
    vectorfield by passing in --vectorfield. Or we can see the radial
    distortion curve by passing --radial

OPTIONS
  POSITIONAL ARGUMENTS
      model                 Input camera model. If "-' is given, we read standard input

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --gridn GRIDN GRIDN   How densely we should sample the imager. By default we report a 60x40 grid
      --scale SCALE         Scale the vectors by this factor. Default is 1.0 (report the truth), but this is often too small to see
      --radial              Show the radial distortion scale factor instead of a colormap/vectorfield
      --vectorfield         Plot the diff as a vector field instead of as a heat map. The vector field contains more information (magnitude AND direction), but is less clear at a
                            glance
      --cbmax CBMAX         Maximum range of the colorbar
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --hardcopy HARDCOPY   Write the output to disk, instead of making an interactive plot
      --set SET             Extra 'set' directives to pass to gnuplotlib. May be given multiple times
      --unset UNSET         Extra 'unset' directives to pass to gnuplotlib. May be given multiple times


#+END_EXAMPLE
** mrcal-show-splined-model-surface
#+BEGIN_EXAMPLE
NAME
    mrcal-show-splined-model-surface - Visualizes the surface represented in
    a splined lens model

SYNOPSIS
      $ mrcal-show-splined-model-surface cam.cameramodel x
      ... a plot pops up showing the surface

DESCRIPTION
    Splined models are built with a splined surface that we index to compute
    the projection. The meaning of what indexes the surface and the values
    of the surface varies by model, but in all cases, visualizing the
    surface is useful.

    This tool can produce a plot in the imager domain (the default) or in
    the spline index domain (normalized stereographic coordinates, usually).
    Both are useful. Pass --spline-index-domain to choose that domain

    One use for this tool is to check that the field-of-view we're using for
    this model is reasonable. We'd like the field of view to be wide-enough
    to cover the whole imager, but not much wider, since representing
    invisible areas isn't useful. Ideally the surface domain boundary (that
    this tool displays) is just wider than the imager edges (which this tool
    also displays).

OPTIONS
  POSITIONAL ARGUMENTS
      model                 Input camera model. If "-' is given, we read standard
                            input
      {x,y}                 Whether we're looking at the x surface or the y
                            surface

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --hardcopy HARDCOPY   Write the output to disk, instead of making an
                            interactive plot
      --set SET             Extra 'set' directives to gnuplotlib. Can be given
                            multiple times
      --unset UNSET         Extra 'unset' directives to gnuplotlib. Can be given
                            multiple times
      --spline-index-domain
                            By default this produces a visualization in the domain
                            of the imager. Sometimes it's more informative to look
                            at this in the domain of the spline-index (normalized
                            stereographic coordinates). This can be selected by
                            this option


#+END_EXAMPLE
** mrcal-show-projection-uncertainty
#+BEGIN_EXAMPLE
NAME
    mrcal-show-projection-uncertainty - Visualize the expected projection
    error due to uncertainty in the calibration-time input

SYNOPSIS
      $ mrcal-show-projection-uncertainty left.cameramodel
      ... a plot pops up showing the projection uncertainty of the intrinsics in
      ... this model

DESCRIPTION
    A calibration process produces the best-fitting camera parameters. To
    use these parameters intelligently we must have some sense of
    uncertainty in these parameters. This tool examines the uncertainty of
    projection of points using a given camera model. The projection
    operation uses the intrinsics only, but the uncertainty must take into
    account the calibration-time extrinsics and the calibration-time
    chessboard poses as well. This tool visualizes the expected value of
    projection error across the imager. Areas with a high expected
    projection error are unreliable for further work.

    There are 3 modes of operation:

    - By default we look at projection of points some distance away from the
    camera (given by --distance). We evaluate the uncertainty of these
    projections everywhere across the imager, and display the results as a
    heatmap with overlaid contours

    - With --observations-xydist we display a 3D plot showing the
    uncertainty everywhere across the imager and at various distances. This
    contains a lot of information, but that makes it challenging to
    interpret

    - With --vs-distance-at we evaluate the uncertainty along an observation
    ray mapping to a single pixel. We show the uncertainty vs distances from
    the camera along this ray

    See mrcal.projection_uncertainty() for a full description of the
    computation performed here

OPTIONS
  POSITIONAL ARGUMENTS
      model                 Input camera model. If "-' is given, we read standard
                            input

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --observations-xydist
                            If given, we operate in a different mode: we make a 3D
                            plot of uncertainties and chessboard observations
                            instead of the 2D uncertainty contours we get
                            normally. This is exclusive with --vs-distance-at,
                            --distance, --isotropic, --observations.
      --vs-distance-at VS_DISTANCE_AT
                            If given, we operate in a different mode: we look at
                            the projection uncertainty at one pixel, at different
                            distances along that observation ray. This is
                            different from the 2D uncertainty contours we get
                            normally. This option takes a single argument: the
                            "X,Y" pixel coordinate we care about, or "center" to
                            look at the center of the imager or "centroid" to look
                            at the center of the calibration-time chessboards.
                            This is exclusive with --observations-xydist and
                            --gridn and --distance and --observations and --cbmax
      --gridn GRIDN GRIDN   How densely we should sample the imager. By default we
                            use a 60x40 grid (or a (15,10) grid if --observations-
                            xydist)
      --distance DISTANCE   By default we display the projection uncertainty
                            infinitely far away from the camera. If we want to
                            look closer in, the desired observation distance can
                            be given in this argument
      --isotropic           By default I display the expected value of the
                            projection error in the worst possible direction of
                            this error. If we want to plot the RMS of the worst
                            and best directions, pass --isotropic. If we assume
                            the errors will apply evenly in all directions, then
                            we can use this metric, which is potentially easier to
                            compute
      --observations        If given, I show where the chessboard corners were
                            observed at calibration time. This should correspond
                            to the low-uncertainty regions.
      --cbmax CBMAX         Maximum range of the colorbar
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --hardcopy HARDCOPY   Write the output to disk, instead of an interactive
                            plot
      --set SET             Extra 'set' directives to gnuplotlib. Can be given
                            multiple times
      --unset UNSET         Extra 'unset' directives to gnuplotlib. Can be given
                            multiple times


#+END_EXAMPLE
** mrcal-show-projection-diff
#+BEGIN_EXAMPLE
NAME
    mrcal-show-projection-diff - Visualize the difference in projection
    between N models

SYNOPSIS
      $ mrcal-show-projection-diff before.cameramodel after.cameramodel
      ... a plot pops up showing how these two models differ in their projections

DESCRIPTION
    It is often useful to compare the projection behavior of two camera
    models. For instance, one may want to validate a calibration by
    comparing the results of two different chessboard dances. Or one may
    want to evaluate the stability of the intrinsics in response to
    mechanical or thermal stresses. This tool makes these comparisons, and
    produces a visualization of the results.

    In the most common case we're given exactly 2 models to compare, and we
    show the xy differences in projection at each point (as a heat map or as
    a vector field). If we're given more than 2 models, we show the STANDARD
    DEVIATION of all the differences instead (as a heat map).

    How are we computing the differences? The details are in the docstring
    of mrcal.projection_diff(). Broadly, we do this:

    - grid the imager - unproject each point in the grid from one camera to
    produce a world point - apply a transformation we compute to match up
    the two camera geometries - reproject the transformed points to the
    other camera - look at the resulting pixel difference in the
    reprojection

    When looking at multiple cameras, their lens intrinsics differ. Less
    obviously, the position and orientation of the camera coordinate system
    in respect to the physical camera housing differ also. These geometric
    uncertainties are baked into the intrinsics. So when we project "the
    same world point" into both cameras, we must apply a geometric
    transformation because we want to be comparing projections of world
    points (relative to the camera housing), not projections relative to the
    (floating) camera coordinate systems. This transformation is unknown,
    but we can estimate it in one of several ways:

    - If we KNOW that there is no geometric difference between our cameras,
    and we thus should look at the intrinsics differences only, we assuming
    that implied_Rt10 = identity. Indicate this case by passing --radius 0

    - Otherwise, we fit projections across the imager: the "right"
    transformation would result in apparent low projection differences in a
    wide area.

    This fitted transformation is computed by
    implied_Rt10__from_unprojections(), and some details of its operation
    are significant:

    - The imager area we use for the fit - Which world points we're looking
    at

    In most practical usages, we would not expect a good fit everywhere in
    the imager: areas where no chessboards were observed will not fit well,
    for instance. From the point of view of the fit we perform, those
    ill-fitting areas should be treated as outliers, and they should NOT be
    a part of the solve. How do we specify the well-fitting area? The best
    way is to use the model uncertainties: these can be used to emphasize
    the confident regions of the imager. This is the default behavior. If
    uncertainties aren't available, or if we want a faster solve, pass
    --no-uncertainties. The well-fitting region can then be passed using
    --where and --radius to indicate the circle in the imager we care about.

    If using uncertainties then we utilize all the data in the imager by
    default. if --no-uncertainties, then the defaults are to use a more
    reasonable circle of radius min(width,height)/6 at the center of the
    imager. Usually this is sufficiently correct, and we don't need to mess
    with it. If we aren't guided to the correct focus region, the
    implied-by-the-intrinsics solve will try to fit lots of outliers, which
    would result in an incorrect transformation, which in turn would produce
    overly-high reported diffs. A common case when this happens is if the
    chessboard observations used in the calibration were concentrated to the
    side of the image (off-center), no uncertainties were used, and --where
    was not pointed to that area.

    Unlike the projection operation, the diff operation is NOT invariant
    under geometric scaling: if we look at the projection difference for two
    points at different locations along a single observation ray, there will
    be a variation in the observed diff. This is due to the geometric
    difference in the two cameras. If the models differed only in their
    intrinsics parameters, then this variation would not appear. Thus we
    need to know how far from the camera to look, and this is specified by
    --distance. By default we look out to infinity. If we care about the
    projection difference at some other distance, pass that here. Generally
    the most confident distance will be where the chessboards were observed
    at calibration time.

OPTIONS
  POSITIONAL ARGUMENTS
      models                Camera models to diff

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --gridn GRIDN GRIDN   How densely we should sample the imager. By default we
                            use a 60x40 grid
      --distance DISTANCE   By default we compute the implied transformation for
                            points infinitely far away from the camera. If we want
                            to look closer in, the desired observation distance
                            can be given in this argument. We can also fit
                            multiple distances at the same time by passing them
                            here in a comma-separated, whitespace-less list. If
                            multiple distances are given, we fit the implied-by-
                            the-intrinsics transformation using ALL the distances,
                            but we display the best-fitting difference for each
                            point. Only one distance is supported if
                            --vectorfield. Multiple distances are especially
                            useful if we have uncertainties: the most confident
                            distance will be found, and displayed.
      --where WHERE WHERE   Center of the region of interest for this diff. It is
                            usually impossible for the models to match everywhere,
                            but focusing on a particular area can work better. The
                            implied transformation will be fit to match as large
                            as possible an area centered on this argument. If
                            omitted, we will focus on the center of the imager
      --radius RADIUS       Radius of the region of interest. If ==0, we do NOT
                            fit an implied transformation at all. If omitted or
                            <0, we use a "reasonable" value: the whole imager if
                            we're using uncertainties, or min(width,height)/6 if
                            --no-uncertainties. To fit with data across the whole
                            imager in either case, pass in a very large radius
      --observations        If given, I show where the chessboard corners were
                            observed at calibration time. These should correspond
                            to the low-diff regions.
      --cbmax CBMAX         Maximum range of the colorbar
      --extratitle EXTRATITLE
                            Extra title string for the plot
      --vectorfield         Plot the diff as a vector field instead of as a heat
                            map. The vector field contains more information
                            (magnitude AND direction), but is less clear at a
                            glance
      --vectorscale VECTORSCALE
                            If plotting a vectorfield, scale all the vectors by
                            this factor. Useful to improve legibility if the
                            vectors are too small to see
      --no-uncertainties    By default we use the uncertainties in the model to
                            weigh the fit. This will focus the fit on the
                            confident region in the models without --where or
                            --radius. The computation will run faster with --no-
                            uncertainties, but the default --where and --radius
                            may need to be adjusted
      --hardcopy HARDCOPY   Write the output to disk, instead of making an
                            interactive plot
      --set SET             Extra 'set' directives to gnuplotlib. Can be given
                            multiple times
      --unset UNSET         Extra 'unset' directives to gnuplotlib. Can be given
                            multiple times


#+END_EXAMPLE
** mrcal-reproject-points
#+BEGIN_EXAMPLE
NAME
    mrcal-reproject-points - Reprojects pixel observations from one model to
    another

SYNOPSIS
      $ < points-in.vnl
        mrcal-reproject-points
          from.cameramodel to.cameramodel
        > points-out.vnl

DESCRIPTION
    This tool takes a set of pixel observations of points captured by one
    camera model, and transforms them into observations of the same points
    captured by another model. This is similar to mrcal-reproject-image, but
    acts on discrete points, rather than on whole images. The two sets of
    intrinsics are always used. The translation component of the extrinsics
    is always ignored; the rotation is ignored as well if --intrinsics-only.

    This allows one to combine multiple image-processing techniques that
    expect different projections. For instance, planes projected using a
    pinhole projection have some nice properties, and we can use those after
    running this tool.

    The input data comes in on standard input, and the output data is
    written to standard output. Both are vnlog data: human-readable text
    with 2 columns: x and y pixel coords. Comments are allowed, and start
    with the '#' character.

OPTIONS
  POSITIONAL ARGUMENTS
      model-from         Camera model for the input points. If "-' is given, we read
                         standard input
      model-to           Camera model for the output points. If "-' is given, we read
                         standard input

  OPTIONAL ARGUMENTS
      -h, --help         show this help message and exit
      --intrinsics-only  By default, the relative camera rotation is used in the
                         transformation. If we want to use the intrinsics ONLY, pass
                         --intrinsics-only. Note that relative translation is ALWAYS
                         ignored


#+END_EXAMPLE
** mrcal-reproject-image
#+BEGIN_EXAMPLE
NAME
    mrcal-reproject-image - Remaps a captured image into another camera
    model

SYNOPSIS
      ### To "undistort" images to reproject to a pinhole projection
      $ mrcal-reproject-image --to-pinhole
          camera0.cameramodel
          image*.jpg
      Wrote image0-pinhole.jpg
      Wrote image1-pinhole.jpg
      ...

      ### To reproject images from one lens model to another
      $ mrcal-reproject-image
          camera0.cameramodel camera1.cameramodel
          image*.jpg
      Wrote image0-reprojected.jpg
      Wrote image1-reprojected.jpg
      Wrote image2-reprojected.jpg
      ...

      ### To reproject two sets of images to a common pihole projection
      $ mrcal-reproject-image --to-pinhole
          camera0.cameramodel camera1.cameramodel
          'image*-cam0.jpg' 'image*-cam1.jpg'
      Wrote image0-reprojected.jpg
      Wrote image1-reprojected.jpg
      Wrote image2-reprojected.jpg
      ...

DESCRIPTION
    This tool takes image(s) of a scene captured by one camera model, and
    produces image(s) of the same scene, as it would appear if captured by a
    different model, taking into account both the different lens parameters
    and geometries. This is similar to mrcal-reproject-points, but acts on a
    full image, rather than a discrete set of points.

    There are several modes of operation, depending on how many camera
    models are given, and whether --to-pinhole is given, and whether
    --plane-n,--plane-d are given.

    To "undistort" (remap to a pinhole projection) a set of images captured
    using a particular camera model, invoke this tool like this:

      mrcal-reproject-image
        --to-pinhole
        model0.cameramodel image*.jpg

    Each of the given images will be reprojected, and written to disk as
    "image....-reprojected.jpg". The pinhole model used for the reprojection
    will be written to standard output.

    To remap images of a scene captured by model0 to images of the same
    scene captured by model1, do this:

      mrcal-reproject-image
        model0.cameramodel model1.cameramodel image*.jpg

    Each of the given images will be reprojected, and written to disk as
    "image....-reprojected.jpg". Nothing will be written to standard output.
    By default, the rotation component of the relative extrinsics between
    the two models is used in the reprojection. To ignore it, pass
    --intrinsics-only. Relative translation is always ignored. The usual use
    case is to validate the relative intrinsics and extrinsics in two
    models. If you have a pair of models and a pair of observed images, you
    can compute the reprojection, and compare the reprojection-to-model1 to
    images that were actually captured by model1. If the intrinsics and
    extrinsics were correct, then the two images would line up exactly for
    objects at infinity (where the translation=0 assumption is correct).
    Computing this reprojection map is often very slow. But if the use case
    is comparing two sets of captured images, the next, much faster
    invocation method can be used.

    To remap images of a scene captured by model0 and images of the same
    scene captured by model1 to a common pinhole projection, do this:

      mrcal-reproject-image
        --to-pinhole
        model0.cameramodel model1.cameramodel 'image*-cam0.jpg' 'image*-cam1.jpg'

    A pinhole model is constructed that has the same extrinsics as model1,
    and both sets of images are reprojected to this model. This is similar
    to the previous mode, but since we're projection to a pinhole mode, this
    computes much faster. The generated pinhole model is written to standard
    output.

    Finally instead of reprojecting to match up images of objects at
    infinity, it is possible to reproject to match up images of arbitrary
    planes. This can be done by a command like this:

      mrcal-reproject-image
        --to-pinhole
        --plane-n 1.1 2.2 3.3
        --plane-d 4.4
        model0.cameramodel model1.cameramodel 'image*-cam0.jpg' 'image*-cam1.jpg'

    This maps observations of a given plane in camera0 coordinates to where
    this plane would be observed in camera1 coordinates. This uses ALL the
    intrinsics, extrinsics and the plane representation. If all of these are
    correct, the observations of this plane would line up exactly in the
    remapped-camera0 image and the camera1 image. The plane is represented
    in camera0 coordinates by a normal vector given by --plane-n, and the
    distance to the normal given by plane-d. The plane is all points p such
    that inner(p,planen) = planed. planen does not need to be normalized.

    If 2 camera models are given, we use the rotation component of the
    extrinsics, unless --intrinsics-only. The translation is always ignored.

    If --to-pinhole, then we generate a pinhole model, that is written to
    standard output. By default, the focal length of this pinhole model is
    the same as that of the input model. The "zoom" level of this pinhole
    model can be adjusted by passing --scale-focal SCALE, or more precisely
    by passing --fit. --fit takes an argument that is one of

    - "corners": make sure all of the corners of the original image remain
    in-bounds of the pinhole projection

    - "centers-horizontal": make sure the extreme left-center and
    right-center points in the original image remain in-bounds of the
    pinhole projection

    - "centers-vertical": make sure the extreme top-center and bottom-center
    points in the original image remain in-bounds of the pinhole projection

    - A list of pixel coordinates x0,y0,x1,y1,x2,y2,.... The focal-length
    will be chosen to fit all of the given points

    By default, the resolution of the generated pinhole model is the same as
    the resolution of the input model. This can be adjusted by passing
    --scale-image. For instance, passing "--scale-image 0.5" will generate a
    pinhole model and images that are half the size of the input images, in
    both the width and height.

    The output image(s) are written into the same directory as the input
    image(s), with annotations in the filename. This tool will refuse to
    overwrite any existing files unless --force is given.

    It is often desired to apply transformations to lots of images in bulk.
    To make this go faster, this tool supports the -j JOBS option. This
    works just like in Make: the work will be parallelized amoung JOBS
    simultaneous processes. Unlike make, the JOBS value must be specified.

OPTIONS
  POSITIONAL ARGUMENTS
      model-from            Camera model for the FROM image(s). If "-' is given, we read
                            standard input
      model-to-and-image-globs
                            Optionally, the camera model for the TO image. Followed, by
                            the from/to image globs. See the mrcal-reproject-image
                            documentation for the details.

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --to-pinhole          If given, we reproject the images to a pinhole model that's
                            generated off the MODEL-FROM and --fit, --scale-focal,
                            --scale-image. The generated pinhole model is written to the
                            standard output
      --intrinsics-only     If two camera models are given, then by default the relative
                            camera rotation is used in the transformation. If we want to
                            use the intrinsics ONLY, pass --intrinsics-only. Note that
                            relative translation is ALWAYS ignored
      --fit FIT             If we generate a target pinhole model (if --to-pinhole is
                            given) then we can choose the focal length of the target
                            model. This is a "zoom" operation. By default just use
                            whatever value model-from has. Or we scale it by the value
                            given in --scale-focal. Or we use --fit to scale the focal
                            length intelligently. The --fit argument could be one of
                            ("corners", "centers-horizontal", "centers-vertical"), or the
                            argument could be given as a list of points
                            x0,y0,x1,y1,x2,y2,.... The focal length scale woudl then be
                            chosen to zoom in as far as possible, while fitting all of
                            these points
      --scale-focal SCALE_FOCAL
                            If we generate a target pinhole model (if --to-pinhole is
                            given) then we can choose the focal length of the target
                            model. This is a "zoom" operation. By default just use
                            whatever value model-from has. Or we scale it by the value
                            given in --scale-focal. Or we use --fit to scale the focal
                            length intelligently.
      --scale-image SCALE_IMAGE
                            If we generate a target pinhole model (if --to-pinhole is
                            given) then we can choose the dimensions of the output image.
                            By default we use the dimensions of model-from. If --scale-
                            image is given, we use this value to scale the imager
                            dimensions of model-from. This parameter changes the
                            RESOLUTION of the output, unlike --scale-focal, which ZOOMS
                            the output
      --plane-n PLANE_N PLANE_N PLANE_N
                            We're reprojecting a plane. The normal vector to this plane
                            is given here, in from-camera coordinates. The normal does
                            not need to be normalized; any scaling is compensated in
                            planed. The plane is all points p such that inner(p,planen) =
                            planed
      --plane-d PLANE_D     We're reprojecting a plane. The distance-along-the-normal to
                            the plane, in from-camera coordinates is given here. The
                            plane is all points p such that inner(p,planen) = planed
      --outdir OUTDIR       Directory to write the output images into. If omitted, we
                            write the output images to the same directory as the input
                            images
      --force, -f           By default existing files are not overwritten. Pass --force
                            to overwrite them without complaint
      --jobs JOBS, -j JOBS  parallelize the processing JOBS-ways. This is like Make,
                            except you're required to explicitly specify a job count.


#+END_EXAMPLE
** mrcal-graft-models
#+BEGIN_EXAMPLE
NAME
    mrcal-graft-models - Combines the intrinsics of one cameramodel with the
    extrinsics of another

SYNOPSIS
      $ mrcal-graft-models
          intrinsics.cameramodel
          extrinsics.cameramodel
          > joint.cameramodel
      Merged intrinsics from 'intrinsics.cameramodel' with extrinsics from
      'exrinsics.cameramodel'

DESCRIPTION
    This tool combines intrinsics and extrinsics from different sources into
    a single model. The output is written to standard output.

    By default, the operation is very simple: we combine the intrinsics from
    one model with the extrinsics of another. A common use case is a system
    where the intrinsics are calibrated prior to moving the cameras to their
    final location, and then computing the extrinsics separately after the
    cameras are moved.

    If we have computed such a combined model, and we decide to recompute
    the intrinsics afterwards, we can graft the new intrinsics to the
    previous extrinsics. However, this won't be a drop-in replacement for
    the previous model, since the intrinsics come with an implied geometric
    transformation, which will be different in the new intrinsics. If the
    "extrinsics" models contains the old intrinsics, then this tool is able
    to compute the relative implied transformation, and to apply it to the
    extrinsics. As a result, on average, the projection of any world point
    ends up at the same pixel coordinate as before.

    The implied transformation logic is controlled by a number of
    commandline arguments, same ones as used by the
    mrcal-show-projection-diff tool. The only difference in options is that
    THIS tool uses --radius 0 by default, so we do not compute or apply the
    implied transformation unless asked. Pass --radius with a non-zero
    argument to compute and apply the implied transformation.

OPTIONS
  POSITIONAL ARGUMENTS
      intrinsics           Input camera model for the intrinsics. If "-' is given, we
                           read standard input
      extrinsics           Input camera model for the extrinsics. If "-' is given, we
                           read standard input

  OPTIONAL ARGUMENTS
      -h, --help           show this help message and exit
      --gridn GRIDN GRIDN  Used if we're computing the implied-by-the-intrinsics
                           transformation. How densely we should sample the imager. By
                           default we use a 60x40 grid
      --distance DISTANCE  Used if we're computing the implied-by-the-intrinsics
                           transformation. By default we compute the implied
                           transformation for points infinitely far away from the camera.
                           If we want to look closer in, the desired observation distance
                           can be given in this argument. We can also fit multiple
                           distances at the same time by passing them here in a comma-
                           separated, whitespace-less list
      --where WHERE WHERE  Used if we're computing the implied-by-the-intrinsics
                           transformation. Center of the region of interest used for the
                           transformatoin fit. It is usually impossible for the models to
                           match everywhere, but focusing on a particular area can work
                           better. The implied transformation will be fit to match as
                           large as possible an area centered on this argument. If
                           omitted, we will focus on the center of the imager
      --radius RADIUS      Used if we're computing the implied-by-the-intrinsics
                           transformation. Radius of the region of interest. If ==0, we
                           do NOT fit an implied transformation at all. If omitted or <0,
                           we use a "reasonable" value: the whole imager if we're using
                           uncertainties, or min(width,height)/6 if --no-uncertainties.
                           To fit with data across the whole imager in either case, pass
                           in a very large radius
      --no-uncertainties   Used if we're computing the implied-by-the-intrinsics
                           transformation. By default we use the uncertainties in the
                           model to weigh the fit. This will focus the fit on the
                           confident region in the models without --where or --radius.
                           The computation will run faster with --no-uncertainties, but
                           the default --where and --radius may need to be adjusted


#+END_EXAMPLE
** mrcal-to-cahvor
#+BEGIN_EXAMPLE
NAME
    mrcal-to-cahvor - Converts model to the cahvor file format

SYNOPSIS
      $ mrcal-to-cahvor model1.cameramodel model2.cameramodel
      Wrote model1.cahvor
      Wrote model2.cahvor

DESCRIPTION
    This tool converts a given model to the cahvor file format. No changes
    to the content are made; this is purely a format converter. Model
    filenames are given on the commandline. Output is written to the same
    directory, with the same filename, but with a .cahvor extension.

    If the model is omitted or given as "-", the input is read from standard
    input, and the output is written to standard output

OPTIONS
  POSITIONAL ARGUMENTS
      model            Input camera model

  OPTIONAL ARGUMENTS
      -h, --help       show this help message and exit
      --force, -f      By default existing files are not overwritten. Pass --force to
                       overwrite them without complaint
      --outdir OUTDIR  Directory to write the output models into. If omitted, we write
                       the output models to the same directory as the input models


#+END_EXAMPLE
** mrcal-to-cameramodel
#+BEGIN_EXAMPLE
NAME
    mrcal-to-cameramodel - Converts model to the cameramodel file format

SYNOPSIS
      $ mrcal-to-cameramodel model1.cahvor model2.cahvor
      Wrote model1.cameramodel
      Wrote model2.cameramodel

DESCRIPTION
    This tool converts a given model to the cameramodel file format. No
    changes to the content are made; this is purely a format converter.
    Model filenames are given on the commandline. Output is written to the
    same directory, with the same filename, but with a .cameramodel
    extension.

    If the model is omitted or given as "-", the input is read from standard
    input, and the output is written to standard output

OPTIONS
  POSITIONAL ARGUMENTS
      model            Input camera model

  OPTIONAL ARGUMENTS
      -h, --help       show this help message and exit
      --force, -f      By default existing files are not overwritten. Pass --force to
                       overwrite them without complaint
      --outdir OUTDIR  Directory to write the output models into. If omitted, we write
                       the output models to the same directory as the input models


#+END_EXAMPLE
** mrcal-show-geometry
#+BEGIN_EXAMPLE
NAME
    mrcal-show-geometry - Displays the calibration-time geometry: the
    cameras and the observed objects

SYNOPSIS
      $ mrcal-show-geometry *.cameramodel
      ... a plot pops up showing the camera arrangement

DESCRIPTION
    This tool visualizes the relative geometry between several cameras and
    the calibration objects they observed when computing the calibration.

OPTIONS
  POSITIONAL ARGUMENTS
      models                Camera models to visualize. Any N cameras can be given

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --scale-axes SCALE_AXES
                            Scale for the camera axes. By default these are 1.0m long
      --title TITLE         Title string for the plot
      --hardcopy HARDCOPY   Write the output to disk, instead of making an interactive plot
      --hide-boards         If given, do not draw the calibration object observations. By default we do draw them if they are available
      --transforms TRANSFORMS
                            Optional transforms.txt. This is a legacy file representing an extra transformation for each camera. Most usages will omit this
      --set SET             Extra 'set' directives to pass to gnuplotlib. May be given multiple times
      --unset UNSET         Extra 'unset' directives to pass to gnuplotlib. May be given multiple times


#+END_EXAMPLE
** mrcal-show-valid-intrinsics-region
#+BEGIN_EXAMPLE

#+END_EXAMPLE
** mrcal-is-within-valid-intrinsics-region
#+BEGIN_EXAMPLE
NAME
    mrcal-is-within-valid-intrinsics-region - Reports which input points lie
    within the valid-intrinsics region

SYNOPSIS
      $ < points-in.vnl
        mrcal-is-within-valid-intrinsics-region --cols-xy x y
          camera.cameramodel
        > points-annotated.vnl

DESCRIPTION
    mrcal camera models may have an estimate of the region of the imager
    where the intrinsics are trustworthy (originally computed with a
    low-enough error and uncertainty). When using a model, we may want to
    process points that fall outside of this region differently from points
    that fall within this region. This tool augments an incoming vnlog with
    a new column, indicating whether each point does or does not fall within
    the region.

    The input data comes in on standard input, and the output data is
    written to standard output. Both are vnlog data: a human-readable table
    of ascii text. The names of the x and y columns in the input are given
    in the required --cols-xy argument. The output contains all the columns
    from the input, with an extra column appended at the end, containing the
    results. The name of this column can be specified with --col-output, but
    this can be omitted if the default 'is-within-valid-intrinsics-region'
    is acceptable.

OPTIONS
  POSITIONAL ARGUMENTS
      model                 Camera model

  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --cols-xy COLS_XY COLS_XY
                            The names of the columns in the input containing the x and y
                            pixel coordinates respectively. This is required
      --col-output COL_OUTPUT
                            The name of the column to append in the output. This is
                            optional; a reasonable default will be used if omitted


#+END_EXAMPLE
** mrcal-cull-corners
#+BEGIN_EXAMPLE
NAME
    mrcal-cull-corners - Filters a corners.vnl on stdin to cut out some
    points

SYNOPSIS
      $ < corners.vnl mrcal-cull-corners --cull-left-of 1000 > corners.culled.vnl

DESCRIPTION
    This tool reads a set of corner detections on stdin, throws some of them
    out, and writes the result to stdout. This is useful for testing and
    evaluating the performance of the mrcal calibration tools.

    The specific operation of this tool is defined on which --cull-...
    option is given. Exactly one is required:

      --cull-left-of X: throw away all corner observations to the left of the given
        X coordinate

      --cull-rad-off-center D: throw away all corner observations further than D
        away from the imager center. --imagersize must be given also so that we know
        where the imager center is

      --cull-random-observations-ratio R: throws away a ratio R object observations
        at random. To throw out half of all object observations, pass R = 0.5.
        --object-width-n and --object-height-n are then required to make the parsing
        work

    --cull-left-of X and --cull-rad-off-center throw out individual points.
    This is done by keeping the point in the output data strem, but setting
    its decimation level to '-'. The downstream tools then know to ignore
    those points

    --cull-random-observations-ratio throws out whole object observations,
    not just individual points. These removed observations do not appear in
    the output data stream at all

OPTIONS
  OPTIONAL ARGUMENTS
      -h, --help            show this help message and exit
      --object-width-n OBJECT_WIDTH_N
                            How many points the calibration board has per horizontal side. This is required if --cull-random-observation-ratio
      --object-height-n OBJECT_HEIGHT_N
                            How many points the calibration board has per vertical side. If omitted, I assume a square object and use the same value as --object-width-n
      --imagersize IMAGERSIZE IMAGERSIZE
                            Size of the imager. This is required if --cull-rad-off-center
      --cull-left-of CULL_LEFT_OF
                            Throw out all observations with x < the given value. Exclusive with the other --cull-... options
      --cull-rad-off-center CULL_RAD_OFF_CENTER
                            Throw out all observations with dist_from_center > the given value. Exclusive with the other --cull-... options
      --cull-random-observations-ratio CULL_RANDOM_OBSERVATIONS_RATIO
                            Throw out a random number of board observations. The ratio of observations is given as the argument. 1.0 = throw out ALL the observations; 0.0 = throw
                            out NONE of the observations. Exclusive with the other --cull-... options


#+END_EXAMPLE

* REPOSITORY

https://github.jpl.nasa.gov/maritime-robotics/mrcal/

* AUTHOR

Dima Kogan (=Dmitriy.Kogan@jpl.nasa.gov=)

* LICENSE AND COPYRIGHT

All of this is currently proprietary. Do not distribute outside of JPL

Copyright 2016-2018 California Institute of Technology
