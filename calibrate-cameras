#!/usr/bin/python2

r'''Calibrate some synchronized cameras

Synopsis:

  $ calibrate-cameras --focal 2000 --outdir /tmp --object-spacing 0.01 --object-width-n 10 '/tmp/left*.png' '/tmp/right*.png'


  ... lots of output as the solve runs ...
  done with DISTORTION_CAHVOR, optimizing DISTORTIONS again
  Wrote /tmp/camera0-0.cahvor
  Wrote /tmp/camera0-1.cahvor


This tools uses the generic mrcal platform to solve this common specific
problem. Run --help for the list of commandline options

'''

import sys
import numpy as np
import numpysane as nps
import cv2
import re
import argparse
import os
import fnmatch
import re
import subprocess
import pipes
import heapq
from tempfile import mkstemp
import shutil
import glob

from mrcal import cahvor
from mrcal import utils
from mrcal import poseutils
from mrcal import projections
from mrcal import cameramodel
import mrcal.optimizer as optimizer




def parse_args():
    parser = \
        argparse.ArgumentParser(description = __doc__,
                                formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('--focal',
                        type=float,
                        default=1970,
                        required=True,
                        help='Initial estimate of the focal length, in pixels')
    parser.add_argument('--outdir',
                        type=lambda d: d if os.path.isdir(d) else \
                                parser.error("--outdir requires an existing directory as the arg, but got '{}'".format(d)),
                        default='.',
                        help='Directory for the output camera models')
    parser.add_argument('--object-spacing',
                        required=True,
                        type=float,
                        help='Width of each square in the calibration board, in meters')
    parser.add_argument('--object-width-n',
                        type=int,
                        required=True,
                        help='How many points the calibration board has per side')
    parser.add_argument('--distortion-model',
                        required=False,
                        default='DISTORTION_OPENCV4',
                        help='''Which distortion model we're using. By default I use DISTORTION_OPENCV4''')
    parser.add_argument('--seed',
                        required=False,
                        type=lambda f: f if os.path.isfile(f) else \
                                parser.error("--seed requires an existing, readable file as the arg, but got '{}'".format(f)),
                        help='''Model to use as a seed. This works ONLY if we have exactly one camera''')
    parser.add_argument('--jobs', '-j',
                        type=int,
                        default=1,
                        help='''How much parallelization we want. Like GNU make. Affects only the chessboard
                        corner finder. If we are reading a cache file, this does nothing''')
    parser.add_argument('--dots-cache',
                        type=lambda f: f if os.path.isfile(f) or not os.path.isdir(f) else \
                                parser.error("--dots-cache requires an existing, readable file as the arg or a non-existing path, but got '{}'".format(f)),
                        required=False,
                        help='Path to read corner-finder data from or (if path does not exist) to write data to')

    parser.add_argument('--muse-extrinsics',
                        action='store_true',
                        required=False,
                        default=False,
                        help='''Apply MUSE's non-identity rotation for camera0''')

    parser.add_argument('--explore',
                        action='store_true',
                        required=False,
                        default=False,
                        help='''After the solve open an interactive shell to examine the solution''')

    parser.add_argument('images',
                        type=str,
                        nargs='+',
                        help='''A glob-per-camera for the images. Include a glob for each camera. It is
                        assumed that the image filenames in each glob are of of
                        the form xxxNNNyyy where xxx and yyy are common to all
                        images in the set, and NNN varies. This NNN is a frame
                        number, and identical frame numbers across different
                        globs signify a time-synchronized observation. I.e. you
                        can pass 'left*.jpg' and 'right*.jpg' to find images
                        'left0.jpg', 'left1.jpg', ..., 'right0.jpg',
                        'right1.jpg', ...''')



    return parser.parse_args()

def get_observations(Nw, Nh, globs, dots_vnl=None, jobs=1, exclude=set()):
    r'''Computes the point observations and returns them in a usable form

    We are given globs of images (one glob per camera), where the filenames
    encode the instantaneous frame numbers. This function invokes the chessboard
    finder to compute the point coordinates, and returns a tuple

      observations, indices_frame_camera, files_sorted

    where observations is an (N,object-width-n,object-width-n,2) array
    describing N board observations where the board has dimensions
    (object-width-n,object-width-n) and each point is an (x,y) pixel observation

    indices_frame_camera is an (N,2) array of integers where each observation is
    (index_frame,index_camera)

    files_sorted is a list of paths of images corresponding to the observations

    '''

    def get_dot_observations(Nw, Nh, globs, dots_vnl, exclude=set()):
        r'''Invokes mrgingham to get dot observations

        Returns a dict mapping from filename to a numpy array with a full grid
        of dot observations. If no grid was observed in a particular image, the
        relevant dict entry is empty

        The dots_vnl argument is for caching corner-finder results. This can be
        None if we want to ignore this. Otherwise, this is treated as a path to
        a file on disk. If this file exists:

            The corner coordinates are read from this file instead of being
            computed. We don't need to actually have the images stored on disk.

        If this file does not exist:

            We process the images to compute the corner coordinates. Before we
            compute the calibration off these coordinates, we create the cache
            file and store this data there. Thus a subsequent identical
            invocation of calibrate-cameras will see this file as existing, and
            will automatically use the data it contains instead of recomputing
            the corner coordinates
        '''

        Ncameras = len(globs)
        files_per_camera = []
        for i in xrange(Ncameras):
            files_per_camera.append([])

        def accum_files(f):
            for i_camera in xrange(Ncameras):
                if fnmatch.fnmatch(f, globs[i_camera]):
                    files_per_camera[i_camera].append(f)
                    return True
            return False


        pipe_dots_write_fd          = None
        pipe_dots_write_tmpfilename = None
        if dots_vnl is not None and os.path.isdir(dots_vnl):
            raise Exception("Given cache path '{}' is a directory. Must be a file or must not exist". \
                            format(dots_vnl))
        if dots_vnl is None or not os.path.isfile(dots_vnl):
            # Need to compute the dot coords. And maybe need to save them into a
            # cache file too
            if Nw != 10 or Nh != 10:
                raise Exception("mrgingham currently accepts ONLY 10x10 grids")

            args_mrgingham = ['mrgingham_from_image', '--chessboard', '--blur', '3', '--clahe', '--jobs',
                              str(jobs)]
            args_mrgingham.extend(globs)

            sys.stderr.write("Computing chessboard corners by running:\n   {}\n". \
                             format(' '.join(pipes.quote(s) for s in args_mrgingham)))
            if dots_vnl is not None:
                # need to save the dots into a cache. I want to do this
                # atomically: if the dot-finding is interrupted I don't want to
                # be writing incomplete results, so I write to a temporary file
                # and then rename when done
                pipe_dots_write_fd,pipe_dots_write_tmpfilename = mkstemp('.vnl')
                sys.stderr.write("Will save corners to '{}'\n".format(dots_vnl))

            dots_output = subprocess.Popen(args_mrgingham, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            pipe_dots_read = dots_output.stdout
            computing_dots = True
        else:
            # Have an existing cache file. Just read it
            pipe_dots_read = open(dots_vnl, 'r')
            computing_dots = False


        mapping = {}
        context = {'f':    '',
                   'grid': np.array(())}

        def finish():
            if context['grid'].size:
                if Nw*Nh != context['grid'].size/2:
                    raise Exception("File '{}' expected to have {}*{}={} elements, but got {}". \
                                    format(context['f'], Nw,Nh,Nw*Nh, context['grid'].size/2))
                if context['f'] not in exclude:
                    if accum_files(context['f']):
                        mapping[context['f']] = context['grid']
                context['f']    = ''
                context['grid'] = np.array(())

        for line in pipe_dots_read:
            if pipe_dots_write_fd is not None:
                os.write(pipe_dots_write_fd, line)

            if line[0] == '#':
                continue
            m = re.match('(\S+)\s+(.*?)$', line)
            if m is None:
                raise Exception("Unexpected line in the dots output: '{}'".format(line))
            if m.group(2)[:2] == '- ':
                finish()
                continue
            if context['f'] != m.group(1):
                finish()
                context['f'] = m.group(1)

            context['grid'] = nps.glue(context['grid'],
                                       np.fromstring(m.group(2), sep=' ', dtype=np.float),
                                       axis=-2)
        finish()

        if computing_dots:
            sys.stderr.write("Done computing chessboard corners\n")

            if dots_output.wait() != 0:
                err = dots_output.stderr.read()
                raise Exception("mrgingham_from_image failed: {}".format(err))
            if pipe_dots_write_fd is not None:
                os.close(pipe_dots_write_fd)
                shutil.move(pipe_dots_write_tmpfilename, dots_vnl)
        else:
            pipe_dots_read.close()

        return mapping,files_per_camera


    indices_frame_camera = np.array((), dtype=np.int32)
    observations         = np.array((), dtype=float)

    # basic logic is this:
    #   for frames:
    #       for cameras:
    #           if have observation:
    #               push observations
    #               push indices_frame_camera

    # inputs[camera][image] = (image_filename, frame_number)
    mapping_file_dots,files_per_camera = get_dot_observations(Nw, Nh, globs, dots_vnl, exclude)
    mapping_file_framecamera,_,_       = utils.get_mapping_file_framecamera(files_per_camera)

    # I create a file list sorted by frame and then camera. So my for(frames)
    # {for(cameras) {}} loop will just end up looking at these files in order
    files_sorted = sorted(mapping_file_dots.keys(), key=lambda f: mapping_file_framecamera[f][1])
    files_sorted = sorted(files_sorted,             key=lambda f: mapping_file_framecamera[f][0])

    i_observation = 0

    i_frame_last = None
    index_frame  = -1
    for f in files_sorted:
        # The frame indices I return are consecutive starting from 0, NOT the
        # original frame numbers
        i_frame,i_camera = mapping_file_framecamera[f]
        if i_frame_last == None or i_frame_last != i_frame:
            index_frame += 1
            i_frame_last = i_frame

        indices_frame_camera = nps.glue(indices_frame_camera,
                                        np.array((index_frame, i_camera), dtype=np.int32),
                                        axis=-2)
        observations = nps.glue(observations,
                                mapping_file_dots[f].reshape(Nh,Nw,2),
                                axis=-4)

    return observations, indices_frame_camera, files_sorted


def estimate_local_calobject_poses( indices_frame_camera, \
                                    dots, dot_spacing, focal, imagersizes,
                                    Nwant):
    r"""Estimates pose of observed object in a single-camera view

    Given observations, and an estimate of camera intrinsics (focal lengths,
    imager size) computes an estimate of the pose of the calibration object in
    respect to the camera for each frame. This assumes that all frames are
    independent and all cameras are independent. This assumes a pinhole camera.

    This function is a wrapper around the solvePnP() openCV call, which does all
    the work.

    The observations are given in a numpy array with axes:

      (iframe, idot_x, idot_y, idot2d_xy)

    So as an example, the observed pixel coord of the dot (3,4) in frame index 5
    is the 2-vector dots[5,3,4,:]

    Missing observations are given as negative pixel coords.

    This function returns an (Nobservations,4,3) array, with the observations
    aligned with the dots and indices_frame_camera arrays. Each observation
    slice is (4,3) in glue(R, t, axis=-2)

    """

    Nobservations = indices_frame_camera.shape[0]

    # this wastes memory, but makes it easier to keep track of which data goes
    # with what
    Rt_all = np.zeros( (Nobservations, 4, 3), dtype=float)

    full_object = utils.get_full_object(Nwant, Nwant, dot_spacing)

    for i_observation in xrange(Nobservations):

        i_camera   = indices_frame_camera[i_observation,1]
        imagersize = imagersizes[i_camera]
        camera_matrix = np.array((( focal, 0,        (imagersize[0] - 1)/2), \
                                  (        0, focal, (imagersize[1] - 1)/2), \
                                  (        0,        0,                 1)))
        d = dots[i_observation, ...]

        d = nps.clump( nps.glue(d, full_object, axis=-1), n=2)
        # d is (Nwant*Nwant,5); each row is an xy pixel observation followed by the xyz
        # coord of the point in the calibration object. I pick off those rows
        # where the observations are both >= 0. Result should be (N,5) where N
        # <= Nwant*Nwant
        i = (d[..., 0] >= 0) * (d[..., 1] >= 0)
        d = d[i,:]

        # copying because cv2.solvePnP() requires contiguous memory apparently
        observations = np.array(d[:,:2][..., np.newaxis])
        ref_object   = np.array(d[:,2:][..., np.newaxis])
        result,rvec,tvec = cv2.solvePnP(np.array(ref_object),
                                        np.array(observations),
                                        camera_matrix, None)
        if not result:
            raise Exception("solvePnP failed!")
        if tvec[2] <= 0:
            raise Exception("solvePnP says that tvec.z <= 0. Maybe needs a flip, but please examine this")

        Rt_all[i_observation, :, :] = poseutils.Rt_from_rt(nps.glue(rvec.ravel(), tvec.ravel(), axis=-1))


    return Rt_all

def estimate_camera_poses( calobject_poses_local_Rt, indices_frame_camera, \
                           dots, dot_spacing, Ncameras,
                           Nwant):
    r'''Estimate camera poses in respect to each other

    We are given poses of the calibration object in respect to each observing
    camera. We also have multiple cameras observing the same calibration object
    at the same time, and we have local poses for each. We can thus compute the
    relative camera pose from these observations.

    We have many frames that have different observations from the same set of
    fixed-relative-pose cameras, so we compute the relative camera pose to
    optimize the observations

    '''
    # I need to compute an estimate of the pose of each camera in the coordinate
    # system of camera0. This is only possible if there're enough overlapping
    # observations. For instance if camera1 has overlapping observations with
    # camera2, but neight overlap with camera0, then I can't relate camera1,2 to
    # camera0. However if camera2 has overlap with camera2, then I can compute
    # the relative pose of camera2 from its overlapping observations with
    # camera0. And I can compute the camera1-camera2 pose from its overlapping
    # data, and then transform to the camera0 coord system using the
    # previously-computed camera2-camera0 pose
    #
    # I do this by solving a shortest-path problem using Dijkstra's algorithm to
    # find a set of pair overlaps between cameras that leads to camera0. I favor
    # edges with large numbers of shared observed frames

    # list of camera-i to camera-0 transforms. I keep doing stuff until this
    # list is full of valid data
    Rt_0c = [None] * (Ncameras-1)

    def compute_pairwise_Rt(icam_to, icam_from):

        # I want to assume that icam_from > icam_to. If it's not true, compute the
        # opposite transform, and invert
        if icam_to > icam_from:
            Rt = compute_pairwise_Rt(icam_from, icam_to)
            return poseutils.invert_Rt(Rt)

        if icam_to == icam_from:
            raise Exception("Got icam_to == icam_from ( = {} ). This was probably a mistake".format(icam_to))

        # Now I KNOW that icam_from > icam_to


        Nobservations = indices_frame_camera.shape[0]

        # This is a hack. I look at the correspondence of camera0 to camera i for i
        # in 1:N-1. I ignore all correspondences between cameras i,j if i!=0 and
        # j!=0. Good enough for now
        full_object = utils.get_full_object(Nwant, Nwant, dot_spacing)

        A = np.array(())
        B = np.array(())

        # I traverse my observation list, and pick out observations from frames
        # that had data from both my cameras
        i_frame_last = -1
        d0  = None
        d1  = None
        Rt0 = None
        Rt1 = None
        for i_observation in xrange(Nobservations):
            i_frame_this,i_camera_this = indices_frame_camera[i_observation, ...]
            if i_frame_this != i_frame_last:
                d0  = None
                d1  = None
                Rt0 = None
                Rt1 = None
                i_frame_last = i_frame_this

            # The cameras appear in order. And above I made sure that icam_from >
            # icam_to, so I take advantage of that here
            if i_camera_this == icam_to:
                if Rt0 is not None:
                    raise Exception("Saw multiple camera{} observations in frame {}".format(i_camera_this,
                                                                                            i_frame_this))
                Rt0 = calobject_poses_local_Rt[i_observation, ...]
                d0  = dots[i_observation, ...]
            elif i_camera_this == icam_from:
                if Rt0 is None: # have camera1 observation, but not camera0
                    continue

                if Rt1 is not None:
                    raise Exception("Saw multiple camera{} observations in frame {}".format(i_camera_this,
                                                                                            i_frame_this))
                Rt1 = calobject_poses_local_Rt[i_observation, ...]
                d1  = dots[i_observation, ...]



                # d looks at one frame and has shape (Nwant,Nwant,7). Each row is
                #   xy pixel observation in left camera
                #   xy pixel observation in right camera
                #   xyz coord of dot in the calibration object coord system
                d = nps.glue( d0, d1, full_object, axis=-1 )

                # squash dims so that d is (Nwant*Nwant,7)
                d = nps.clump(d, n=2)

                ref_object = nps.clump(full_object, n=2)

                # # It's possible that I could have incomplete views of the
                # # calibration object, so I pull out only those point
                # # observations that have a complete view. In reality, I
                # # currently don't accept any incomplete views, and much outside
                # # code would need an update to support that. This doesn't hurt, however

                # # d looks at one frame and has shape (10,10,7). Each row is
                # #   xy pixel observation in left camera
                # #   xy pixel observation in right camera
                # #   xyz coord of dot in the calibration object coord system
                # d = nps.glue( d0, d1, full_object, axis=-1 )

                # # squash dims so that d is (100,7)
                # d = nps.transpose(nps.clump(nps.mv(d, -1, -3), n=2))

                # # I pick out those points that have observations in both frames
                # i = (d[..., 0] >= 0) * (d[..., 1] >= 0) * (d[..., 2] >= 0) * (d[..., 3] >= 0)
                # d = d[i,:]

                # # ref_object is (N,3)
                # ref_object = d[:,4:]

                A = nps.glue(A, nps.matmult( ref_object, nps.transpose(Rt0[:3,:])) + Rt0[3,:],
                             axis = -2)
                B = nps.glue(B, nps.matmult( ref_object, nps.transpose(Rt1[:3,:])) + Rt1[3,:],
                             axis = -2)

        return utils.align3d_procrustes(A, B)


    def compute_connectivity_matrix():
        r'''Returns a connectivity matrix of camera observations

        Returns a symmetric (Ncamera,Ncamera) matrix of integers, where each
        entry contains the number of frames containing overlapping observations
        for that pair of cameras

        '''

        camera_connectivity = np.zeros( (Ncameras,Ncameras), dtype=int )
        def finish_frame(i0, i1):
            for ic0 in xrange(i0, i1):
                for ic1 in xrange(ic0+1, i1+1):
                    camera_connectivity[indices_frame_camera[ic0,1], indices_frame_camera[ic1,1]] += 1
                    camera_connectivity[indices_frame_camera[ic1,1], indices_frame_camera[ic0,1]] += 1

        f_current       = -1
        i_start_current = -1

        for i in xrange(len(indices_frame_camera)):
            f,c = indices_frame_camera[i]
            if f < f_current:
                raise Exception("I'm assuming the frame indices are increasing monotonically")
            if f > f_current:
                # first camera in this observation
                f_current = f
                if i_start_current >= 0:
                    finish_frame(i_start_current, i-1)
                i_start_current = i
        finish_frame(i_start_current, len(indices_frame_camera)-1)
        return camera_connectivity


    shared_frames = compute_connectivity_matrix()

    class Node:
        def __init__(self, camera_idx):
            self.camera_idx    = camera_idx
            self.from_idx      = -1
            self.cost_to_node  = None

        def __lt__(self, other):
            return self.cost_to_node < other.cost_to_node

        def visit(self):
            '''Dijkstra's algorithm'''
            self.finish()

            for neighbor_idx in xrange(Ncameras):
                if neighbor_idx == self.camera_idx                  or \
                   shared_frames[neighbor_idx,self.camera_idx] == 0:
                    continue
                neighbor = nodes[neighbor_idx]

                if neighbor.visited():
                    continue

                cost_edge = Node.compute_edge_cost(shared_frames[neighbor_idx,self.camera_idx])

                cost_to_neighbor_via_node = self.cost_to_node + cost_edge
                if not neighbor.seen():
                    neighbor.cost_to_node = cost_to_neighbor_via_node
                    neighbor.from_idx     = self.camera_idx
                    heapq.heappush(heap, neighbor)
                else:
                    if cost_to_neighbor_via_node < neighbor.cost_to_node:
                        neighbor.cost_to_node = cost_to_neighbor_via_node
                        neighbor.from_idx     = self.camera_idx
                        heapq.heapify(heap) # is this the most efficient "update" call?

        def finish(self):
            '''A shortest path was found'''
            if self.camera_idx == 0:
                # This is the reference camera. Nothing to do
                return

            Rt_fc = compute_pairwise_Rt(self.from_idx, self.camera_idx)

            if self.from_idx == 0:
                Rt_0c[self.camera_idx-1] = Rt_fc
                return

            Rt_0f = Rt_0c[self.from_idx-1]
            Rt_0c[self.camera_idx-1] = poseutils.compose_Rt( Rt_0f, Rt_fc)

        def visited(self):
            '''Returns True if this node went through the heap and has then been visited'''
            return self.camera_idx == 0 or Rt_0c[self.camera_idx-1] is not None

        def seen(self):
            '''Returns True if this node has been in the heap'''
            return self.cost_to_node is not None

        @staticmethod
        def compute_edge_cost(shared_frames):
            # I want to MINIMIZE cost, so I MAXIMIZE the shared frames count and
            # MINIMIZE the hop count. Furthermore, I really want to minimize the
            # number of hops, so that's worth many shared frames.
            cost = 100000 - shared_frames
            assert(cost > 0) # dijkstra's algorithm requires this to be true
            return cost



    nodes = [Node(i) for i in xrange(Ncameras)]
    nodes[0].cost_to_node = 0
    heap = []

    nodes[0].visit()
    while heap:
        node_top = heapq.heappop(heap)
        node_top.visit()

    if any([x is None for x in Rt_0c]):
        raise Exception("ERROR: Don't have complete camera observations overlap!\n" +
                        ("Past-camera-0 Rt:\n{}\n".format(Rt_0c))                   +
                        ("Shared observations matrix:\n{}\n".format(shared_frames)))


    return nps.cat(*Rt_0c)



def estimate_frame_poses(calobject_poses_local_Rt, camera_poses_Rt, indices_frame_camera, dot_spacing,
                         Nwant):
    r'''Estimate poses of the calibration object observations

    We're given

    calobject_poses_local_Rt:

      an array of dimensions (Nobservations,4,3) that contains a
      calobject-to-camera transformation estimate, for each observation of the
      board

    camera_poses_Rt:

      an array of dimensions (Ncameras-1,4,3) that contains a camerai-to-camera0
      transformation estimate. camera0-to-camera0 is the identity, so this isn't
      stored

    indices_frame_camera:

      an array of shape (Nobservations,2) that indicates which frame and which
      camera has observed the board

    With this data, I return an array of shape (Nframes,6) that contains an
    estimate of the pose of each frame, in the camera0 coord system. Each row is
    (r,t) where r is a Rodrigues rotation and t is a translation that map points
    in the calobject coord system to that of camera 0

    '''


    def process(i_observation0, i_observation1):
        R'''Given a range of observations corresponding to the same frame, estimate the
        frame pose'''

        def T_camera_board(i_observation):
            r'''Transform from the board coords to the camera coords'''
            i_frame,i_camera = indices_frame_camera[i_observation, ...]

            Rt_f = calobject_poses_local_Rt[i_observation, :,:]
            if i_camera == 0:
                return Rt_f

            # T_cami_cam0 T_cam0_board = T_cami_board
            Rt_cam = camera_poses_Rt[i_camera-1, ...]

            return poseutils.compose_Rt( Rt_cam, Rt_f)


        # frame poses should map FROM the frame coord system TO the ref coord
        # system (camera 0).

        # special case: if there's a single observation, I just use it
        if i_observation1 - i_observation0 == 1:
            return T_camera_board(i_observation0)

        # Multiple cameras have observed the object for this frame. I have an
        # estimate of these for each camera. I merge them in a lame way: I
        # average out the positions of each point, and fit the calibration
        # object into the mean point cloud
        obj = utils.get_full_object(Nwant, Nwant, dot_spacing)

        sum_obj_unproj = obj*0
        for i_observation in xrange(i_observation0, i_observation1):
            Rt = T_camera_board(i_observation)
            sum_obj_unproj += poseutils.transform_point_Rt(Rt, obj)

        mean = sum_obj_unproj / (i_observation1 - i_observation0)

        # Got my point cloud. fit

        # transform both to shape = (N*N, 3)
        obj  = nps.clump(obj,  n=2)
        mean = nps.clump(mean, n=2)
        return utils.align3d_procrustes( mean, obj )





    frame_poses_rt = np.array(())

    i_frame_current          = -1
    i_observation_framestart = -1;

    for i_observation in xrange(indices_frame_camera.shape[0]):
        i_frame,i_camera = indices_frame_camera[i_observation, ...]

        if i_frame != i_frame_current:
            if i_observation_framestart >= 0:
                Rt = process(i_observation_framestart, i_observation)
                frame_poses_rt = nps.glue(frame_poses_rt, poseutils.rt_from_Rt(Rt), axis=-2)

            i_observation_framestart = i_observation
            i_frame_current = i_frame

    if i_observation_framestart >= 0:
        Rt = process(i_observation_framestart, indices_frame_camera.shape[0])
        frame_poses_rt = nps.glue(frame_poses_rt, poseutils.rt_from_Rt(Rt), axis=-2)

    return frame_poses_rt

def make_seed(inputs, model_seed):
    r'''Generate a solution seed for a given input'''


    def make_intrinsics_vector(i_camera, inputs):
        imager_w,imager_h = inputs['imagersizes'][i_camera]
        return np.array( (inputs['focal_estimate'], inputs['focal_estimate'],
                          float(imager_w-1)/2.,
                          float(imager_h-1)/2.))




    if model_seed is None:
        intrinsics_data = nps.cat( *[make_intrinsics_vector(i_camera, inputs) \
                                     for i_camera in xrange(inputs['Ncameras'])] )
        intrinsics = ('DISTORTION_NONE', intrinsics_data)
    else:
        intrinsics = list(model_seed.intrinsics())
        intrinsics[1] = nps.dummy(intrinsics[1], -2) # add 'cameras' dimension of length 1

    # I compute an estimate of the poses of the calibration object in the local
    # coord system of each camera for each frame. This is done for each frame
    # and for each camera separately. This isn't meant to be precise, and is
    # only used for seeding.
    #
    # I get rotation, translation in a (4,3) array, such that R*calobject + t
    # produces the calibration object points in the coord system of the camera.
    # The result has dimensions (N,4,3)
    calobject_poses_local_Rt = \
        estimate_local_calobject_poses( inputs['indices_frame_camera'],
                                        inputs['dots'],
                                        inputs['dot_spacing'],
                                        inputs['focal_estimate'],
                                        inputs['imagersizes'],
                                        inputs['object_width_n'])
    # these map FROM the coord system of the calibration object TO the coord
    # system of this camera

    # I now have a rough estimate of calobject poses in the coord system of each
    # frame. One can think of these as two sets of point clouds, each attached to
    # their camera. I can move around the two sets of point clouds to try to match
    # them up, and this will give me an estimate of the relative pose of the two
    # cameras in respect to each other. I need to set up the correspondences, and
    # align3d_procrustes() does the rest
    #
    # I get transformations that map points in 1-Nth camera coord system to 0th
    # camera coord system. Rt have dimensions (N-1,4,3)
    camera_poses_Rt = estimate_camera_poses( calobject_poses_local_Rt,
                                             inputs['indices_frame_camera'],
                                             inputs['dots'],
                                             inputs['dot_spacing'],
                                             inputs['Ncameras'],
                                             inputs['object_width_n'])

    if len(camera_poses_Rt):
        # extrinsics should map FROM the ref coord system TO the coord system of the
        # camera in question. This is backwards from what I have
        extrinsics = nps.atleast_dims( poseutils.rt_from_Rt(poseutils.invert_Rt(camera_poses_Rt)),
                                       -2 )
    else:
        extrinsics = np.zeros((0,6))

    frames = \
        estimate_frame_poses(calobject_poses_local_Rt, camera_poses_Rt,
                             inputs['indices_frame_camera'],
                             inputs['dot_spacing'],
                             inputs['object_width_n'])
    return intrinsics,extrinsics,frames


def get_imagersize_one(g):
    r'''Returns the imager dimensions for a given image glob

    This reports the size for ONE camera

    I only look at the first match. It is assumed that all the images matching
    this glob have the same dimensions.

    Note that if I didn't do this, then you wouldn't need to keep the images
    around after the dots cache has been computed. This function requires (some
    of) the images even with a cache. Should reconsider maybe.

    '''
    files = glob.glob(g)
    if len(files) == 0:
        raise Exception("Glob '{}' matched no files. I need this for the imager size".format(g))

    img = cv2.imread(files[0]);
    h,w = img.shape[:2]
    return [w,h]







args = parse_args()
# expand ~/ into $HOME/
args.images = [os.path.expanduser(g) for g in args.images]

Ncameras = len(args.images)
if Ncameras > 10:
    raise Exception("Got {} image globs. It should be one glob per camera, and this sounds like WAY too make cameras. Did you forget to escape your glob?". \
                    format(Ncameras))
if args.seed is not None:
    if not Ncameras == 1:
        raise Exception("Currently --seed works only with ONE camera")
    args.seed = cameramodel(args.seed)


object_spacing = args.object_spacing
object_width_n = args.object_width_n

observations, indices_frame_camera,paths = \
    get_observations(object_width_n,
                     object_width_n,
                     args.images,
                     args.dots_cache,
                     jobs = args.jobs)

# list of imager dimensions; one per camera
imagersizes = [get_imagersize_one(g) for g in args.images]

inputs = {'imagersizes':          imagersizes,
          'focal_estimate':       args.focal,
          'Ncameras':             Ncameras,
          'indices_frame_camera': indices_frame_camera,
          'dots':                 observations,
          'dot_spacing':          object_spacing,
          'object_width_n':       object_width_n}



intrinsics,extrinsics,frames = make_seed(inputs, args.seed)
distortion_model = intrinsics[0]
intrinsics       = intrinsics[1]

# done with everything. Run the calibration, in several passes. If we were given
# a seed, I presumably have an estimate of the solution, so I skip straight to
# the final pass
if args.seed is None:
    distortion_model = "DISTORTION_NONE"
    optimizer.optimize(intrinsics, extrinsics, frames, None,
                       observations, indices_frame_camera,
                       None, None,
                       distortion_model,
                       do_optimize_intrinsic_core        = False,
                       do_optimize_intrinsic_distortions = False,
                       calibration_object_spacing        = object_spacing,
                       calibration_object_width_n        = object_width_n,
                       VERBOSE                           = False)

    stats = optimizer.optimize(intrinsics, extrinsics, frames, None,
                       observations, indices_frame_camera,
                       None, None,
                       distortion_model,
                       do_optimize_intrinsic_core        = True,
                       do_optimize_intrinsic_distortions = False,
                       calibration_object_spacing        = object_spacing,
                       calibration_object_width_n        = object_width_n,
                       VERBOSE                           = False)

    MMt = stats['intrinsic_covariances']
    print "done with {}".format(distortion_model)

# final pass
Ndistortions0      = optimizer.getNdistortionParams(distortion_model)

distortion_model   = args.distortion_model
Ndistortions       = optimizer.getNdistortionParams(distortion_model)
Ndistortions_delta = Ndistortions - Ndistortions0
intrinsics         = nps.glue( intrinsics, np.random.random((Ncameras, Ndistortions_delta))*1e-5, axis=-1 )
stats = optimizer.optimize(intrinsics, extrinsics, frames, None,
                           observations, indices_frame_camera,
                           None, None,
                           distortion_model,
                           do_optimize_intrinsic_core        = True,
                           do_optimize_intrinsic_distortions = True,
                           calibration_object_spacing        = object_spacing,
                           calibration_object_width_n        = object_width_n,
                           VERBOSE                           = True)
MMt = stats['intrinsic_covariances']
print "done with {}, optimizing DISTORTIONS".format(distortion_model)





# Write the output models
for i_camera in xrange(Ncameras):
    if args.muse_extrinsics:
        Rt_r0 = np.array([[ 0.,  0.,  1.],
                          [ 1.,  0.,  0.],
                          [ 0.,  1.,  0.],
                          [ 0.,  0.,  0.]])
    else:
        # identity
        Rt_r0 = np.array([[ 1.,  0.,  0.],
                          [ 0.,  1.,  0.],
                          [ 0.,  0.,  1.],
                          [ 0.,  0.,  0.]])

    if i_camera >= 1:
        rt_x0 = extrinsics[i_camera-1,:].ravel()
    else:
        rt_x0 = np.zeros(6)
    Rt_rx = poseutils.compose_Rt(Rt_r0,
                                 poseutils.invert_Rt( poseutils.Rt_from_rt(rt_x0)))

    c = cameramodel( intrinsics          = (distortion_model, intrinsics[i_camera,:]),
                     extrinsics_Rt_toref = Rt_rx,
                     dimensions          = imagersizes[i_camera])

    cahvorfile = '{}/camera-{}.cahvor'.format(args.outdir, i_camera)
    cahvor.write(cahvorfile, c,
                 "generated with {}\n".format(sys.argv))
    print "Wrote {}".format(cahvorfile)

    cameramodelfile = '{}/camera-{}.cameramodel'.format(args.outdir, i_camera)
    c.write(cameramodelfile,
            "generated with {}\n".format(sys.argv))
    print "Wrote {}".format(cameramodelfile)

if args.explore:
    import gnuplotlib as gp


    print r'''Calibration results REPL.
Potential things to look at:

   show_reprojection_errors_worst(i_observation_in_order_from_worst)
   show_intrinsics_map()
   i_observations_worst
   rms_err_all_points_perimage
   rms_err_ignoring_outliers_perimage
   paths[i_observations_worst[0]]
'''


    projected = projections.calobservations_project(distortion_model, intrinsics, extrinsics, frames, object_spacing, object_width_n)
    err_all_points,err_ignoring_outliers = projections.calobservations_compute_reproj_error(projected, observations,
                                                                                            indices_frame_camera, object_width_n,
                                                                                            stats['outlier_indices'])
    norm2_err_all_points_perimage = nps.inner( nps.clump(err_all_points,n=-3),
                                               nps.clump(err_all_points,n=-3) )
    norm2_err_ignoring_outliers_perimage = nps.inner( nps.clump(err_ignoring_outliers,n=-3),
                                                      nps.clump(err_ignoring_outliers,n=-3) )
    rms_err_all_points_perimage          = np.sqrt( norm2_err_all_points_perimage        / (object_width_n*object_width_n) )
    rms_err_ignoring_outliers_perimage   = np.sqrt( norm2_err_ignoring_outliers_perimage / (object_width_n*object_width_n) )

    i_observations_worst    = list(reversed(np.argsort(rms_err_ignoring_outliers_perimage)))
    i_observation_from_path = dict( [(paths[_i],_i) for _i in xrange(len(observations))] )

    def show_reprojection_errors(observation):
        r'''Visualize calibration residuals

        Takes either an integer (observation index) or a string (path)

        '''

        if isinstance(observation, int):
            i_observation = observation
        elif isinstance(observation, str):
            i_observation = i_observation_from_path[observation]
        else:
            raise Exception("observation should be a string (image path) or an integer; got type(observation) = {}".format(type(observation)))

        ioutlier0 = np.searchsorted(stats['outlier_indices'], object_width_n*object_width_n*i_observation,     'left')
        ioutlier1 = np.searchsorted(stats['outlier_indices'], object_width_n*object_width_n*(i_observation+1), 'left')
        outlier_indices_thisobservation = stats['outlier_indices'][ioutlier0:ioutlier1] - object_width_n*object_width_n*i_observation

        obs              = nps.clump( observations[i_observation], n=2)
        i_frame,i_camera = indices_frame_camera[i_observation]
        reproj           = nps.clump( projected[i_frame,i_camera], n=2)

        # error per dot
        err = np.sqrt(nps.inner(reproj - obs,
                                reproj - obs))

        nonoutlier_indices_thisobservation = np.ones((object_width_n*object_width_n,),dtype=bool)
        nonoutlier_indices_thisobservation[outlier_indices_thisobservation] = False
        obs_ignoring_outliers = obs[nonoutlier_indices_thisobservation, :]
        gp.plot( (obs_ignoring_outliers[:,0], obs_ignoring_outliers[:,1], err[nonoutlier_indices_thisobservation],
                  {'with': 'points pt 7 ps 2 palette', 'legend': 'reprojection error', 'tuplesize': 3}),
                 (obs   [:,0], obs   [:,1], {'with': 'points', 'legend': 'observed'}),
                 (reproj[:,0], reproj[:,1], {'with': 'points', 'legend': 'hypothesis'}),
                 rgbimage=paths[i_observation],
                 square=1,cbmin=0,
                 _set='autoscale noextend',
                 title='i_observation={}, i_frame={}, i_camera={}, path={}, error_RMS_all_points={}, error_RMS_ignoring_outliers={}'.format( i_observation, i_frame, i_camera, paths[i_observation], rms_err_all_points_perimage[i_observation], rms_err_ignoring_outliers_perimage[i_observation]))

    def show_reprojection_errors_worst(i):
        show_reprojection_errors( i_observations_worst[i] )

    plots_intrinsics_sensitivity = [None] * Ncameras
    def show_intrinsics_map(i_camera = None, gridn = 40):
        r'''Visualize intrinsics uncertainty

        Visualizes the expected value of projection uncertainty across the
        imager. The uncertainty ultimately comes from pixel observation
        uncertainty in the input to the calibration.

        The i_camera argument is the camera to visualize, or None for ALL the
        cameras

        The gridn argument specifies the sample width: how many points across
        the imager in both dimensions

        I visualize the intrinsics uncertainty due to uncertainty in the input
        image coordinates of chessboard corners. A part of this computation was
        done in the mrcal core, and intermediate results are available in MMt =
        stats['intrinsic_covariances']. Comment from the mrcal core:

            This function is part of sensitivity analysis to quantify how much errors in
            the input pixel observations affect our solution. A "good" solution will not
            be very sensitive: measurement noise doesn't affect the solution very much.

            I minimize a cost function E = norm2(x) where x is the measurements. Some
            elements of x depend on inputs, and some don't (regularization for instance).
            I perturb the inputs, reoptimize (assuming everything is linear) and look
            what happens to the state p. I'm at an optimum p*:

              dE/dp (p=p*) = 2 Jt x (p=p*) = 0

            I perturb the inputs:

              E(x(p+dp, m+dm)) = norm2( x + J dp + dx/dm dm)

            And I reoptimize:

              dE/ddp ~ ( x + J dp + dx/dm dm)t J = 0

            I'm at an optimum, so Jtx = 0, so

              -Jt dx/dm dm = JtJ dp

            So if I perturb my input observation vector m by dm, the resulting effect on
            the parameters is dp = M dm

              where M = -inv(JtJ) Jt dx/dm

            In order to be useful I need to do something with M. Let's say I want to
            quantify how precise our optimal intrinsics are. Ultimately these are always
            used in a projection operation. So given a 3d observation vector v, I project
            it onto our image plane:

              q = project(v, intrinsics)

            I assume an independent, gaussian noise on my input observations, and for a
            set of given observation vectors v, I compute the effect on the projection.

              dq = dprojection/dintrinsics dintrinsics

            dprojection/dintrinsics comes from cvProjectPoints2()
            dintrinsics is the shift in our optimal state: M dm

            If dm represents noise of the zero-mean, independent, gaussian variety, then
            dp is also zero-mean gaussian, but no longer independent.

              Var(dp) = M Var(dm) Mt = M Mt s^2

            where s is the standard deviation of the noise of each parameter in dm.

            The intrinsics of each camera have 3 components:

            - f: focal lengths
            - c: center pixel coord
            - d: distortion parameters

            Let me define dprojection/df = F, dprojection/dc = C, dprojection/dd = D.
            These all come from cvProjectPoints2().

            Rewriting the projection equation I get

              q = project(v,  f,c,d)
              dq = F df + C dc + D dd

            df,dc,dd are random variables that come from dp.

              Var(dq) = F Covar(df,df) Ft +
                        C Covar(dc,dc) Ct +
                        D Covar(dd,dd) Dt +
                        F Covar(df,dc) Ct +
                        F Covar(df,dd) Dt +
                        C Covar(dc,df) Ft +
                        C Covar(dc,dd) Dt +
                        D Covar(dd,df) Ft +
                        D Covar(dd,dc) Ct

            Covar(dx,dy) are all submatrices of the larger Var(dp) matrix we computed
            above: M Mt s^2.

            Here I look ONLY at the interactions of intrinsic parameters for a particular
            camera with OTHER intrinsic parameters of the same camera. I ignore
            cross-camera interactions and interactions with other parameters, such as the
            frame poses and extrinsics.

            For mrcal, the measurements are

            1. reprojection errors of chessboard grid observations
            2. reprojection errors of individual point observations
            3. range errors for points with known range
            4. regularization terms

            The observed pixel measurements come into play directly into 1 and 2 above,
            but NOT 3 and 4. Let's say I'm doing ordinary least squares, so x = f(p) - m

              dx/dm = [ -I ]
                      [  0 ]

            I thus ignore measurements past the observation set.

            My matrices are large and sparse. Thus I compute the blocks of M Mt that I
            need here, and return these densely to the upper levels (python). These
            callers will then use these dense matrices to finish the computation

              M Mt = sum(outer(col(M), col(M)))
              col(M) = solve(JtJ, row(J))
        '''



        print 'need to use the input sigma, need to label plots better'

        global plots_intrinsics_sensitivity
        i_camera_all = (i_camera,) if i_camera is not None else range(len(intrinsics))
        for i_camera in i_camera_all:

            W,H=imagersizes[i_camera]
            w = np.linspace(0,W-1,gridn)
            h = np.linspace(0,H-1,gridn)
            # shape: Nwidth,Nheight,2
            grid  = nps.reorder(nps.cat(*np.meshgrid(w,h)), -1, -2, -3)

            # shape: Nwidth,Nheight,3
            V = projections.unproject(grid, distortion_model, intrinsics[i_camera])

            p = projections.project(V, distortion_model, intrinsics[i_camera], get_gradients=True)
            imagePoints = p[..., 0]
            F           = p[..., 1:3]
            C           = p[..., 3:5]
            D           = p[..., 5: ]

            Cff = MMt[i_camera, 0:2, 0:2]
            Cfc = MMt[i_camera, 0:2, 2:4]
            Cfd = MMt[i_camera, 0:2, 4: ]
            Ccf = MMt[i_camera, 2:4, 0:2]
            Ccc = MMt[i_camera, 2:4, 2:4]
            Ccd = MMt[i_camera, 2:4, 4: ]
            Cdf = MMt[i_camera, 4:,  0:2]
            Cdc = MMt[i_camera, 4:,  2:4]
            Cdd = MMt[i_camera, 4:,  4: ]

            # shape Nwidth,Nheight,2,2: each slice is a 2x2 covariance
            Vdprojection = \
                nps.matmult(F,Cff,nps.transpose(F)) + \
                nps.matmult(F,Cfc,nps.transpose(C)) + \
                nps.matmult(F,Cfd,nps.transpose(D)) + \
                nps.matmult(C,Ccf,nps.transpose(F)) + \
                nps.matmult(C,Ccc,nps.transpose(C)) + \
                nps.matmult(C,Ccd,nps.transpose(D)) + \
                nps.matmult(D,Cdf,nps.transpose(F)) + \
                nps.matmult(D,Cdc,nps.transpose(C)) + \
                nps.matmult(D,Cdd,nps.transpose(D))

            # Let x be a 0-mean normally-distributed 2-vector with covariance V.
            # E(norm2(x)) = E(x0*x0 + x1*x1) = E(x0*x0) + E(x1*x1) = trace(V)

            @nps.broadcast_define( (('n','n'),), ())
            def trace(x):
                return np.trace(x)
            Erms = np.sqrt(trace(Vdprojection))

            if plots_intrinsics_sensitivity[i_camera] is None:
                plots_intrinsics_sensitivity[i_camera] = \
                    gp.gnuplotlib(_3d=1,
                                  unset='grid',
                                  set=['xrange noextend',
                                       'yrange noextend reverse',
                                       'view equal xy',
                                       'view map',
                                       'contour surface',
                                       'cntrparam levels incremental 10,-0.2,0'],
                                  zrange=[0,5],
                                  cbrange=[0,5],
                                  title="Camera {}".format(i_camera))

            # Currently "with image" can't produce contours. I work around this, by
            # plotting the data a second time.
            # Yuck.
            # https://sourceforge.net/p/gnuplot/mailman/message/36371128/
            plots_intrinsics_sensitivity[i_camera]. \
                plot( (Erms, dict(               tuplesize=3, _with='image')),
                      (Erms, dict(legend="Erms", tuplesize=3, _with='lines nosurface')))

    def check_confidence_computations():
        r'''These all test the confidence computations using the debugging exports
        from computeConfidence_MMt() in mrcal.c'''

        # E0 = nps.inner(x,x)
        # E0_rms = np.sqrt(E0/(x.shape[0]/2))
        # mu_sumofsquares = E0
        # s_sumofsquares  = 2*np.sqrt(E0)
        # mu_meanofsquares = mu_sumofsquares/(x.shape[0]/2)
        # s_meanofsquares  = s_sumofsquares /(x.shape[0]/2)

        # gp.plot( equation='1./sqrt(2.*pi*{var})*exp(-({x_meanofsquares}-{mu})**2. / (2.*{var}))*2*x'. \
        #          format(x_meanofsquares = '(x*x)',
        #                 mu  = mu_meanofsquares,
        #                 var = s_meanofsquares*s_meanofsquares),
        #          _xrange=[0,E0_rms*2],
        #          _set='samples 1000')

        i_camera = 0


        import IPython
        import numpy as np
        import numpysane as nps
        import gnuplotlib as gp
        from mrcal import projections

        global stats,observations,MMt
        x0            = stats['x']
        dm            = np.loadtxt("/tmp/dm")
        Jtdm          = np.loadtxt("/tmp/Jtdm")
        dp            = np.loadtxt("/tmp/dp")
        dx_hypothesis = np.loadtxt("/tmp/dx_hypothesis")
        dx            = np.loadtxt("/tmp/dx")

        observations += dm.reshape(116,10,10,2)
        intrinsics0 = np.copy(intrinsics)
        frames0     = np.copy(frames)
        p0          = nps.glue( intrinsics.ravel(),
                                frames.ravel(),
                                axis=-1)

        J      = np.fromfile("/tmp/J1_23200_704.dat").reshape(23200,704)
        M      = np.linalg.pinv(J)
        MMt_ref = nps.matmult(M,nps.transpose(M))
        Nintrinsics = intrinsics.shape[-1]
        print "MMt difference should be 0: {}".format(np.linalg.norm(MMt[i_camera] -
                                                                     MMt_ref[Nintrinsics*i_camera:Nintrinsics*(i_camera+1),
                                                                             Nintrinsics*i_camera:Nintrinsics*(i_camera+1)]))

        stats = optimizer.optimize(intrinsics, extrinsics, frames, None,
                                   observations, indices_frame_camera,
                                   None, None,
                                   distortion_model,
                                   do_optimize_intrinsic_core        = True,
                                   do_optimize_intrinsic_distortions = True,
                                   calibration_object_spacing        = object_spacing,
                                   calibration_object_width_n        = object_width_n,
                                   VERBOSE                           = False,
                                   skip_outlier_rejection            = True,
                                   skip_regularization               = True)

        p1          = nps.glue( intrinsics.ravel(),
                                frames.ravel(),
                                axis=-1)
        x1 = stats['x']
        intrinsics1 = intrinsics
        E0 = np.sqrt(nps.inner(x0,x0)/(x0.shape[0]/2))
        E1 = np.sqrt(nps.inner(x1,x1)/(x0.shape[0]/2))


        dp_observed = p1-p0
        dx_observed = x1-x0

        dp_ref = nps.matmult(M, nps.transpose(dm)).ravel()

        print "dp difference should be 0: {}".format(np.linalg.norm(dp - dp_observed))
        print "dp difference should be 0: {}".format(np.linalg.norm(dp - dp_ref))
        print "dx difference should be 0: {}".format(np.linalg.norm(dx - dx_observed))
        # gp.plot(nps.cat(dp,            dp_observed), legend=np.array(("computed",   "observed")), title="dp")
        # gp.plot(nps.cat(dp,            dp_ref),      legend=np.array(("computed",   "reference")),title="dp")
        # gp.plot(nps.cat(dx,            dx_observed), legend=np.array(("computed",   "observed")), title="dx")

        # unperturbed projection: q0
        # perturbed   projection: q1
        # If my math is right, and things are locally linear, I can predict q1-q0:
        # dq = F df + C dc + D dd =
        #    = (F Mf + C Mc + D Md) dm
        v    = np.array((-0.81691696, -0.02852554,  0.57604945))
        p    = projections.project(v, distortion_model, intrinsics0[i_camera], get_gradients=True)
        q0   = p[..., 0]
        F    = p[..., 1:3]
        C    = p[..., 3:5]
        D    = p[..., 5: ]
        q1   = projections.project(v, distortion_model, intrinsics1[i_camera], get_gradients=False)
        Mf = M[Nintrinsics*i_camera+0 : Nintrinsics*i_camera+2,   :]
        Mc = M[Nintrinsics*i_camera+2 : Nintrinsics*i_camera+4,   :]
        Md = M[Nintrinsics*i_camera+4 : Nintrinsics*(i_camera+1) ,:]
        dq = nps.matmult((nps.matmult(F, Mf) +
                          nps.matmult(C, Mc) +
                          nps.matmult(D, Md) ),
                         nps.transpose(dm)).ravel()
        dq_observed = q1-q0
        print "dq got, difference: {}, {}".format(dq, dq-dq_observed)



    def show_all_errors(i_camera, ignore_outliers=True, vectorfield=False):
        r'''Visualize the full distribution of reprojection errors

        i_camera is the camera in question

        if ignore_outliers: we show the ignored-outlier distribution. Otherwise
        we show the distribution for all our data

        if vectorfield: we render a vectorfield from the observation point
        showing the residual as a vector. else: we render only the residual as a
        scatter plot

        '''

        err = err_ignoring_outliers if ignore_outliers else err_all_points

        idx = indices_frame_camera[:,1] == i_camera
        err = nps.clump(err         [idx, ...], n=3)
        obs = nps.clump(observations[idx, ...], n=3)

        if vectorfield:
            # Same thing as a vector field, color-coded by absolute error
            W,H=imagersizes[i_camera]
            gp.plot( obs[:,0], obs[:,1],
                     err[:,0], err[:,1],
                     np.sqrt(nps.inner(err,err)),
                     _with='vectors size screen 0.005,10 fixed filled palette',
                     tuplesize=5, square=1,
                     _xrange=[0,W], yrange=[H,0])
        else:
            # Reprojection error distribution
            gp.plot( err[:,0], err[:,1],
                     _with='points', square=1)



    import IPython
    IPython.embed()
