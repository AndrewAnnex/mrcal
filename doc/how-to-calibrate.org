#+TITLE: How to run a calibration
#+OPTIONS: toc:t

Calibrating cameras is a very common task mrcal is expected to solve, and this
is made available with the [[file:mrcal-calibrate-cameras.html][=mrcal-calibrate-cameras=]] tool. A high-level demo is
shown in the [[file:tour.org][tour of mrcal]]. A deeper description follows.

* Capturing images
We need to get observations of a [[file:formulation.org::#Calibration object][calibration object]], a board containing an
observable grid of points. mrcal doesn't care where these observations come
from, but the recommended method is to use a chessboard target detected by
[[http://github.com/dkogan/mrgingham][mrgingham]]. Chessboards are recommended in favor of circles to obtain accurate
results. See the mrgingham documentation for a .pdf of a chessboard pattern.
This pattern should be printed and mounted onto a /rigid/ and /flat/ surface to
produce the calibration object.

As shown in the [[file:tour.org::#choreography][dance study]], we want chessboard observations from as near as
possible to the lens, so the chessboard should be sized to fill the imager as
much as possible, from as close as possible. My chessboard has 10x10 internal
corners with a spacing of 7.7cm. A big chessboard such as this is never
completely rigid or completely flat. I'm using a board backed with 2cm of foam,
which keeps the shape stable over short periods of time, which is sufficient:
mrcal estimates the chessboard shape as part of its calibration solve.

Now that we have a calibration object, this object needs to be shown to the
camera(s) to produce the images that we will use to find the corner coordinates,
which mrcal will then use in its computations.

It is important that the images contain clear corners. Motion blur or focus
issues or exposure issues will all cause bias in the resulting calibration.

As shown in the [[file:tour.org::#choreography][study]], the most useful observations to gather are

- close-ups: the chessboard should fill the whole frame as much as possible

- oblique views: tilt the board forward/back and left/right. I generally tilt by
  more than 45 degrees. At a certain point the corners become indistinct and
  mrgingham starts having trouble, but depending on the lens, that point could
  come with quite a bit of tilt. A less dense chessboard eases this also, at the
  cost of requiring more board observations to get the same number of points.

- If you are calibrating multiple cameras, and they are synchronized, you can
  calibrate them all at the same time, and obtain intrinsics /and/ extrinsics.
  In that case you want frames where multiple cameras see the calibration object
  at the same time. Depending on the geometry, it may be impossible to place a
  calibration object in a location where it's seen by all the cameras, /and/
  where it's a close-up for all the cameras at the same time. In that case, get
  close-ups for each camera individually, and get observations common to
  multiple cameras, that aren't necessarily close-ups. The former will serve to
  define your camera intrinsics, and the latter will serve to define your
  extrinsics (geometry).

A dataset composed primarily of tilted closeups will produce good results. It is
better to have more data rather than less. mrgingham will throw away frames
where no chessboard can be found, so it is perfectly reasonable to grab too many
images with the expectation that they won't all end up being used in the
computation.

I usually aim for about 100 usable frames, but you can often get away with far
fewer. The mrcal uncertainty feedback (see below) will tell you if you need more
data.

* Detecting corners
mrgingham tries to handle a variety of lighting conditions, including varying
illumination across the image, but the corners must exist in the image in some
form. A fundamental design decision in mrgingham is to only output chessboards
that we are very confident in, and a consequence of this is that mrgingham
requires the /whole/ chessboard to be visible in order to produce any results.
Thus it requires a bit of effort to produce any data at the edges and in the
corners of the imager: if even a small number of the chessboard corners are out
of bounds, mrgingham will not detect the chessboard at all. Another requirement
due to the design of mrgingham is that the board should be held with a flat edge
parallel to the camera xz plane (parallel to the ground, usually). mrgingham
looks for vertical and horizontal sequences of corners, but if the board is
rotated diagonally, then none of these sequences are "horizontal" or "vertical",
but they're all "diagonal", which isn't what mrgingham is looking for.

If we use a different grid detector than mrgingham, we need to produce the same
output that mrgingham produces: a [[https://www.github.com/dkogan/vnlog][=vnlog=]] (text table) describing each corner
detection. This file (let's call it =corners.vnl=) should contain this header:

#+begin_example
# filename x y level
#+end_example

The columns are

- =filename=: the path to the chessboard image
- =x=, =y=: pixel coordinates of the corner. These should be given in a
  consistent grid order
- =level=: the decimation level used in detecting that corner

Images where no chessboard was detected should be omitted, or represented with a
single record

#+begin_example
FILENAME - - -
#+end_example

The =level= column is the decimation level used in detecting that corner. =0=
means "full-resolution", =1= means "half-resolution", =2= means
"quarter-resolution" and so on. A level of =-= means "skip this point". This is
how incomplete board observations are specified. A file with a missing =level=
column will fill in =0= for all corners.

* Optimization
Once we have gathered input images, we're ready to run the calibration tool. For
instance:

#+begin_src sh
mrcal-calibrate-cameras
  --corners-cache corners.vnl
  --focal 2000
  --object-spacing 0.1
  --object-width-n 10
  --outdir /tmp
  --lensmodel LENSMODEL_OPENCV8
  --observed-pixel-uncertainty 1.0
  --explore
  'frame*-camera0.png' 'frame*-camera1.png' 'frame*-camera2.png'
#+end_src

Clearly, the arguments would be adjusted for each specific case.

The first argument says that the chessboard corner coordinates live in a file
called =corners.vnl=. If this file exists, we'll use that data. If that file
does not exist (which is what will happen the first time), mrgingham will be
invoked to compute the corners from the images, and the results will be written
to that file. So the same command is used to both compute the corners initially,
and to reuse the pre-computed corners with subsequent runs.

- =--focal 2000= says that the initial estimate for the camera focal lengths is
  2000 pixels. This doesn't need to be precise at all, but do try to get this
  roughly correct if possible. Simple geometry says that

\[ 2 f \tan \frac{\mathrm{field\_of\_view\_horizontal}}{2} = \mathrm{imager\_width} \]

- =--object-spacing= is the width of each square in your chessboard. This
  depends on the specific chessboard object you are using. --object-width-n is
  the corner count of the calibration object.

- =--outdir= specifies the directory where the output models will be written

- =--lensmodel= specifies which lens model we're using for /all/ the cameras. In
  this example we're using the =LENSMODEL_OPENCV8= model. This works reasonably
  well for most lenses. Longer lenses can get away with even leaner models:
  =LENSMODEL_OPENCV5= or even =LENSMODEL_OPENCV4=. All lean models will have
  some fitting errors and overly-optimistic reported uncertainties. So rich,
  splined stereographic models are recommended to address these issues.
  Currently =LENSMODEL_SPLINED_STEREOGRAPHIC_...= is the only rich splined
  model. Please read the [[file:lensmodels.org::#splined models configuration selection][notes]] about choosing the configuration parameters. See.
  The full set of supported lens models is described in detail [[file:lensmodels.org][here]].

- =--observed-pixel-uncertainty 1.0= says that the $x,y$ corner coordinates in
  =corners.vnl= are distributed normally, independently, and with the standard
  deviation of 1.0 pixels. This will be used for the [[file:uncertainty.org][projection uncertainty]]
  reporting. There isn't a reliable tool to estimate this currently (there's an
  [[https://github.com/dkogan/mrgingham/blob/master/mrgingham-observe-pixel-uncertainty][attempt]] here, but it needs more testing). The recommendation is to eyeball a
  conservative value, and to treat the resulting reported uncertainties
  conservatively.

- =--explore= requestes that after the models are computed, a REPL be opened so
  that the user can look at various metrics describing the output

After all the options, globs describing the images are passed in. Note that
these are /globs/, not /filenames/. So you need to quote or escape each glob to
prevent the shell from expanding it. You want one glob per camera; in the above
example we have 3 cameras. The program will look for all files matching the
globs, and filenames with identical matched strings are assumed to have been
gathered at the same instant in time. I.e. if in the above example we found
=frame003-camera0.png= and =frame003-camera1.png=, we will assume that these two
images were time-synchronized. If your capture system doesn't have
fully-functional frame syncronization, you should run a series of monocular
calibrations. Otherwise the models won't fit well (high reprojection errors
and/or high outlier counts) and you might see a frame with systematic
reprojection errors where one supposedly-synchronized camera's observation pulls
the solution in one direction, and another camera's observation pulls it in
another.

When you run the program as given above, the tool will spend a bit of time
computing (usually 10-20 seconds is enough, but this is highly dependent on the
specific problem, the amount of data, and the computational hardware). When
finished, it will write the resulting models to disk, and open a REPL (if
=--explore= was given). The resulting filenames are =camera-N.cameramodel= where
=N= is the index of the camera, starting at 0. The models contain the intrinsics
and extrinsics, with camera 0 sitting at the reference coordinate system.

When the solve is completed, you'll see a summary such as this one:

#+begin_example
RMS reprojection error: 0.3 pixels
Worst reprojection error: 4.0 pixels
Noutliers: 7 out of 9100 total points: 0.1% of the data
#+end_example

The reprojection errors should look reasonable given your
=--observed-pixel-uncertainty=. Since any outliers will be thrown out, the
reported reprojection errors will be reasonable.

Higher outlier counts are indicative of some/all of these:

- Issues in the input data, such as incorrectly-detected chessboard corners,
  unsynchronized cameras, rolling shutter, motion blur, focus issues, etc. We
  can examine these with =show_residuals_observation_worst()=; see below.
- Badly-fitting lens model

A non-fitting lens model can generally happen /only/ if we're using a too-lean
lens model. A splined model should be able to fit any lens. If we /are/ using a
too-lean model, we could potentially still get reasonable results in the subset
of the imager that does fit, but it is very much recommended to switch to a
richer lens model.

With =--explore= you get a REPL, and a message that points out some useful
functions. Try them all out. We want to start by looking at the worst residuals:

#+begin_example
show_residuals_observation_worst(0, vectorscale = 100)
#+end_example

This pops up an interactive plot showing the worst-fitting chessboard
observation overlaid with its observed and predicted corners, as an error vector
(scaled up by a factor of 100 for legibility). The reprojection errors are given
by a colored circle. The size of each circle represents the weight given to that
point. The weight is reduced for points that were detected at a lower resolution
by the chessboard detector. Points thrown out as outliers are not shown at all.

This is the worst-fitting image, so any data-gathering issues will show up in
this plot. Zooming in at the worst point (easily identifiable by the color) will
clearly show any motion blur or focus issues. Incorrectly-detected corners will
be visible: they will be outliers or they will have a high error. Especially
with lean models, the errors will be higher towards the edge of the imager: the
lens models fit the worst there. There should be no discernible pattern to the
errors. In a perfect world, these residuals will look like random samples.
Out-of-sync camera observations would show up as a systematic error vectors
pointing in one direction. And the /other/ out-of-sync image would display equal
and opposite errors. Rolling shutter effects would show a more complicated, but
clearly non-random pattern. It is usually impossible to get clean-enough data to
make all the patterns disappear, but these systematic errors are not represented
by the [[file:uncertainty.org::#noise model][noise model]] method, so they will result in overly-optimistic
uncertainties.

Now let's look at another visualization of residuals that will highlight any
non-random pattern in the residuals.

#+begin_example
show_residuals_directions(icam = 0, unset='key')
#+end_example

This displays /all/ the observations in camera 0, color-coded by their
direction, ignoring the magnitude. As before, if the model fit the observations,
the errors would represent random noise, and no color pattern would be
discernible in these dots. This plot is very useful for seeing the effect of an
overly-lean lens model. Try that out for each camera in the solve. If clear
patterns are discernible, a richer models is recommended.

Many other diagnostics are available, and do try them out. I want to talk about
the most significant one: the [[file:uncertainty.org][uncertainty reporting]]. If we're using a lean
model, the uncertainties will be much more optimistic than the value in reality.
If not using a splined model, then treat the reported uncertainties as the lower
bound of the true value.

#+begin_example
show_projection_uncertainty(icam = 0)
#+end_example

This will pop up a plot of projection uncertainties for each camera. The
uncertainties are shown as a color-map along with contours. These are the
expected value of projection errors based on noise in input corner observations
(given in =--observed-pixel-uncertainty=). By default, uncertainties for
projection out to infinity are shown. If another distance is desired, pass that
in the =distance= keyword argument. The lowest uncertainties are at roughly the
range where the chessboards were observed. Similarly, the area in the imager
where the chessboards were observed will have the best uncertainties. Gaps in
chessboard coverage will manifest as areas of high uncertainty (this is easier
to see if we overlay the observations by passing the =observations = True=
keyword argument).

These uncertainty metrics are complementary to the residual metrics described
above. If we have too little data, the residuals will be low, but the
uncertainties will be very high. The more data we gather, the lower the
uncertainties. A richer lens model lowers the residuals, but raises the
uncertainties (from an overly-optimistic level to a realistic one).

* Other                                                           :noexport:
talk about --seed and how that can be used to validate intrinsics
