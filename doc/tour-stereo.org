#+title: A tour of mrcal: stereo processing
#+OPTIONS: toc:t

* Previous
We just [[file:tour-choreography.org][found the best chessboard-dancing technique]]

* Stereo
We computed intrinsics from chessboards observations earlier, so let's use these
for stereo processing. I only use the splined model here.

I took several images off [[https://www.openstreetmap.org/#map=19/34.05565/-118.25333][a catwalk over Figueroa St in downtown Los Angeles]].
This is the view S along Figueroa St. There're tall buildings ahead and on
either side, making for an interesting stereo scene.

#+begin_src sh :exports none :eval no-export
# all the images downsampled for view on the page like this
for img ( data/figueroa-overpass-looking-S/{[01].jpg,[01]-reprojected-scale*.jpg,jplv-stereo-rect-*-scale*.png,rectified[01]-*.jpg~*narrow*,{range,disparity}-*.png~*narrow*} ) { convert $img -scale 12% ${img:t:r}.downsampled.${img:e} }

for img ( data/figueroa-overpass-looking-S/{rectified[01]-narrow.jpg,narrow-{left,right}.jpg,{range,disparity}-narrow.png} ) { convert $img -scale 25% ${img:t:r}.downsampled.${img:e} }
#+end_src

The two images out of the camera look like this:

[[file:external/data/figueroa-overpass-looking-S/0.jpg][file:external/figures/stereo/0.downsampled.jpg]]
[[file:external/data/figueroa-overpass-looking-S/1.jpg][file:external/figures/stereo/1.downsampled.jpg]]

All the full-size images are available by clicking on an image.

The cameras are 7ft (2.1m) apart. In order to compute stereo images we need an
accurate estimate of the geometry of the cameras. Usually we get this as an
output of the calibration, but here I only had one camera to calibrate, so I
don't have this geometry estimate. I used a separate tool to compute the
geometry from corresponding feature detections. The details aren't important;
for the purposes of this document we can assume that we did calibrate a stereo
pair, and that's where the geometry came from. The resulting with-geometry
models:

- [[file:external/data/figueroa-overpass-looking-S/splined-0.cameramodel][camera 0]]
- [[file:external/data/figueroa-overpass-looking-S/splined-1.cameramodel][camera 1]]

#+begin_src sh :exports none :eval no-export

# How did I make these? Like this!


# I reprojected the images to a pinhole model

for s (0.6 0.35) { for what (splined opencv8) { ~/jpl/mrcal/mrcal-reproject-image -f --to-pinhole --scale-focal $s data/board/$what.cameramodel data/figueroa-overpass-looking-S/[01].jpg | ~/jpl/mrcal/mrcal-to-cahvor > data/figueroa-overpass-looking-S/$what.pinhole.scale$s.cahvor; for c (0 1) { mv data/figueroa-overpass-looking-S/{$c-reprojected.jpg,$c.$what.pinhole.scale$s.jpg} } } }



# Then I computed a few features on the pavement

# Then I constructed a homography from those features using
# cv2.findHomography(), and fed that to img-any to find lots of features on the
# pavement:

~/jpl/img_any/binsrc/feature_track -L0 -T2200 -C6000 -R1800 -M 2000 -H data/figueroa-overpass-looking-S/homography.initial.scale0.6.txt data/figueroa-overpass-looking-S/[01].opencv8.pinhole.scale0.6.jpg | vnl-filter 'Corner1>500' 'Feat1x>1000' 'Feat2x>1000' > data/figueroa-overpass-looking-S/features.imgany.scale0.6.vnl

# Then I transformed those features back to the input image coords
paste \
  <( < data/figueroa-overpass-looking-S/features.imgany.scale0.6.vnl vnl-filter -p Feat1x,Feat1y | ~/jpl/mrcal/mrcal-reproject-points --intrinsics-only data/figueroa-overpass-looking-S/opencv8.pinhole.scale0.6.cahvor data/board/opencv8.cameramodel) \
  <( < data/figueroa-overpass-looking-S/features.imgany.scale0.6.vnl vnl-filter -p Feat2x,Feat2y | ~/jpl/mrcal/mrcal-reproject-points --intrinsics-only data/figueroa-overpass-looking-S/opencv8.pinhole.scale0.6.cahvor data/board/opencv8.cameramodel) > \
  data/figueroa-overpass-looking-S/features.imgany.inputimage.vnl

# And THEN I could use deltapose to compute extrinsics

D=data/figueroa-overpass-looking-S;

rm -f $D/{splined,opencv8}-{0,1}.cameramodel;

for what (splined opencv8) { PYTHONPATH=/home/dima/jpl/mrcal:/home/dima/jpl/img_any LD_LIBRARY_PATH=/home/dima/jpl/mrcal ~/jpl/deltapose-lite/calibrate-extrinsics --skip-outlier-rejection \
--correspondences <( < data/figueroa-overpass-looking-S/features.imgany.inputimage.vnl vnl-filter 'y1<3200 && y2<3200') --regularization t --seedrt01 0 0 0 $((7.*12*2.54/100)) 0 0 --cam0pose identity --observed-pixel-uncertainty 1 data/board/$what.cameramodel{,} && zmv -W 'camera-*.cameramodel' $D/$what-\*.cameramodel }
#+end_src

I then use the mrcal APIs to compute rectification maps, rectify the images,
compute disparities and convert them to ranges. This is done with the
[[file:mrcal-stereo.html][=mrcal-stereo=]] tool. We run it like this:

#+begin_src sh
mrcal-stereo                     \
  --az-fov-deg 145               \
  --el-fov-deg 135               \
  --el0-deg    5                 \
  --disparity-range 0 400        \
  --sgbm-p1 600                  \
  --sgbm-p2 2400                 \
  --sgbm-uniqueness-ratio 5      \
  --sgbm-disp12-max-diff 1       \
  --sgbm-speckle-window-size 200 \
  --sgbm-speckle-range 2         \
  --valid-intrinsics-region      \
  splined-[01].cameramodel       \
  [01].jpg
#+end_src
#+begin_src sh :exports none :eval no-export
D=doc/out/external/data/figueroa-overpass-looking-S/

mrcal-stereo                     \
  --az-fov-deg 145               \
  --el-fov-deg 135               \
  --el0-deg    5                 \
  --disparity-range 0 400        \
  --sgbm-p1 600                  \
  --sgbm-p2 2400                 \
  --sgbm-uniqueness-ratio 5      \
  --sgbm-disp12-max-diff 1       \
  --sgbm-speckle-window-size 200 \
  --sgbm-speckle-range 2         \
  --valid-intrinsics-region      \
  --outdir /tmp                  \
  $D/splined-[01].cameramodel    \
  $D/[01].jpg

zmv -f -W \
  '/tmp/[01]-rectified.png' \
  'doc/out/external/figures/stereo/rectified[01]-splined.png'

mv \
  /tmp/0-disparity.png \
  doc/out/external/figures/stereo/disparity-splined.png

mv \
  /tmp/0-range.png \
  doc/out/external/figures/stereo/range-splined.png

for img ( doc/out/external/figures/stereo/{rectified[01],disparity,range}-splined.png ) { \
  convert $img -scale 12% ${img:r}.downsampled.${img:e}
}
#+end_src

The =--sgbm-...= options configure the [[https://docs.opencv.org/4.5.3/d2/d85/classcv_1_1StereoSGBM.html][OpenCV SGBM stereo matcher]]. Not
specifying them uses the OpenCV defaults, which usually produces poor results.

The rectified images look like this:

[[file:external/figures/stereo/rectified0-splined.png][file:external/figures/stereo/rectified0-splined.downsampled.png]]
[[file:external/figures/stereo/rectified1-splined.png][file:external/figures/stereo/rectified1-splined.downsampled.png]]

And the disparity and range images looks like this:

[[file:external/figures/stereo/disparity-splined.png][file:external/figures/stereo/disparity-splined.downsampled.png]]
[[file:external/figures/stereo/range-splined.png][file:external/figures/stereo/range-splined.downsampled.png]]

This appears to be working well.

If you've used other stereo libraries previously, these rectified images may
look odd. In mrcal I produce images that sample the azimuth and elevation angles
evenly, which should minimize visual distortion inside each image row. A
side-effect is the the vertical expansion in the rectified image at the azimuth
extremes. Stereo matching works primarily by correlating the rows independently,
so this is a good trade-off. Some other implementations use un-even azimuth
spacing, which can't be good for matching performance.

* ranged pixels ground-truth                                       :noexport:
**** Buildings
top of Paul Hastings building. 530m away horizontally, 200m vertically: 566m away
https://en.wikipedia.org/wiki/City_National_Plaza

top of 7th/metro building at 7th/figueroa: 860m horizontally, 108m vertically: 870m
Figueroa Tower
https://www.emporis.com/buildings/116486/figueroa-tower-los-angeles-ca-usa

Top of library tower at 5th/figueroa. 513m horizontally, 300m vertically: 594

Near the top of the wilshire grand: 825m horizontall 250m vertically: 862
http://www.skyscrapercenter.com/building/wilshire-grand-center/9686

Near the top of the N Wells Fargo plaza building. 337m horizontally, 220m vertically: 402m
https://en.wikipedia.org/wiki/Wells_Fargo_Center_(Los_Angeles)

Los Angeles Center studios ~ 50m tall, on a hill. 520m horizontally: 522m

333 S Beaudry building. 291m horizontally 111m vertically: 311m
https://www.emporis.com/buildings/116570/beaudry-center-los-angeles-ca-usa

**** tests

Command to test all the ranges

#+begin_src sh :exports none :eval no-export
what=opencv8; (
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 2874 1231 --range-estimate 566 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 2968 1767 --range-estimate 870 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 1885 864  --range-estimate 594 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 3090 1384 --range-estimate 862 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg  541  413 --range-estimate 402 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 4489 1631 --range-estimate 522 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 5483  930 --range-estimate 311 --search-radius 10
./mrcal-triangulate $D/$what-[01].cameramodel $D/[01].jpg 5351  964 --range-estimate 311 --search-radius 10
) | egrep 'q1|Range'
#+end_src

=tst.py= to just look at a set of ranged features, and to compute the extrinsics
with a simple procrustes fit. Bypasses deltapose entirely. Works ok, but not
amazingly well

#+begin_src python :exports none :eval no-export
#!/usr/bin/python3

import sys
import numpy as np
import numpysane as nps

sys.path[:0] = '/home/dima/jpl/mrcal',
sys.path[:0] = '/home/dima/jpl/deltapose-lite',
sys.path[:0] = '/home/dima/jpl/img_any',
import mrcal

model_intrinsics = mrcal.cameramodel('data/board/splined.cameramodel')
t01              = np.array((7.*12*2.54/100, 0, 0))  # 7ft separation on the x

xy_xy_range = \
    np.array((

        (2874, 1231, 2831.68164062, 1233.9498291,  566.0),
        (2968, 1767, 2916.48388672, 1771.91601562, 870.0),
        (1885, 864,  1851.86499023, 843.52398682,  594.0),
        (3090, 1384, 3046.8894043,  1391.49401855, 862.0),
        (541,  413,  513.77832031,  355.37588501,  402.0),
        (4489, 1631, 4435.24023438, 1665.17492676, 522.0),
        (5483, 930,  5435.96582031, 987.39813232,  311.0),
        (5351, 964,  5304.21630859, 1018.49682617, 311.0),

        # Ranged pavement points. These don't appear to help
        (3592.350428, 3199.133514, 3198.330034, 3227.890159, 14.6),
        (3483.817362, 3094.172913, 3117.605605, 3115.684005, 15.76),
 ))

xy_xy = None
#xy_xy = np.array(( (3483.817362, 3094.172913,	3117.605605, 3115.684005),))





q0 = xy_xy_range[:,0:2]
q1 = xy_xy_range[:,2:4]
r  = xy_xy_range[:,(4,)]

# Points observed by camera0, represented in camera1 frame
p0 = mrcal.unproject(q0, *model_intrinsics.intrinsics(), normalize=True)*r - t01

# The unit observation vectors from the two cameras, observed in camera1. These
# must match via a rotation
v0 = p0 / nps.dummy(nps.mag(p0), -1)
v1 = mrcal.unproject(q1, *model_intrinsics.intrinsics(), normalize=True)

R01  = mrcal.align_procrustes_vectors_R01(v0,v1)
Rt01 = nps.glue(R01, t01, axis=-2)


if xy_xy is not None:
    import deltapose_lite
    rt10 = mrcal.rt_from_Rt(mrcal.invert_Rt(Rt01))
    p = \
        deltapose_lite.compute_3d_intersection_lindstrom(rt10,
                                                         model_intrinsics.intrinsics(),
                                                         model_intrinsics.intrinsics(),
                                                         xy_xy[:,0:2],
                                                         xy_xy[:,2:4],)
    print(nps.mag(p))
    sys.exit()


model0 = mrcal.cameramodel(model_intrinsics)
model0.extrinsics_Rt_toref(mrcal.identity_Rt())
model0.write('/tmp/0.cameramodel')

model1 = mrcal.cameramodel(model_intrinsics)
model1.extrinsics_Rt_toref( Rt01 )
model1.write('/tmp/1.cameramodel')
#+end_src

* Stereo rectification outside of mrcal
:PROPERTIES:
:CUSTOM_ID: stereo-without-mrcal
:END:

What if we want to do our stereo processing with some other tool, and what if
that tool doesn't support the splined model we want to use? We can use mrcal to
reproject the image to whatever model we like, and then hand off the processed
image and new models to that tool. Let's demonstrate with a pinhole model.

We can use the [[file:mrcal-reproject-image.html][=mrcal-reproject-image=]] tool to reproject the images. Mapping
fisheye images to a pinhole model introduces an unwinnable trade-off: the
angular resolution changes dramatically as you move towards the edges of the
image. At the edges the angular resolution becomes tiny, and you need far more
pixels to represent the same arc in space as you do in the center. So you
usually need to throw out pixels in the center, and gain low-information pixels
at the edges (the original image doesn't have more resolutions at the edges, so
we must interpolate). Cutting off the edges (i.e. using a narrower lens) helps
bring this back into balance.

So let's do this using two different focal lengths:

- =--scale-focal 0.35=: fairly wide. Looks extreme in a pinhole projection
- =--scale-focal 0.6=: not as wide. Looks more reasonable in a pinhole
  projection, but we cut off big chunks of the image at the edges

#+begin_src sh
for scale in 0.35 0.6; do
  for c in 0 1; do
    mrcal-reproject-image       \
      --valid-intrinsics-region \
      --to-pinhole              \
      --scale-focal $scale      \
      splined-$c.cameramodel    \
      $c.jpg                    \
    | mrcal-to-cahvor           \
    > splined-$c.scale$scale.cahvor;

    mv $c-reprojected{,-scale$scale}.jpg;
  done
done
#+end_src

We will use jplv (a stereo library used at NASA/JPL) to process these pinhole
images into a stereo map, so I converted the models to the [[file:cameramodels.org::#cameramodel-file-formats][=.cahvor= file
format]], as that tool expects.

The wider pinhole resampling of the two images:

[[file:external/data/figueroa-overpass-looking-S/0-reprojected-scale0.35.jpg][file:external/figures/stereo/0-reprojected-scale0.35.downsampled.jpg]]
[[file:external/data/figueroa-overpass-looking-S/1-reprojected-scale0.35.jpg][file:external/figures/stereo/1-reprojected-scale0.35.downsampled.jpg]]

The narrower resampling of the two images:

[[file:external/data/figueroa-overpass-looking-S/0-reprojected-scale0.6.jpg][file:external/figures/stereo/0-reprojected-scale0.6.downsampled.jpg]]
[[file:external/data/figueroa-overpass-looking-S/1-reprojected-scale0.6.jpg][file:external/figures/stereo/1-reprojected-scale0.6.downsampled.jpg]]

And the camera models:

- [[file:external/data/figueroa-overpass-looking-S/splined-0.scale0.35.cahvor][camera 0, wider scaling]]
- [[file:external/data/figueroa-overpass-looking-S/splined-1.scale0.35.cahvor][camera 1, wider scaling]]
- [[file:external/data/figueroa-overpass-looking-S/splined-0.scale0.6.cahvor][camera 0, narrower scaling]]
- [[file:external/data/figueroa-overpass-looking-S/splined-1.scale0.6.cahvor][camera 1, narrower scaling]]

Both clearly show the uneven resolution described above, with the wider image
being far more extreme. I can now use these images to compute stereo with jplv:

#+begin_src sh
for scale in 0.35 0.6; do \
  stereo --no-ran --no-disp --no-pre --corr-width 5 --corr-height 5 \
         --blob-area 10 --disp-min 0 --disp-max 400                 \
         splined-[01].scale$scale.cahvor                            \
         [01]-reprojected-scale$scale.jpg;

  for f in rect-left rect-right diag-left; do \
    mv 00-$f.png jplv-stereo-$f-scale$scale.png;
  done
done
#+end_src

The rectified images look like this.

For the wider mapping:

[[file:external/data/figueroa-overpass-looking-S/jplv-stereo-rect-left-scale0.35.png][file:external/figures/stereo/jplv-stereo-rect-left-scale0.35.downsampled.png]]
[[file:external/data/figueroa-overpass-looking-S/jplv-stereo-rect-right-scale0.35.png][file:external/figures/stereo/jplv-stereo-rect-right-scale0.35.downsampled.png]]

For the narrow mapping:

[[file:external/data/figueroa-overpass-looking-S/jplv-stereo-rect-left-scale0.6.png][file:external/figures/stereo/jplv-stereo-rect-left-scale0.6.downsampled.png]]
[[file:external/data/figueroa-overpass-looking-S/jplv-stereo-rect-right-scale0.6.png][file:external/figures/stereo/jplv-stereo-rect-right-scale0.6.downsampled.png]]

Here we see that jplv's rectification function uses a pinhole model, so the
scale within each row is dramatically uneven.

The above command gave me jplv's computed disparities, but to compare
apples-to-apples, let's re-compute them using the same OpenCV SGBM routine from
above:

#+begin_src sh
python3 stereo.py - - jplv-stereo-rect-{left,right}-scale0.35.png jplv-scale0.35
python3 stereo.py - - jplv-stereo-rect-{left,right}-scale0.6.png  jplv-scale0.6
#+end_src

*REDO OR THROW OUT THIS SECTION. SHOW JPLV DISPARITIES?*

#+begin_src sh
mrcal-stereo                     \
  --az-fov-deg 145               \
  --el-fov-deg 135               \
  --el0-deg    5                 \
  --disparity-range 0 400        \
  --sgbm-p1 600                  \
  --sgbm-p2 2400                 \
  --sgbm-uniqueness-ratio 5      \
  --sgbm-disp12-max-diff 1       \
  --sgbm-speckle-window-size 200 \
  --sgbm-speckle-range 2         \
  --valid-intrinsics-region      \
  splined-[01].scale0.35.cahvor  \
  jplv-stereo-rect-{left,right}-scale0.35.png
#+end_src
#+begin_src sh :exports none :eval no-export
zmv -f -W \
  '/tmp/[01]-rectified.png' \
  'doc/out/external/figures/stereo/rectified[01]-splined.png'

mv \
  /tmp/0-disparity.png \
  doc/out/external/figures/stereo/disparity-splined.png

mv \
  /tmp/0-range.png \
  doc/out/external/figures/stereo/range-splined.png

for img ( doc/out/external/figures/stereo/{rectified[01],disparity,range}-splined.png ) { \
  convert $img -scale 12% ${img:r}.downsampled.${img:e}
}
#+end_src

[[file:external/data/figueroa-overpass-looking-S/disparity-jplv-scale0.35.png][file:external/figures/stereo/disparity-jplv-scale0.35.downsampled.png]]
[[file:external/data/figueroa-overpass-looking-S/disparity-jplv-scale0.6.png][file:external/figures/stereo/disparity-jplv-scale0.6.downsampled.png]]

Looks reasonable.

* Splitting a wide view into multiple narrow views
:PROPERTIES:
:CUSTOM_ID: stereo-narrow
:END:

So we can use jplv to handle mrcal lenses in this way, but at the cost of
degraded feature-matching accuracy due to unevenly-scaled rectified images. A
way to resolve the geometric challenges of wide-angle lenses would be to
subdivide the wide field of view into multiple narrower virtual lenses. Then
we'd have several narrow-angle stereo pairs instead of a single wide stereo
pair.

mrcal makes necessary transformations simple, so let's do it. For
each image we need to construct

- The narrow pinhole model we want that looks at the area we want to (45 degrees
  to the left in this example)
- The image of the scene that such a model would have observed

This requires writing a little bit of code: [[https://www.github.com/dkogan/mrcal/blob/master/doc/narrow-section.py][=narrow-section.py=]]. Let's run that
for each of my images:

#+begin_src sh
python3 narrow-section.py splined-0.cameramodel 0.jpg -45 left
python3 narrow-section.py splined-1.cameramodel 1.jpg -45 right
#+end_src
#+begin_src sh :exports none :eval no-export
D=doc/out/external/data/figueroa-overpass-looking-S/
PYTHONPATH=/home/dima/projects/mrcal python3 doc/narrow-section.py $D/splined-0.cameramodel $D/0.jpg -45 left
PYTHONPATH=/home/dima/projects/mrcal python3 doc/narrow-section.py $D/splined-1.cameramodel $D/1.jpg -45 right

mv                                                     \
    /tmp/narrow-{left,right}.jpg                       \
    /tmp/pinhole-narrow-yawed-{left,right}.cameramodel \
    $D

for img ( $D/narrow-{left,right}.jpg ) { \
  convert $img -scale 12% doc/out/external/figures/stereo/${img:r:t}.downsampled.${img:e}
}
#+end_src

The images look like this:

[[file:external/data/figueroa-overpass-looking-S/narrow-left.jpg][file:external/figures/stereo/narrow-left.downsampled.jpg]]
[[file:external/data/figueroa-overpass-looking-S/narrow-right.jpg][file:external/figures/stereo/narrow-right.downsampled.jpg]]

Note that these are pinhole images, but the field of view is much more narrow,
so they don't look distorted like before. The corresponding pinhole models:

- [[file:external/data/figueroa-overpass-looking-S/pinhole-narrow-yawed-left.cameramodel][left]]
- [[file:external/data/figueroa-overpass-looking-S/pinhole-narrow-yawed-right.cameramodel][right]]

We can feed these to the [[file:mrcal-stereo.html][=mrcal-stereo=]] tool as before:

#+begin_src sh
mrcal-stereo                                    \
  --az-fov-deg 80                               \
  --el-fov-deg 80                               \
  --pixels-per-deg -0.5                         \
  --disparity-range 0 200                       \
  --sgbm-p1 600                                 \
  --sgbm-p2 2400                                \
  --sgbm-uniqueness-ratio 5                     \
  --sgbm-disp12-max-diff 1                      \
  --sgbm-speckle-window-size 200                \
  --sgbm-speckle-range 2                        \
  pinhole-narrow-yawed-{left,right}.cameramodel \
  narrow-{left,right}.jpg
#+end_src
#+begin_src sh :exports none :eval no-export
D=doc/out/external/data/figueroa-overpass-looking-S/

./mrcal-stereo                                    \
  --az-fov-deg 80                               \
  --el-fov-deg 80                               \
  --pixels-per-deg -0.5                         \
  --disparity-range 0 200                       \
  --sgbm-p1 600                                 \
  --sgbm-p2 2400                                \
  --sgbm-uniqueness-ratio 5                     \
  --sgbm-disp12-max-diff 1                      \
  --sgbm-speckle-window-size 200                \
  --sgbm-speckle-range 2                        \
  --outdir /tmp                                 \
  $D/pinhole-narrow-yawed-{left,right}.cameramodel \
  $D/narrow-{left,right}.jpg

mv \
  /tmp/narrow-left-rectified.png \
  $D/rectified0-narrow.png

mv \
  /tmp/narrow-right-rectified.png \
  $D/rectified1-narrow.png

mv \
  /tmp/narrow-left-disparity.png \
  $D/disparity-narrow.png

mv \
  /tmp/narrow-left-range.png \
  $D/range-narrow.png

for img ( $D/{rectified[01],disparity,range}-narrow.png ) { \
  convert $img -scale 12% doc/out/external/figures/stereo/${img:r:t}.downsampled.${img:e}
}
#+end_src

Here we have slightly non-trivial geometry, so it is instructive to visualize
it:

#+begin_src sh
mrcal-stereo                                    \
  --az-fov-deg 80                               \
  --el-fov-deg 80                               \
  --show-geometry                               \
  --title ''                                    \
  pinhole-narrow-yawed-{left,right}.cameramodel \
  narrow-{left,right}.jpg
#+end_src
#+begin_src sh :exports none :eval no-export
D=doc/out/external/data/figueroa-overpass-looking-S/

./mrcal-stereo                                    \
  --az-fov-deg 80                               \
  --el-fov-deg 80                               \
  --show-geometry \
  --title '' \
  --terminal 'svg size 800,600 noenhanced solid dynamic font ",14"' \
  --hardcopy doc/out/external/figures/stereo/stereo-geometry-narrow.svg \
  $D/pinhole-narrow-yawed-{left,right}.cameramodel \
  $D/narrow-{left,right}.jpg
#+end_src

[[file:external/figures/stereo/stereo-geometry-narrow.svg]]

Here we're looking at the left and right cameras in the stereo pair, /and/ at
the axes of the stereo system. Now that we have rotated each camera to look to
the left, the baseline is no longer perpendicular to the central axis of each
camera. The stereo system is still attached to the baseline, however. That means
that $\theta = 0$ no longer corresponds to the center of the view. We don't need
to care, however: [[file:mrcal-python-api-reference.html#-stereo_rectify_prepare][=mrcal.stereo_rectify_prepare()=]] figures that out, and
compensates.

And we get nice-looking rectified images:

[[file:external/data/figueroa-overpass-looking-S/rectified0-narrow.png][file:external/figures/stereo/rectified0-narrow.downsampled.png]]
[[file:external/data/figueroa-overpass-looking-S/rectified1-narrow.png][file:external/figures/stereo/rectified1-narrow.downsampled.png]]

And disparity and range images:

[[file:external/data/figueroa-overpass-looking-S/disparity-narrow.png][file:external/figures/stereo/disparity-narrow.downsampled.png]]
[[file:external/data/figueroa-overpass-looking-S/range-narrow.png][file:external/figures/stereo/range-narrow.downsampled.png]]

And this is despite running pinhole-reprojected stereo from a very wide lens.

* Range accuracy                                                   :noexport:
:PROPERTIES:
:CUSTOM_ID: stereo-range-accuracy
:END:
A good punchline to all this would be to show that we can now get great ranges,
and the splined model does better than the =LENSMODEL_OPENCV8= model. I'm not
reporting this because the full propagation of uncertainty from the calibration
to the extrinsics estimation to ranging isn't implemented yet. And until that is
done, the results are only easily interpretable if the splined model does 1000
times better, which it does not. I will write that eventually.

