#+TITLE: Triangulation methods and uncertainty
#+OPTIONS: toc:t

#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{argmin}
#+LATEX_HEADER: \DeclareMathOperator*{\Var}{Var}

#+BEGIN_HTML
\(
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Var}{Var}
\)
#+END_HTML

A very common thing to want to do with a calibrated camera system is to convert
a pair of pixel observations to a point in space that produced these
observations, a process known as [[https://en.wikipedia.org/wiki/Triangulation_(computer_vision)][triangulation]]. mrcal supports both sparse
triangulation (processing a small number of discrete pixel observations) and
dense triangulation (processing every pixel in a pair of images; [[file:stereo.org][stereo vision]]).
This can be sensitive to noise, creating a strong need for proper error modeling
and propagation. This describes the logic behind the [[file:mrcal-triangulate.html][=mrcal-triangulate=]] tool
and the [[file:mrcal-python-api-reference.html#-triangulate][=mrcal.triangulate()=]] Python routine.

* Overview
Let's say we have an idealized geometry:

[[file:figures/triangulation-symmetric.svg]]

Let $b \equiv \mathrm{baseline}$ and $r \equiv \mathrm{range}$. Two cameras are
looking at a point in space. Given two camera models and a pair of pixel
observations we can compute the range to the point. Basic geometry tells us that

\[\frac{r}{\sin \phi} = \frac{b}{\sin \theta}\]

When looking far away, straight ahead, we have $\theta \approx 0$ and $\phi \approx 90^\circ$, so

\[ r \approx \frac{b}{\theta}\]

Differentiating, we get

\[\frac{\mathrm{d}r}{\mathrm{d}\theta} \propto \frac{b}{\theta^2} \propto \frac{r^2}{b}\]


Thus a small error in $\theta$ causes an error in the computed range that is
proportional to the /square/ of $r$. This relationship sets the fundamental
limit for the ranging capabilities of stereo systems: if you try to look out too
far, the precision of $\theta$ required to get a precise-enough $r$ becomes
unattainable. And because we have $r^2$, this range limit is approached very
quickly. A bigger baseline helps, but does so only linearly.

The angle $\theta$ comes from the extrinsics and intrinsics in the camera model,
so the noise modeling and uncertainty propagation in mrcal are essential to a
usable long-range stereo system.

* Triangulation routines
Before we can talk about quantifying the uncertainty of a triangulation
operation, we should define what that operation is. Each triangulation operation
takes as input

- Two camera models. Intrinsics (lens behavior) and extrinsics (geometry) are
  required for both

- Pixel coordinates $\vec q$ of the same observed feature in the two images
  captured by each camera

And it outputs

- A 3D coordinate of the point $\vec p$ in space that produced the given pixel
  observations

The "right" way to implement this operation is to minimize the reprojection
error:

\[
E\left(\vec p\right) \equiv \left\lVert \vec q_0 - \mathrm{project}_\mathrm{cam0}\left(\vec p\right) \right\rVert^2 +
                            \left\lVert \vec q_1 - \mathrm{project}_\mathrm{cam1}\left(\vec p\right) \right\rVert^2
\]

\[
\vec p^* \equiv \argmin{E\left(\vec p\right)}
\]

This is correct, but it's complex and requires a nonlinear optimization, which
limits the usefulness of this approach. mrcal implements several
slightly-imprecise but /much/ faster methods to compute a triangulation. All of
these precompute $\vec v \equiv \mathrm{unproject} \left( \vec q \right)$, and
then operate purely geometrically. The methods are described in these papers,
listed in chronological order:

- "Triangulation Made Easy", Peter Lindstrom. IEEE Conference on Computer Vision
  and Pattern Recognition, 2010

- "Closed-Form Optimal Two-View Triangulation Based on Angular Errors", Seong Hun
  Lee and Javier Civera. https://arxiv.org/abs/1903.09115

- "Triangulation: Why Optimize?", Seong Hun Lee and Javier Civera
  https://arxiv.org/abs/1907.11917

The last paper compares the available methods from /all/ the papers. Currently
=leecivera_mid2= is recommended for most usages. The triangulation methods
available in mrcal:

** =geometric=
This is the basic [[https://en.wikipedia.org/wiki/Triangulation_(computer_vision)#Mid-point_method][midpoint method]]: it computes the point in space that minimizes
the distance between the two observation rays. This is the simplest method, but
also produces the most bias. Not recommended. Implemented in
[[file:mrcal-python-api-reference.html#-triangulate_geometric][=mrcal.triangulate_geometric()=]] (in Python) and [[https://www.github.com/dkogan/mrcal/blob/master/triangulation.h#mrcal_triangulate_geometric][=mrcal_triangulate_geometric()=]]
(in C).

** =lindstrom=
Described in the "Triangulation Made Easy" paper above. The method is a close
approximation to a reprojection error minimization (the "right" approach above)
/if we have pinhole lenses/. Implemented in [[file:mrcal-python-api-reference.html#-triangulate_lindstrom][=mrcal.triangulate_lindstrom()=]] (in
Python) and [[https://www.github.com/dkogan/mrcal/blob/master/triangulation.h#mrcal_triangulate_lindstrom][=mrcal_triangulate_lindstrom()=]] (in C).

** =leecivera_l1=
Described in the "Closed-Form Optimal Two-View Triangulation Based on Angular
Errors" paper above. Minimizes the L1 norm of the observation angle error.
Implemented in [[file:mrcal-python-api-reference.html#-triangulate_leecivera_l1][=mrcal.triangulate_leecivera_l1()=]] (in Python) and
[[https://www.github.com/dkogan/mrcal/blob/master/triangulation.h#mrcal_triangulate_leecivera_l1][=mrcal_triangulate_leecivera_l1()=]] (in C).

** =leecivera_linf=
Described in the "Closed-Form Optimal Two-View Triangulation Based on Angular
Errors" paper above. Minimizes the L-infinity norm of the observation angle
error. Implemented in [[file:mrcal-python-api-reference.html#-triangulate_leecivera_linf][=mrcal.triangulate_leecivera_linf()=]] (in Python) and
[[https://www.github.com/dkogan/mrcal/blob/master/triangulation.h#mrcal_triangulate_leecivera_linf][=mrcal_triangulate_leecivera_linf()=]] (in C).

** =leecivera_mid2=
Described in the "Triangulation: Why Optimize?" paper above: this is the "Mid2"
method. Doesn't explicitly minimize anything, but rather is a heuristic that
works well in practice. Implemented in [[file:mrcal-python-api-reference.html#-triangulate_leecivera_mid2][=mrcal.triangulate_leecivera_mid2()=]] (in
Python) and [[https://www.github.com/dkogan/mrcal/blob/master/triangulation.h#mrcal_triangulate_leecivera_mid2][=mrcal_triangulate_leecivera_mid2()=]] (in C).

** =leecivera_wmid2=
Described in the "Triangulation: Why Optimize?" paper above: this is the "wMid2"
method. Doesn't explicitly minimize anything, but rather is a heuristic that
works well in practice. Similar to =leecivera_mid2=, but contains a bit of extra
logic to improve the behavior for points very close to the cameras (not
satisfying $r \gg b$). Implemented in [[file:mrcal-python-api-reference.html#-triangulate_leecivera_wmid2][=mrcal.triangulate_leecivera_wmid2()=]] (in
Python) and [[https://www.github.com/dkogan/mrcal/blob/master/triangulation.h#mrcal_triangulate_leecivera_wmid2][=mrcal_triangulate_leecivera_wmid2()=]] (in C).

* Triangulation uncertainty
We compute the uncertainty of a triangulation operation using the usual
error-propagation technique:

- We define the input noise
- We compute the operation through which we're propagating this input noise,
  evaluating the gradients of the output in respect to all the noisy inputs
- We assume the behavior is locally linear and that the input noise is Gaussian,
  which allows us to easily compute the output noise using the usual
  noise-propagation relationship

** Noise sources
We want to capture the effect of two different sources of error:

- /Calibration-time/ noise. We propagate the noise in chessboard observations
  obtained during the chessboard dance. This is the [[file:formulation.org::#noise-model-inputs][noise]] that we propagate when
  evaluating [[file:uncertainty.org][projection uncertainty]]. This is specified in the
  =--q-calibration-stdev= argument to [[file:mrcal-triangulate.html][=mrcal-triangulate=]] or in the
  =q_calibration_stdev= argument to [[file:mrcal-python-api-reference.html#-triangulate][=mrcal.triangulate()=]]. This is usually known
  from the calibration, and we can request the calibrated value by passing a
  stdev of -1. See the relevant interface documentation (just-mentioned links)
  for details.
- /Observation-time/ noise. Each triangulation processes observations $\vec q$
  of a feature in space. These are noisy, and we propagate the noise. As with
  the calibration-time noise, this noise is assumed to be normally distributed,
  independent in $x$ and $y$. This is specified in the =--q-observation-stdev=
  argument to [[file:mrcal-triangulate.html][=mrcal-triangulate=]] or in the =q_observation_stdev= argument to
  [[file:mrcal-python-api-reference.html#-triangulate][=mrcal.triangulate()=]]. A common source of these pixel observations is a pixel
  correlation operation where a patch in one image is matched against the second
  image. Corresponding pixel observations observed this way are correlated: the
  noise in $\vec q_0$ not independent of the noise in $\vec q_1$. I do not yet
  know how to estimate this correlation, but the tools are able to ingest and
  propagate such an estimate: using the =--q-observation-stdev-correlation=
  commandline option to [[file:mrcal-triangulate.html][=mrcal-triangulate=]] or the
  =q_observation_stdev_correlation= argument to [[file:mrcal-python-api-reference.html#-triangulate][=mrcal.triangulate()=]].

A big point to note here is that repeated observations of the same feature have
independent observation-time noise. So these observation-time errors average out
with multiple observations. This is /not/ true of the calibration-time noise
however. Using the same calibration to observe a feature multiple times will
produce correlated triangulation results. So calibration-time noise is biased,
and it is thus essential to make and use low-uncertainty calibrations to
minimize this effect.

** Sample uncertainties
The [[https://github.com/dkogan/mrcal/blob/master/test/test-triangulation-uncertainty.py][=test-triangulation-uncertainty.py=]] test generates models and triangulation
scenarios. It can be used to produce an illustrative diagram:

#+begin_src sh
test/test-triangulation-uncertainty.py  \
  --do-sample                           \
  --cache write                         \
  --observed-point -2 0 10              \
  --fixed cam0                          \
  --Nsamples 200                        \
  --Ncameras 2                          \
  --q-observation-stdev-correlation 0.5 \
  --q-calibration-stdev 0.2             \
  --q-observation-stdev 0.2             \
  --make-documentation-plots
#+end_src
#+begin_src sh :exports none :eval no-export
test/test-triangulation-uncertainty.py  \
  --do-sample                           \
  --cache write                         \
  --observed-point -2 0 10              \
  --fixed cam0                          \
  --Nsamples 200                        \
  --Ncameras 2                          \
  --q-observation-stdev-correlation 0.5 \
  --q-calibration-stdev 0.2             \
  --q-observation-stdev 0.2             \
  --make-documentation-plots ~/projects/mrcal-doc-external/figures/triangulation/sample
#+end_src

[[file:external/figures/triangulation/sample--ellipses.svg]]

Here we have *two* cameras arranged in the usual left/right stereo
configuration, looking at *two* points somewhere ahead. We generate calibration
and observation noise, and display the results in the horizontal plane. The
vertical dimension is insignificant here, so it is not shown, even though all
the computations are performed in full 3D. For each of the two observed points
we display:

- The empirical noise samples, and the 1-sigma ellipse they represent
- The predicted 1-sigma ellipse for the calibration-time noise
- The predicted 1-sigma ellipse for the observation-time noise
- The predicted 1-sigma ellipse for the joint noise

We can see that the observed and predicted covariances line up nicely. We can
also see that the observation-time noise acts primarily in the forward/backward
direction, while the calibration-time noise has a much larger lateral effect.
This pattern varies greatly depending on the lenses and the calibration and the
geometry. As we get further out, the uncertainty in the forward/backward
direction dominates for both noise sources, as expected.

** Stabilization
In the above plot, the uncertainties are displayed in the coordinate system of
the left camera. But, as described on the [[file:uncertainty.org::#propagating-through-projection][projection uncertainty page]], the
origin and orientation of each camera's coordinate system is subject to
calibration noise:

[[file:figures/uncertainty.svg]]

So what we usually want to do is to consider the covariance of the triangulation
in the coordinates of the camera housing, /not/ the camera coordinate system. We
achieve this with "stabilization", computed exactly as described on the
[[file:uncertainty.org::#propagating-through-projection][projection uncertainty page]]. We can recompute the triangulation uncertainty in
the previous example (same geometry, lens, etc), but with stabilization enabled:

#+begin_src sh
test/test-triangulation-uncertainty.py  \
  --do-sample                           \
  --cache write                         \
  --observed-point -2 0 10              \
  --fixed cam0                          \
  --Nsamples 200                        \
  --Ncameras 2                          \
  --q-observation-stdev-correlation 0.5 \
  --q-calibration-stdev 0.2             \
  --q-observation-stdev 0.2             \
  --stabilize                           \
  --make-documentation-plots
#+end_src
#+begin_src sh :exports none :eval no-export
test/test-triangulation-uncertainty.py  \
  --do-sample                           \
  --cache write                         \
  --observed-point -2 0 10              \
  --fixed cam0                          \
  --Nsamples 200                        \
  --Ncameras 2                          \
  --q-observation-stdev-correlation 0.5 \
  --q-calibration-stdev 0.2             \
  --q-observation-stdev 0.2             \
  --stabilize                           \
  --make-documentation-plots ~/projects/mrcal-doc-external/figures/triangulation/sample-stabilized
#+end_src

[[file:external/figures/triangulation/sample-stabilized--ellipses.svg]]

We can now clearly see that the forward/backward uncertainty was a real effect,
/but/ the lateral uncertainty was largely due to the moving camera coordinate
system.

** Calibration-time noise produces correlated estimates
As mentioned above, the calibration-time noise produces correlations (and thus
biases) in the triangulated measurements. Since the
[[https://github.com/dkogan/mrcal/blob/master/test/test-triangulation-uncertainty.py][=test-triangulation-uncertainty.py=]] command triangulates two different points,
we can directly observe these correlations. Let's look at the magnitude of each
element of $\Var {\vec p_{01}}$ where $\vec p_{01}$ is a 6-dimensional vector
that contains both the triangulated 3D points: $\vec p_{01} \equiv
\left[ \begin{array}{cc} \vec p_0 \\ \vec p_1 \end{array} \right]$. If we had
/only/ observation-time noise, $\vec p_0$ and $\vec p_1$ would be independent,
and the off-diagonal terms in the covariance matrix would be 0. However, we also
have calibration-time noise, so the errors are correlated:

[[file:external/figures/triangulation/sample--p0-p1-magnitude-covariance.png]]

As before, the exact pattern varies greatly depending on the lenses and the
calibration and the geometry, but calibration-time noise always creates these
correlations. To reduce these correlations and the biases they cause: lower the
uncertainty of your calibrations by [[file:tour-choreography.org][dancing better]]

* Applying these techniques
** Object tracking
Visual tracking of an object over time is one application that would benefit
from a more complete error model of its input. Repeated noisy observations of a
moving object $\vec q_{01}(t)$ can be triangulated into a noisy estimate of the
object motion $\vec p(t)$. If for each point in time $t$ we have $\Var \vec
p(t)$, we can combine everything into an estimate $\hat p(t)$. The better our
covariances, the closer the estimate. The [[file:mrcal-python-api-reference.html#-triangulate][=mrcal.triangulate()=]] routine can be
used to compute the triangulations, and to report the full covariances matrices.

** Ranging with diagnostics
The existing tools can be used to compute a single triangulation operation, with
lots of diagnostics. This is very useful on its own, or to debug a bigger
ranging system.

Let's use the Downtown Los Angeles images in the [[file:tour-stereo.org][the tour of mrcal]]. Before we
start, one important caveat: there's only one camera, which was calibrated
monocularly. I moved this camera to capture the two images used to triangulate.
The extrinsics were computed with a not-yet-in-mrcal tool, and mrcal cannot yet
propagate the calibration noise in this scenario. Thus we only propagate the
observation-time noise here.

Image from the left camera:

[[file:external/data/figueroa-overpass-looking-S/0.jpg][file:external/figures/stereo/0.downsampled.jpg]]

Let's compute the range to the top of the [[https://en.wikipedia.org/wiki/City_National_Plaza]["Paul Hastings" tower]], near the center
of the image. I'm looking at the "Paul Hastings" logo, roughly 566m from the
camera. I have a pixel coordinate on the logo. This is enough information to
triangulate:

#+begin_example
$ mrcal-triangulate                       \
    --range-estimate 566                  \
    --q-observation-stdev 0.2             \
    --q-observation-stdev-correlation 0.5 \
    --stabilize-coords                    \
    --template-size 31 17                 \
    --search-radius 10                    \
    --viz match                           \
    splined-[01].cameramodel              \
    [01].jpg                              \
    2874 1231 

## Feature [2874. 1231.] in the left image corresponds to [2832.473 1234.974] at 566.0m
## Feature match found at [2831.536 1233.916]
## q1 - q1_perfect_at_range = [-0.937 -1.058]
## Triangulated point at [ -41.153 -165.328  473.295]; direction: [-0.082 -0.328  0.941]
## Range: 503.03 m (error: -62.97 m)
## q0 - q0_triangulation = [-0.02   0.531]
## Uncertainty propagation: observation-time noise suggests worst confidence of sigma=13.70m along [ 0.084  0.329 -0.941]
## Observed-pixel sensitivity: 55.94m/pixel (q1). Worst direction: [0.99  0.139]. Linearized correction: 1.13 pixels
## Calibration yaw (rotation in epipolar plane) sensitivity: -2067.73m/deg. Linearized correction: -0.030 degrees of yaw
## Calibration yaw (cam0 y axis)                sensitivity: -1949.28m/deg. Linearized correction: -0.032 degrees of yaw
## Calibration pitch (tilt of epipolar plane) sensitivity: 246.88m/deg.
## Calibration translation sensitivity: 236.83m/m. Worst direction: [0.986 0.    0.165]. Linearized correction: 0.27 meters of translation
## Optimized yaw   (rotation in epipolar plane) correction = -0.025 degrees
## Optimized pitch (tilt of epipolar plane)     correction = 0.029 degrees
## Optimized relative yaw (1 <- 0): -1.365 degrees
#+end_example
#+begin_src sh :exports none :eval no-export
D=../mrcal-doc-external/data/figueroa-overpass-looking-S
./mrcal-triangulate                       \
    --range-estimate 566                  \
    --q-observation-stdev 0.2             \
    --q-observation-stdev-correlation 0.5 \
    --stabilize-coords                    \
    --template-size 31 17                 \
    --search-radius  10                   \
    --viz uncertainty                         \
    --hardcopy ../mrcal-doc-external/figures/triangulation/figueroa-ellipse.svg \
    $D/splined-[01].cameramodel           \
    $D/[01].jpg                           \
    2874 1231 
#+end_src

We used the splined model computed in [[file:tour-stereo.org][the tour of mrcal]]. We gave it the range
estimate. And we gave it the expected observation noise level: 0.2 pixels. We
declared the left-camera/right-camera pixel observations to be correlated with a
factor of 0.5 on the stdev, so the relevant cross terms of the covariance are
(0.2*0.5 pixels)^2. It's not yet clear how to get the true value of this
correlation, but we can use this tool to gauge its effects.

The [[file:mrcal-triangulate.html][=mrcal-triangulate=]] tool finds the corresponding feature in the second
image, and the =--viz match= pops up an interactive window so that a human can
validate the match (which is good here). We could also pass =--viz uncertainty=,
which shows the uncertainty ellipse. Unless we're looking very close, this
ellipse is almost always extremely long and extremely skinny. Here we have:

[[file:external/figures/triangulation/figueroa-ellipse.svg]]

So /looking/ at the ellipse usually isn't very useful, and the value printed in
the statistics presents the same information better. We get /lots/ of reported
statistics. We see that

- The range we compute here is 503.03m, not 566m as desired
- There's a vertical shift 0.531pixels between the triangulated point and the
  observation in the left camera: the epipolar lines aren't quite aligned, which
  means the calibration is a bit off. Either in the intrinsics or the extrinsics
- With the given observation noise, the 1-sigma uncertainty in the range is
  13.70m, almost exactly in the observation direction. This is significantly
  smaller than the actual error of 62.97m, which could be explained by our
  made-up values of =--q-observation-stdev= and
  =--q-observation-stdev-correlation=
- Moving the matched feature coordinate in the right image affects the range at
  worst at a rate of 55.94m/pixel. Unsurprisingly, the most sensitive direction
  of motion is left/right. At this rate, it would take 1.13 pixels of motion to
  "fix" our range measurement
- Similarly, we compute and report the range sensitivity of extrinsic yaw
  (defined as the rotation in the epipolar plane or around the y axis of the
  left camera). In either case, an extrinsics yaw shift of 0.03 degrees would
  "fix" the range measurement.
- We also compute sensitivities for pitch and translation, but we don't expect
  those to affect the range very much, and we see that
- Finally, we reoptimize the extrinsics, and compute a better yaw correction to
  "fix" the range: 0.025 degrees. This is different from the previous value of
  0.03 degrees because that computation used a linearized yaw-vs-range
  dependence

This is all quite useful, and suggests that a small extrinsics error is a
problem.

What about =--q-observation-stdev-correlation=? What would be the effect of more
or less correlation in our pixel observations? Running the same command with

- =--q-observation-stdev-correlation 0= (the left and right pixel observations
  are independent) produces

  #+begin_example
## Uncertainty propagation: observation-time noise suggests worst confidence of sigma=15.82m along [ 0.084  0.329 -0.941]
  #+end_example


- =--q-observation-stdev-correlation 1= (the left and right pixel observations
  are perfectly coupled) produces

  #+begin_example
## Uncertainty propagation: observation-time noise suggests worst confidence of sigma=0.26m along [-0.11  -0.153  0.982]
  #+end_example

I.e. correlations in the pixel measurements decrease our range uncertainty. To
the point where perfectly-correlated observations produce almost perfect
ranging. We'll still have range errors, but they would come from other sources
than slightly mismatched feature observations.
