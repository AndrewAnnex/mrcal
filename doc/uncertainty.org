#+TITLE: Projection uncertainty
#+OPTIONS: toc:t

After a calibration has been computed, it is essential to get a sense of how
good the calibration is (how closely it represents reality). Traditional
(non-mrcal) calibration routines rely on one metric of calibration quality: the
residual fit error. This is clearly inadequate because we can always improve
this metric by throwing away some input data, and it doesn't make sense that
using less data would make a calibration /better/.

There are two main sources of error in the calibration solve. Without these
errors, the calibration data would fit perfectly, producing a solve residual
vector that's exactly $\vec 0$. The two sources of error are:

- *Sampling error*. Our computations are based on fitting a model to
  observations of chessboard corners. These observations aren't perfect, and
  contain a sample of some noise distribution. We can [[file:formulation.org::#noise-model][characterize this
  distribution]] and we can analytically predict the effects of that noise

- *Model error*. These result when the solver's model of the world is
  insufficient to describe what is actually happening. When model errors are
  present, even the best set of parameters aren't able to completely fit the
  data. Some sources of model errors: motion blur, unsynchronized cameras,
  chessboard detector errors, too-simple (or unstable) [[file:lensmodels.org][lens models]] or chessboard
  deformation models, and so on. Since these errors are unmodeled (by
  definition), we can't analytically predict their effects. Instead we try hard
  to force these errors to zero, so that we can ignore them. We do this by using
  rich-enough models and by gathering clean data. To detect model errors we
  [[file:how-to-calibrate.org::#interpreting-results][look at the solve
  diagnostics]] and we compute [[file:tour-cross-validation.org][cross-validation diffs]].

Let's do as much as we can analytically: let's gauge the effects of sampling
error by computing a /projection uncertainty/ for a model. Since /only/ the
sampling error is evaluated:

*Any promises of a high-quality low-uncertainty calibration are valid only if
the model errors are small*.

The method to estimate the projection uncertainty is accessed via the
[[file:mrcal-python-api-reference.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]] function. Here the "uncertainty" is the
sensitivity to sampling error: the calibration-time pixel noise. This tells us
how good a calibration is (we aim for low projection uncertainties), and it can
tell us how good the downstream results are as well (by propagating projection
uncertainties through the downstream computation).

To estimate the projection uncertainty we:

1. Estimate the [[file:formulation.org::#noise-model-inputs][noise in the chessboard observations]]
2. Propagate that noise to the optimal parameters $\vec b^*$ reported by the
   calibration routine
3. Propagate the uncertainty in calibration parameters $\vec b^*$ through the
   projection function to get uncertainty in the resulting pixel coordinate $\vec
   q$

This overall approach is sound, but it implies some limitations:

- Once again, model errors are not included in this uncertainty estimate

- The choice of lens model affects the reported uncertainties. Lean models
  (those with few parameters) are less flexible than rich models, and don't fit
  general lenses as well as rich models do. This stiffness also serves to limit
  the model's response to noise in their parameters. Thus the above method will
  report less uncertainty for leaner models than rich models. So, unless we're
  /sure/ that a given lens follows some particular lens model perfectly, a
  [[file:splined-models.org][splined lens model]] (i.e. a very rich model) is recommended for truthful
  uncertainty reporting. Otherwise the reported confidence comes from the model
  itself, rather than the calibration data.

* Estimating the input noise
We're measuring the sensitivity to the noise in the calibration-time
observations. In order to propagate this noise, we need to know what that input
noise is. The current approach is described in the [[file:formulation.org::#noise-model][optimization problem
formulation]].

* Propagating input noise to the state vector
:PROPERTIES:
:CUSTOM_ID: propagating-to-state-vector
:END:

We solved the [[file:formulation.org][least squares problem]], so we have the optimal state vector $\vec
b^*$.

We apply a perturbation to the observations $\vec q_\mathrm{ref}$, reoptimize
this slightly-perturbed least-squares problem, assuming everything is linear,
and look at what happens to the optimal state vector $\vec b^*$.

We have

\[ E \equiv \left \Vert \vec x \right \Vert ^2 \]
\[ J \equiv \frac{\partial \vec x}{\partial \vec b} \]

At the optimum $E$ is minimized, so

\[ \frac{\partial E}{\partial \vec b} \left(\vec b = \vec b^* \right) = 2 J^T \vec x^* = 0 \]

We perturb the problem:

\[ E( \vec b + \Delta \vec b, \vec q_\mathrm{ref} + \Delta \vec q_\mathrm{ref})) \approx
\left \Vert \vec x + \frac{\partial \vec x}{\partial \vec b} \Delta \vec b + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} \Delta \vec q_\mathrm{ref} \right \Vert ^2 =
\left \Vert \vec x + J \Delta \vec b + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} \Delta \vec q_\mathrm{ref} \right \Vert ^2 \]

And we reoptimize:

\[ \frac{\mathrm{d}E}{\mathrm{d}\Delta \vec b} \approx 
2 \left( \vec x + J \Delta \vec b + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \right)^T J = 0\]

We started at an optimum, so $\vec x = \vec x^*$ and $J^T \vec x^* = 0$, and thus

\[ J^T J \Delta \vec b = -J^T \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \]

As defined on the [[file:formulation.org::#noise-model][input noise page]], we have

\[ \vec x_\mathrm{observations} = W (\vec q - \vec q_\mathrm{ref}) \]

where $W$ is a diagonal matrix of weights. These are the only elements of $\vec
x$ that depend on $\vec q_\mathrm{ref}$. Let's assume the non-observation
elements of $\vec x$ are at the end, so

\[ \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} =
\left[ \begin{array}{cc} - W \\ 0 \end{array} \right] \]

and thus

\[ J^T J \Delta \vec b = J_\mathrm{observations}^T W \Delta \vec q_\mathrm{ref} \]

So if we perturb the input observation vector $q_\mathrm{ref}$ by $\Delta
q_\mathrm{ref}$, the resulting effect on the optimal parameters is $\Delta \vec
b = M \Delta \vec q_\mathrm{ref}$ where

\[ M = \left( J^T J \right)^{-1} J_\mathrm{observations}^T W \]

As usual,

\[ \mathrm{Var}\left(\vec b\right) = M \mathrm{Var}\left(\vec q_\mathrm{ref}\right) M^T \]

As stated on the [[file:formulation.org::#noise-model][input noise page]], we're assuming independent noise on all
observed pixels, with a standard deviation inversely proportional to the weight:

\[ \mathrm{Var}\left( \vec q_\mathrm{ref} \right) = \sigma^2 W^{-2} \]

so

\begin{aligned}
\mathrm{Var}\left(\vec b\right) &= \sigma^2 M W^{-2} M^T \\
&= \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T W W^{-2} W J_\mathrm{observations} \left( J^T J \right)^{-1} \\
&= \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T J_\mathrm{observations}  \left( J^T J \right)^{-1}
\end{aligned}

If we have no regularization, then $J_\mathrm{observations} = J$ and we can
simplify even further:

\[\mathrm{Var}\left(\vec b\right) = \sigma^2 \left( J^T J \right)^{-1} \]

Note that these expressions do not explicitly depend on $W$, but the weights
still have an effect, since they are a part of $J$. So if an
observation $i$ were to become less precise, $w_i$ and $x_i$ and $J_i$ would all
decrease. And as a result, $\mathrm{Var}\left(\vec b\right)$ would increase, as
expected.

* Propagating the state vector noise through projection
:PROPERTIES:
:CUSTOM_ID: propagating-through-projection
:END:
We now have the variance of the full optimization state $\vec b$, and we can
propagate this to evaluate the uncertainty of any component of the solve. Here I
focus on the uncertainty of the *intrinsics*, since this is the biggest issue in
most calibration tasks.

I want to propagate $\mathrm{Var}\left(\vec b\right)$ through projection to get
the projection uncertainty at any given pixel $\vec q$. This is challenging
because we reoptimize with each new sample of input noise $\Delta \vec
q_\mathrm{ref}$, and each optimization moves around all the coordinate systems:

[[file:figures/uncertainty.svg]]

Thus evaluating the projection uncertainty of a $\vec p_\mathrm{cam}$, a point
in camera coordinates is not meaningful: the coordinate system itself moves with
each re-optimization. Currently mrcal has multiple methods to address this.
First I describe the /cross-reprojection/ uncertainty method: the new method
available in mrcal 3.0. This is the preferred formulation. Then, for
completeness, I describe the /mean-frames/ method: the simpler but somewhat
problematic method used in earlier versions of mrcal.

** Cross-reprojection uncertainty

Let's say I have a baseline solve (parameter vector $\vec b$) and a perturbed
solve (parameter vector $\vec b^+$) obtained from perturbing the observations
$\vec q_\mathrm{ref}$ and re-optimizing. I also have an arbitrary baseline query
pixel $\vec q$ and distance from which I compute the perturbed reprojection
$\vec q^+$.

I need to eventually compute $\mathrm{Var}\left(\vec q^+\right)$. I linearize
everything to get

\[
\Delta \vec q^+ \approx \frac{\partial\vec q^+}{\partial\vec b} \frac{\partial\vec b}{\partial\vec q_\mathrm{ref}}
\Delta \vec q_\mathrm{ref}
\]

Let

\[
P \equiv \frac{\partial\vec q^+}{\partial\vec b}
\]

and

\[
M \equiv \frac{\partial\vec b}{\partial\vec q_\mathrm{ref}}
\]

Then

\[
\Delta \vec q^+ \approx P M \Delta \vec q_\mathrm{ref}
\]

And

\[
\mathrm{Var} \left( \vec q^+ \right) = P M \mathrm{Var} \left( \vec q_\mathrm{ref} \right) M^T P^T
\]

I have $M$ from the [[#propagating-to-state-vector][above uncertainty propagation logic]], so I just need $P$.

In my usual least squares solve using chessboards, each point produces two
elements (horizontal, vertical error) of the measurements vector:

\[
\vec x_i = W \left( \mathrm{project}\left(\vec b_\mathrm{intrinsics}, T_\mathrm{cr} \, T_\mathrm{rf} \, \vec p_i \right) -
\vec q_{\mathrm{ref}_i} \right)
\]

The data flow is:

\[
\xymatrix{
\vec p_\mathrm{board}   \ar[d]^{T_\mathrm{rf}} \\
\vec p_\mathrm{ref}     \ar[d]^{T_\mathrm{cr}} \\
\vec p_\mathrm{cam}     \ar[d]^{\vec b_\mathrm{intrinsics} } \\
\vec q
}
\]

The optimization vector $\vec b$ contains the calibration object warp, which
affects $\vec p_\mathrm{board}$, and it contains $T_\mathrm{cr}$,
$T_\mathrm{rf}$ and $\vec b_\mathrm{intrinsics}$. When optimizing discrete
points the flow is slightly different:

\[
\xymatrix{
\vec p_\mathrm{point}   \ar[d]^{T_\mathrm{cr}} \\
\vec p_\mathrm{cam}     \ar[d]^{\vec b_\mathrm{intrinsics} } \\
\vec q
}
\]

Each point is defined in the reference coordinate system, and each point
coordinate is stored in $\vec b$. The analysis is very similar for the two cases
(boards, points). In this writeup I mostly focus on boards, but the
implementation supports both formulations.

If we perturb $\vec q_\mathrm{ref}$ then we have a different optimization, with
different quantities in this same data flow:

\[
\xymatrix{
\vec p^+_\mathrm{board}  \ar[d]^{T_\mathrm{r^+f^+}} \\
\vec p^+_\mathrm{ref}    \ar[d]^{T_\mathrm{c^+r^+}} \\
\vec p^+_\mathrm{cam}    \ar[d]^{\vec b^+_\mathrm{intrinsics} } \\
\vec q^+
}
\]

As noted above, all the coordinate systems have shifted, so the two
optimizations aren't directly comparable. In order to gauge the effect of
$\Delta \vec q_\mathrm{ref}$ I optimize the "cross-reprojection error": I treat
the two optimized worlds (unperturbed, perturbed) contant, and I compute an
optimal transformation $T_\mathrm{rr^+}$ to relate the two sets of coordinate
systems. I do this by solving the original optimization problem, but using half
of the data flow from each of the unperturbed and perturbed optimizations. The
data flow in this cross-reprojection optimization appears in red:

\[
\xymatrix{
  \vec p  _\mathrm{board}  \ar[d]^{T_\mathrm{rf}}
& {\color{red} \vec p^+_\mathrm{board}}  \ar@[red][d]^{\color{red} T_\mathrm{r^+f^+}} \\
  {\color{red} \vec p  _\mathrm{ref}}    \ar@[red][d]^{\color{red} T_\mathrm{cr}}
& {\color{red} \vec p^+_\mathrm{ref}}    \ar[d]^{T_\mathrm{c^+r^+}}
  \ar@[red][l]^{\color{red} T_\mathrm{rr^+}} \\
  {\color{red} \vec p  _\mathrm{cam}}    \ar@[red][d]^{\color{red} \vec b_\mathrm{intrinsics} }
& \vec p^+_\mathrm{cam}    \ar[d]^{\vec b^+_\mathrm{intrinsics} } \\
  {\color{red} \vec q}
& \vec q^+
}
\]

All the quantities *except* $T_\mathrm{rr^+}$ are available in the perturbed and
unperturbed optimizations. From those quantities we can compute the optimal
$T_\mathrm{rr^+}$.

In this read data flow I look at the /perturbed/ chessboard,frames,points and
the /unperturbed/ camera intrinsics, extrinsics. Note that I could have
traversed this data flow diagram from the top-left to the bottom-right instead.
This produces a [[#cross-reprojection-rt-rpr][different formulation, described below]].

So let's compute $T_\mathrm{rr^+}$. For a given perturbation $\Delta \vec
q_\mathrm{ref}$ I compute an optimization

\[
\vec{ \mathrm{rt}_\mathrm{rr^+}} = \mathrm{argmin}\Vert \vec x_\mathrm{cross} \Vert^2
\]

where

\begin{aligned}
\vec x_\mathrm{cross} \equiv \,
& W_\mathrm{board} \left( \mathrm{project}\left(\vec b_\mathrm{intrinsics},
                  T_\mathrm{cr} T_\mathrm{rr^+} T_\mathrm{r^+f^+} \vec p^+_\mathrm{board}\right)
  - \vec q_\mathrm{refboard} \right) + \\
& W_\mathrm{point} \left( \mathrm{project}\left(\vec b_\mathrm{intrinsics},
                  T_\mathrm{cr} T_\mathrm{rr^+} \vec p^+_\mathrm{point}\right)
  -  \vec q_\mathrm{refpoint} \right)
\end{aligned}

$\vec{ \mathrm{rt}_\mathrm{rr^+}}$ is a single transform (a vector with 6
elements, regardless of how many cameras or observations we have). This vector
defines the $T_\mathrm{rr^+}$ transform.

I optimize $\Vert\vec x_\mathrm{cross}\Vert^2$ by making a linearity assumption,
and taking a Newton step from the operating point $\vec
{\mathrm{rt}_\mathrm{rr^+}} = 0$. The baseline $\vec x_\mathrm{cross_0}$ comes
from the above expression at the operating point. Furthermore I have

\[
J_\mathrm{cross} \equiv
\frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}}
\]

I assume everything is locally linear, so $\Delta \vec x_\mathrm{cross} \approx
J_\mathrm{cross} \Delta \vec{\mathrm{rt}_\mathrm{rr^+}}$. I minimize $E \equiv
\Vert \vec x_\mathrm{cross_0} + \Delta \vec x_\mathrm{cross}\Vert^2$ by setting
the derivative to $\vec 0$:

\[
0 = \frac{\partial E}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \propto (\vec x_\mathrm{cross_0} + \Delta \vec x_\mathrm{cross})^T J_\mathrm{cross}
\]

So

\begin{aligned}
J_\mathrm{cross}^T \vec x_\mathrm{cross_0} &= -J_\mathrm{cross}^T \Delta \vec x_\mathrm{cross} \\
& \approx -J_\mathrm{cross}^T J_\mathrm{cross} \Delta \vec{\mathrm{rt}_\mathrm{rr^+}}
\end{aligned}

and

\[
\Delta \vec{\mathrm{rt}_\mathrm{rr^+}} \approx -\left(J_\mathrm{cross}^T J_\mathrm{cross}\right)^{-1} J_\mathrm{cross}^T \vec x_\mathrm{cross_0 }
\]

The operating point is at $\vec{\mathrm{rt}_\mathrm{rr^+}} = 0$ so

\begin{aligned}
\vec{\mathrm{rt}_\mathrm{rr^+}} &= 0 + \Delta \vec{\mathrm{rt}_\mathrm{rr^+}} \\
                                &= -\left(J_\mathrm{cross}^T J_\mathrm{cross}\right)^{-1} J_\mathrm{cross}^T \vec x_\mathrm{cross_0}
\end{aligned}

This is good, but requires that $\vec x_\mathrm{cross}$ and $J_\mathrm{cross}$
be computed directly. We can do better.

Since everything I'm looking at is near the original solution to the main
optimization problem, I can look at /everything/ in the linear space defined by
the optimal measurements $\vec x^*$ and their gradient $J$:

\[
\vec x \approx \vec x_0 + J \Delta \vec b
\]

Once again, we have this data flow:

\[
\xymatrix{
  \vec p  _\mathrm{board}  \ar[d]^{T_\mathrm{rf}}
& {\color{red} \vec p^+_\mathrm{board}}  \ar@[red][d]^{\color{red} T_\mathrm{r^+f^+}} \\
  {\color{red} \vec p  _\mathrm{ref}}    \ar@[red][d]^{\color{red} T_\mathrm{cr}}
& {\color{red} \vec p^+_\mathrm{ref}}    \ar[d]^{T_\mathrm{c^+r^+}}
  \ar@[red][l]^{\color{red} T_\mathrm{rr^+}} \\
  {\color{red} \vec p  _\mathrm{cam}}    \ar@[red][d]^{\color{red} \vec b_\mathrm{intrinsics} }
& \vec p^+_\mathrm{cam}    \ar[d]^{\vec b^+_\mathrm{intrinsics} } \\
  {\color{red} \vec q}
& \vec q^+
}
\]

implying this cost vector:

\begin{aligned}
\vec x_\mathrm{cross} \equiv \,
& W_\mathrm{board} \left( \mathrm{project}\left(\vec b_\mathrm{intrinsics},
                  T_\mathrm{cr} T_\mathrm{rr^+} T_\mathrm{r^+f^+} \vec p^+_\mathrm{board}\right)
  - \vec q_\mathrm{refboard} \right) + \\
& W_\mathrm{point} \left( \mathrm{project}\left(\vec b_\mathrm{intrinsics},
                  T_\mathrm{cr} T_\mathrm{rr^+} \vec p^+_\mathrm{point}\right)
  -  \vec q_\mathrm{refpoint} \right)
\end{aligned}

I evaluate $\vec x_\mathrm{cross_0}$ at $\vec{\mathrm{rt}_\mathrm{rr^+}} = 0$.
This is exactly the $\vec x^*$ from the original optimization, except I perturb
$\vec b_\mathrm{frames}$ and $\vec b_\mathrm{points}$ and $\vec
b_\mathrm{calobjectwarp}$:

\begin{aligned}
\vec x_\mathrm{cross_0} = \, & \vec x^* +
J_\mathrm{frames,points,calobjectwarp} \Delta \vec b_\mathrm{frames,points,calobjectwarp} \\
= \, & \vec x^* +
J_\mathrm{frames,points,calobjectwarp}  M_\mathrm{frames,points,calobjectwarp} \Delta \vec q_\mathrm{ref}
\end{aligned}

I can evaluate $J_\mathrm{cross}$ in two different ways:

- a $\vec {\mathrm{rt}_\mathrm{cr}}$ shift to $\mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{cr}},\vec{\mathrm{rt}_\mathrm{rr^+}} \right)$. So
  \begin{aligned}
  J_{\mathrm{cross}_\mathrm{e}} & = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
  & = J_\mathrm{extrinsics} \frac{\partial \mathrm{rt}_\mathrm{cr^+}}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
  & = J_\mathrm{extrinsics} \frac{\partial \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{cr}},\vec{\mathrm{rt}_\mathrm{rr^+}} \right)}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}}
  \end{aligned}

  For observations that have no extrinsics (the camera is defined to sit at the
  referene coord system) this formulation is not possible. Because there is no
  $J_\mathrm{extrinsics}$

- a $\vec {\mathrm{rt}_\mathrm{rf}}$ shift to $\mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{rr^+}},\mathrm{rt}_\mathrm{r^+f^+}\right)$ and/or a point
  shift to $T_\mathrm{rr^+} \vec p^+$

  $\vec{\mathrm{rt}_\mathrm{r^+f^+}}$ is a tiny shift off
  $\vec{\mathrm{rt}_\mathrm{rf}}$ /and/ I'm assuming that everything is locally
  linear. So this shift is insignificant for evaluating the gradient, and I use
  $\vec{\mathrm{rt}_\mathrm{rf}}$ to compute the gradient instead. Similarly for
  $p^+$ and $p$:

  \begin{aligned}
  J_{\mathrm{cross}_\mathrm{f}} & = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
            & =       J_\mathrm{frame}  \frac{\partial \mathrm{rt}_\mathrm{rf^+}}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
            & =       J_\mathrm{frame}  \frac{\partial \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{rr^+}},\mathrm{rt}_\mathrm{r^+f^+}\right)}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
            & \approx J_\mathrm{frame}  \frac{\partial \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{rr^+}},\mathrm{rt}_\mathrm{rf}\right)}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}}
 \\
  J_{\mathrm{cross}_\mathrm{p}} & = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
            & =       J_\mathrm{points} \frac{\partial p^+}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
            & =       J_\mathrm{points} \frac{\partial T_\mathrm{rr^+} p^+}{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
            & \approx J_\mathrm{points} \frac{\partial T_\mathrm{rr^+} p  }{\partial \vec{\mathrm{rt}_\mathrm{rr^+}}} \\
  \end{aligned}



NEED THE SAME THING FOR CALOBJECTWARP



There's one more simplification available. The original optimization problem was
solved, so we have $\frac{\partial E}{\partial \vec b} =
\frac{\partial}{\partial \vec b} \Vert \vec x \Vert^2 = 0$, and thus $J^T \vec
x^* = 0$.

From above:

\[
\vec x_\mathrm{cross_0} = \vec x_* +
J_\mathrm{frames,points,calobjectwarp}  M_\mathrm{frames,points,calobjectwarp} \Delta \vec q_\mathrm{ref}
\]

We can combine those two to simplify
\begin{aligned}
\vec{\mathrm{rt}_\mathrm{rr^+}} &= -\left(J_\mathrm{cross}^T J_\mathrm{cross}\right)^{-1} J_\mathrm{cross}^T \vec x_\mathrm{cross_0} \\
&= \cdots J_\mathrm{some\_state\_subset}^T \vec x_\mathrm{cross_0} \\
&= \cdots J_\mathrm{some\_state\_subset}^T \left(\vec x^* + \Delta \vec x\right) \\
&= \cdots J_\mathrm{some\_state\_subset}^T \Delta \vec x \\
&= -\left(J_\mathrm{cross}^T J_\mathrm{cross}\right)^{-1} J_\mathrm{cross}^T \Delta \vec x_\mathrm{cross_0}
\end{aligned}

So instead of $\vec x_\mathrm{cross_0}$ we can use

\begin{aligned}
\Delta \vec x_\mathrm{cross_0} = \, & J_\mathrm{frames,points,calobjectwarp} \Delta \vec b_\mathrm{frames,points,calobjectwarp} \\
& J_\mathrm{frames,points,calobjectwarp}  M_\mathrm{frames,points,calobjectwarp} \Delta \vec q_\mathrm{ref}
\end{aligned}

So we have $\vec{\mathrm{rt}_\mathrm{rr^+}} = K \Delta \vec b$ for some $K$ that
depends on the various $J$ matrices that are constant for each solve.

Now that I have $\vec{\mathrm{rt}_\mathrm{rr^+}}$, I can use it to compute $\vec
q^+$. This can accept arbitrary $\vec q$, not just those in the solve, so I
actually need to compute projections, rather than looking at a linearized space
defined by $J$. I traverse the data flow diagram in a different direction to compute
$\vec q^+$ and then $\mathrm{Var}\left(\vec q^+\right)$:

\[
\xymatrix{
  {\vec p  _\mathrm{ref}} \ar[r]^{T_\mathrm{r^+r}}
& {\vec p^+_\mathrm{ref}}    \ar[d]^{T_\mathrm{c^+r^+}} \\
  {\vec p  _\mathrm{cam}} \ar[u]_{T_\mathrm{rc}}
& {\vec p^+_\mathrm{cam}}    \ar[d]^{\vec b^+_\mathrm{intrinsics} } \\
  {\vec q} \ar[u]_{\vec b_\mathrm{intrinsics} }
& {\vec q^+}
}
\]

So
\begin{aligned}
\vec p_\mathrm{ref}   & = T_\mathrm{rc} \mathrm{unproject}\left(\vec b_\mathrm{intrinsics}, \vec q\right) \\
\vec p^+_\mathrm{ref} & = T_\mathrm{r^+r} \vec p_\mathrm{ref} \\
\vec p^+_\mathrm{cam} & = T_\mathrm{c^+r^+} \vec p^+_\mathrm{ref} \\
\vec q^+              & = \mathrm{project}\left(\vec b^+_\mathrm{intrinsics}, \vec p^+_\mathrm{cam}\right)
\end{aligned}

I can thus compute the gradient of $\vec q^+$ in respect to all the variables,
and I can propagate those gradients to get

\[
P \equiv \frac{\partial\vec q^+}{\partial\vec b}
\]

and then $\mathrm{Var} \left( \vec q^+ \right)$

*** Cross-reprojection uncertainty via $T_\mathrm{r^+r}$
:PROPERTIES:
:CUSTOM_ID: cross-reprojection-rt-rpr
:END:

I can also go the other way: traversing the data flow diagram above from the
top-left to bottom-right:

\[
\xymatrix{
  {\color{red}\vec p  _\mathrm{board}} \ar@[red][d]^{\color{red} T_\mathrm{rf}}
& {           \vec p^+_\mathrm{board}} \ar      [d]^{            T_\mathrm{r^+f^+}} \\
  {\color{red} \vec p _\mathrm{ref}}   \ar      [d]^{            T_\mathrm{cr}}
  \ar@[red][r]^{\color{red} T_\mathrm{r^+r}}
& {\color{red} \vec p^+_\mathrm{ref}}  \ar@[red][d]^{\color{red} T_\mathrm{c^+r^+}} \\
  {            \vec p  _\mathrm{cam}}  \ar      [d]^{            \vec b_\mathrm{intrinsics} }
& {\color{red} \vec p^+_\mathrm{cam}}  \ar@[red][d]^{\color{red} \vec b^+_\mathrm{intrinsics} } \\
  {            \vec q}
& {\color{red} \vec q^+}
}
\]

The derivation is mostly similar, with slightly different results. We have

\begin{aligned}
\vec x_\mathrm{cross} \equiv \,
& W_\mathrm{board} \left( \mathrm{project}\left(\vec b^+_\mathrm{intrinsics},
                  T_\mathrm{c^+r^+} T_\mathrm{r^+r} T_\mathrm{rf} \vec p_\mathrm{board}\right)
  - \vec q^+_\mathrm{refboard} \right) + \\
& W_\mathrm{point} \left( \mathrm{project}\left(\vec b^+_\mathrm{intrinsics},
                  T_\mathrm{c^+r^+} T_\mathrm{r^+r} \vec p_\mathrm{point}\right)
  -  \vec q^+_\mathrm{refpoint} \right)
\end{aligned}

And the optimum is similarly at

\[
\vec{\mathrm{rt}_\mathrm{r^+r}} = -\left(J_\mathrm{cross}^T J_\mathrm{cross}\right)^{-1} J_\mathrm{cross}^T \Delta \vec x_\mathrm{cross_0}
\]

And we can compute the linearized quantities near $\vec {\mathrm{rt}_\mathrm{r^+r}} = 0$:
\[
\vec{\mathrm{rt}_\mathrm{c^+r}} = \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{c^+r^+}}, \vec{\mathrm{rt}_\mathrm{r^+r}}\right)
\]

\begin{aligned}
\vec x_\mathrm{cross_0} = \, & \vec x^* +
J_\mathrm{intrinsics,extrinsics} \Delta \vec b_\mathrm{intrinsics,extrinsics} - W \Delta \vec q_\mathrm{ref} \\
= \, & \vec x^* +
J_\mathrm{intrinsics,extrinsics}  M_\mathrm{intrinsics,extrinsics} \Delta \vec q_\mathrm{ref} - W \Delta \vec q_\mathrm{ref}
\end{aligned}

For points that have no extrinsics (the camera is defined to sit at the ref
coord system) there is no $J_\mathrm{extrinsics}$, and we can ignore it here;
but we must then use the $J_{\mathrm{cross}_\mathrm{f}}$ form below.

When evaluating $J_\mathrm{cross} = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}}$ I can once again look at it in
two ways:

- a $\vec{\mathrm{rt}_\mathrm{cr}}$ shift to $\mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{c^+r^+}},\vec{\mathrm{rt}_\mathrm{r^+r}}\right)$.

  $\vec{\mathrm{rt}_\mathrm{c^+r^+}}$ is a tiny shift off
  $\vec{\mathrm{rt}_\mathrm{cr}}$ /and/ I'm assuming that everything is locally
  linear. So this shift is insignificant, and I use
  $\vec{\mathrm{rt}_\mathrm{cr}}$ to compute the gradient instead

  \begin{aligned}
  J_{\mathrm{cross}_\mathrm{e}} & = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}} \\
            & = J_\mathrm{extrinsics} \frac{\partial \vec{\mathrm{rt}_\mathrm{c^+r}}}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}} \\
            & = J_\mathrm{extrinsics} \frac{\partial \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{c^+r^+}},\vec{\mathrm{rt}_\mathrm{r^+r}})}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}} \\
            & = J_\mathrm{extrinsics} \frac{\partial \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{cr}},  \vec{\mathrm{rt}_\mathrm{r^+r}})}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}}
  \end{aligned}

  As before, for points that have no extrinsics (the camera is defined to sit at
  the reference coord system) there is no $J_\mathrm{extrinsics}$, so this
  formulation is not possible here. Use $J_{\mathrm{cross}_\mathrm{f}}$ and/or
  $J_{\mathrm{cross}_\mathrm{p}}$


- a $\vec {\mathrm{rt}_\mathrm{rf}}$ shift to $\mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{r^+r}}, \vec {\mathrm{rt}_\mathrm{rf}}\right)$ and/or a point
  shift to $T_\mathrm{r^+r} \vec p$

  \begin{aligned}
  J_{\mathrm{cross}_\mathrm{f}} & = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}} \\
            & = J_\mathrm{frame} \frac{\partial \vec {\mathrm{rt}_\mathrm{r^+f}}}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}} \\
            & = J_\mathrm{frame} \frac{\partial \mathrm{compose\_rt}\left(\vec{\mathrm{rt}_\mathrm{r^+r}},\vec {\mathrm{rt}_\mathrm{rf}}\right)}{\vec{\mathrm{rt}_\mathrm{r^+r}}} \\
  J_{\mathrm{cross}_\mathrm{p}} & = \frac{\partial \vec x_\mathrm{cross}}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}} \\
            & = J_\mathrm{points} \frac{\partial \vec p^+}{\partial \vec{\mathrm{rt}_\mathrm{r^+r}}} \\
            & = J_\mathrm{points} \frac{T__\mathrm{r^+r} \vec p}{\parital \vec{\mathrm{rt}_\mathrm{r^+r}}}
  \end{aligned}

There's one more simplification available. From above:

  \vec{\mathrm{rt}_\mathrm{r^+r}} = -pinv(J_\mathrm{cross}) x_\mathrm{cross_0}
              = -inv() J_\mathrm{cross}_t x_\mathrm{cross_0}
              = ... J_frame_t (x0 + ...)

And we can simplify to

\[
\vec{\mathrm{rt}_\mathrm{r^+r}} = -pinv(J_\mathrm{cross}) \partial\vec x_\mathrm{cross_0}
\]

where

\[
\partial\vec x_\mathrm{cross_0} = J[intrinsics,extrinsics] db[intrinsics,extrinsics] - W delta_q_\mathrm{ref}
\]

So we have \vec{\mathrm{rt}_\mathrm{r^+r}} = K db - W delta_q_\mathrm{ref} for some K that depends on the
various J matrices that are constant for each solve:

  K = -pinv(J_\mathrm{cross}) J[intrinsics,extrinsics]

The rest of the data flow is the same as above, except we already have
\vec{\mathrm{rt}_\mathrm{r^+r}}, so we don't need to invert the transform when applying it.






cross-reprojection--rrp-Jfp

** mean-frames
**** problems this old method has that are solved with the new method
The current projection uncertainty method works badly if given chessboards at
multiple different ranges from the camera. This is due to the aphysical
transform $T_{\mathrm{r}^+\mathrm{r}}$ computed as part of the [[file:uncertainty.org::#propagating-through-projection][uncertainty
computation]]. We can clearly see this in the dance study:

#+begin_src sh
./dance-study.py                          \
    --scan num_far_constant_Nframes_near  \
    --range 2,10                          \
    --method cross-reprojection--rrp-Jfp  \
    --Ncameras 1                          \
    --Nframes-near 100                    \
    --observed-pixel-uncertainty 2        \
    --ymax 4                              \
    --uncertainty-at-range-sampled-max 35 \
    ~/projects/mrcal-doc-external/2022-11-05--dtla-overpass--samyang--alpha7/3-f22-infinity/opencv8.cameramodel
#+end_src

This tells us that adding /any/ observations at 10m to the bulk set at 2m
makes the projection uncertainty /worse/. One could expect no improvement from
the far-off observations, but they shouldn't break anything. The issue is the
averaging in 3D point space. Observation noise causes the far-off geometry to
move much more than the nearby chessboards, and that far-off motion then
dominates the average. We can also see it with the much larger ellipse we get
when we add =--extra-observation-at= to

#+begin_src sh
test/test-projection-uncertainty.py \
  --fixed cam0                      \
  --model opencv4                   \
  --show-distribution               \
  --range-to-boards 4               \
  --extra-observation-at 40         \
  --do-sample                       \
  --explore
#+end_src

Some experimental fixes are implemented in
[[https://www.github.com/dkogan/mrcal/blob/master/test/test-projection-uncertainty.py][=test/test-projection-uncertainty.py=]]. For instance:

#+begin_src sh
test/test-projection-uncertainty.py \
  --fixed cam0                      \
  --model opencv4                   \
  --show-distribution               \
  --range-to-boards 4               \
  --extra-observation-at 40         \
  --do-sample                       \
  --explore                         \
  --reproject-perturbed mean-frames-using-meanq-penalize-big-shifts
#+end_src

It is important to solve this to be able to clearly say if non-closeup
observations are useful at all or not. There was quick a bit of thought and
experimentation in this area, but no conclusive solutions yet.


When asked to compute the uncertainty of many pixels at once (such as what
[[file:mrcal-show-projection-uncertainty.html][=mrcal-show-projection-uncertainty=]] tool does), mrcal currently computes a
separate $T_{\mathrm{r}^+\mathrm{r}}$ for each pixel. But there exists only
one $T_{\mathrm{r}^+\mathrm{r}}$, and this should be computed once for all
pixels, and applied to all of them.

Currently we are able to compute projection uncertainties only when given a
vanilla calibration problem: stationary cameras are observing a moving
chessboard. We should support more cases, for instance structure-from-motion
coupled with intrinsics optimization. And computing uncertainty from a
points-only chessboard-less solve should be possible

**** how

The state vector $\vec b$ is a random variable, and we know its distribution. To
evaluate the projection uncertainty we want to project a /fixed/ point, to see
how this projection $\vec q$ moves around as the chessboards and cameras and
intrinsics shift due to the uncertainty in $\vec b$. In other words, we want to
project a point defined in the coordinate system of the camera housing, as the
origin of the mathematical camera moves around inside this housing:




How do we operate on points in a fixed coordinate system when all the coordinate
systems we have are floating random variables? We use the most fixed thing we
have: chessboards. As with the camera housing, the chessboards themselves are
fixed in space. We have noisy camera observations of the chessboards that
implicitly produce estimates of the fixed transformation $T_{\mathrm{cf}_i}$ for
each chessboard $i$. The explicit transformations that we /actually/ have in
$\vec b$ all relate to a floating reference coordinate system: $T_\mathrm{cr}$
and $T_\mathrm{rf}$. /That/ coordinate system doesn't have any physical meaning,
and it's useless in producing our fixed point.

Thus if we project points from a chessboard frame, we would be unaffected by the
untethered reference coordinate system. So points in a chessboard frame are
somewhat "fixed" for our purposes.

To begin, let's focus on just /one/ chessboard frame: frame 0. We want to know
the uncertainty at a pixel coordinate $\vec q$, so let's unproject and transform
$\vec q$ out to frame 0:

\[ \vec p_{\mathrm{frame}_0} = T_{\mathrm{f}_0\mathrm{r}} T_\mathrm{rc} \mathrm{unproject}\left( \vec q \right) \]

We then transform and project $\vec p_{\mathrm{frame}_0}$ back to the imager to
get $\vec q^+$. But here we take into account the uncertainties of each
transformation to get the desired projection uncertainty $\mathrm{Var}\left(\vec
q^+ - \vec q\right)$. The full data flow looks like this, with all the perturbed
quantities marked with a $+$ superscript.

\[
\xymatrix{
   \vec q^+ & &
   \vec p^+_\mathrm{camera}          \ar[ll]_-{\vec b_\mathrm{intrinsics}^+} &
   \vec p^+_{\mathrm{reference}_0}   \ar[l]_-{T^+_\mathrm{cr}} &
   \vec p_{\mathrm{frame}_0}         \ar[l]_-{T^+_{\mathrm{rf}_0}} &
   \vec p_\mathrm{reference}         \ar[l]_-{T_\mathrm{fr}} &
   \vec p_\mathrm{camera}            \ar[l]_-{T_\mathrm{rc}} & &
   \vec q                            \ar[ll]_-{\vec b_\mathrm{intrinsics}}
}
\]

# Another way to do this (without xymatrix):
# \[
#    \vec q^+                         \xleftarrow{\vec b_\mathrm{intrinsics}^+}
#    \vec p^+_\mathrm{camera}         \xleftarrow{T^+_\mathrm{cr}}
#    \vec p^+_{\mathrm{reference}_0}  \xleftarrow{T^+_{\mathrm{rf}_0}} \vec p_{\mathrm{frame}_0} \xleftarrow{T_\mathrm{fr}}
#    \vec p_\mathrm{reference}
#    \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
#    \xleftarrow{\vec b_\mathrm{intrinsics}}
#    \vec q
# \]

This works, but it depends on $\vec p_{\mathrm{frame}_0}$ being "fixed". We can
do better. We're observing more than one chessboard, and /in aggregate/ all the
chessboard frames can represent an even-more "fixed" frame. Currently we take a
very simple approach towards combinining the frames: we compute the mean of all
the $\vec p^+_\mathrm{reference}$ estimates from each frame. The full data flow
then looks like this:

\[
\xymatrix{
& & & & \vec p^+_{\mathrm{reference}_0} \ar[dl]_{\mathrm{mean}} & \vec p^+_{\mathrm{frame}_0} \ar[l]_-{T^+_{\mathrm{rf}_0}} \\
\vec q^+ & &
\vec p^+_\mathrm{camera}          \ar[ll]_-{\vec b_\mathrm{intrinsics}^+} &
\vec p^+_{\mathrm{reference}}   \ar[l]_-{T^+_\mathrm{cr}} &
\vec p^+_{\mathrm{reference}_1} \ar[l]_{\mathrm{mean}} & \vec p^+_{\mathrm{frame}_1} \ar[l]_-{T^+_{\mathrm{rf}_1}} &
\vec p_\mathrm{reference}         \ar[l]_-{T_\mathrm{f_1 r}} \ar[lu]_-{T_\mathrm{f_0 r}} \ar[ld]^-{T_\mathrm{f_2 r}} &
\vec p_\mathrm{camera}            \ar[l]_-{T_\mathrm{rc}} & &
\vec q                            \ar[ll]_-{\vec b_\mathrm{intrinsics}} \\
& & & & \vec p^+_{\mathrm{reference}_2} \ar[ul]^{\mathrm{mean}} & \vec p^+_{\mathrm{frame}_2} \ar[l]_-{T^+_{\mathrm{rf}_2}}
}
\]

# Another way to do this (without xymatrix):
# \begin{aligned}
#    & \swarrow                   & \vec p^+_{\mathrm{reference}_0}  & \xleftarrow{T^+_{\mathrm{rf}_0}} & \vec p_{\mathrm{frame}_0} & \nwarrow & \\
#    \vec q^+                      \xleftarrow{\vec b_\mathrm{intrinsics}^+}
#    \vec p^+_\mathrm{camera}      \xleftarrow{T^+_\mathrm{cr}}
#    \vec p^+_\mathrm{reference}
#    & \xleftarrow{\mathrm{mean}} & \vec p^+_{\mathrm{reference}_1}  & \xleftarrow{T^+_{\mathrm{rf}_1}} & \vec p_{\mathrm{frame}_1} & \xleftarrow{T_\mathrm{fr}} &
#    \vec p_\mathrm{reference}
#    \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
#    \xleftarrow{\vec b_\mathrm{intrinsics}}
#    \vec q \\
#    & \nwarrow                   & \vec p^+_{\mathrm{reference}_2}  & \xleftarrow{T^+_{\mathrm{rf}_2}} & \vec p_{\mathrm{frame}_2} & \swarrow
# \end{aligned}




This is better, but there's another issue. What is the transformation relating
the original and perturbed reference coordinate systems?

\[ T_{\mathrm{r}^+\mathrm{r}} = \mathrm{mean}_i \left( T_{\mathrm{r}^+\mathrm{f}_i} T_{\mathrm{f}_i\mathrm{r}} \right) \]

Each transformation $T$ includes a rotation matrix $R$, so the above constructs
a new rotation as a mean of multiple rotation matrices, which is aphysical: the
resulting matrix is not a valid rotation. In practice, the perturbations are
tiny, and this is sufficiently close. Extreme geometries do break it, and this
will be fixed in the future.

So to summarize, to compute the projection uncertainty at a pixel $\vec q$ we

1. Unproject $\vec q$ and transform to /each/ chessboard coordinate system to
   obtain $\vec p_{\mathrm{frame}_i}$

2. Transform and project back to $\vec q^+$, useing the mean of all the $\vec
   p_{\mathrm{reference}_i}$ and taking into account uncertainties

We have $\vec q^+\left(\vec b\right) = \mathrm{project}\left( T_\mathrm{cr} \,
\mathrm{mean}_i \left( T_{\mathrm{rf}_i} \vec p_{\mathrm{frame}_i} \right)
\right)$ where the transformations $T$ and the intrinsics used in
$\mathrm{project}()$ come directly from the optimization state vector $\vec b$. So

\[ \mathrm{Var}\left( \vec q \right) = \frac{\partial \vec q^+}{\partial \vec b} \mathrm{Var}\left( \vec b \right) \frac{\partial \vec q^+}{\partial \vec b}^T \]

We computed $\mathrm{Var}\left( \vec b \right)$ earlier, and $\frac{\partial
\vec q^+}{\partial \vec b}$ comes from the projection expression above.

The [[file:mrcal-python-api-reference.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]] function implements this logic. For the
special-case of visualizing the uncertainties, call the any of the uncertainty
visualization functions:
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty][=mrcal.show_projection_uncertainty()=]]: Visualize the uncertainty in camera projection
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty_vs_distance][=mrcal.show_projection_uncertainty_vs_distance()=]]: Visualize the uncertainty in camera projection along one observation ray

or use the [[file:mrcal-show-projection-uncertainty.html][=mrcal-show-projection-uncertainty=]] tool.

A sample uncertainty map of the splined model calibration from the [[file:tour-uncertainty.org][tour of mrcal]]
looking out to infinity:

#+begin_src sh
mrcal-show-projection-uncertainty splined.cameramodel --cbmax 1 --unset key
#+end_src
#+begin_src sh :exports none :eval no-export
# THIS IS GENERATED IN tour-uncertainty.org
#+end_src

[[file:external/figures/uncertainty/uncertainty-splined.png]]

* The effect of range
:PROPERTIES:
:CUSTOM_ID: effect-of-range
:END:
We glossed over an important detail in the above derivation. Unlike a projection
operation, an /unprojection/ is ambiguous: given some camera-coordinate-system
point $\vec p$ that projects to a pixel $\vec q$, we have $\vec q =
\mathrm{project}\left(k \vec v\right)$ /for all/ $k$. So an unprojection gives
you a direction, but no range. The direct implication of this is that we can't
ask for an "uncertainty at pixel coordinate $\vec q$". Rather we must ask about
"uncertainty at pixel coordinate $\vec q$ looking $x$ meters out".

And a surprising consequence of that is that while /projection/ is invariant to
scaling ($k \vec v$ projects to the same $\vec q$ for any $k$), the uncertainty
of projection is /not/ invariant to this scaling:

[[file:figures/projection-scale-invariance.svg]]

Let's look at the projection uncertainty at the center of the imager at
different ranges for an arbitrary model:

#+begin_src sh
mrcal-show-projection-uncertainty \
  --vs-distance-at center         \
  --set 'yrange [0:0.1]'          \
  opencv8.cameramodel
#+end_src
#+begin_src sh :exports none :eval no-export
# THIS IS GENERATED IN tour-effect-of-range.org
#+end_src

[[file:external/figures/uncertainty/uncertainty-vs-distance-at-center.svg]]

So the uncertainty grows without bound as we approach the camera. As we move
away, there's a sweet spot where we have maximum confidence. And as we move
further out still, we approach some uncertainty asymptote at infinity.
Qualitatively this is the figure I see 100% of the time, with the position of
the minimum and of the asymptote varying.

As we approach the camera, the uncertainty is unbounded because we're looking at
the projection of a fixed point into a camera whose position is uncertain. As we
get closer to the origin, the noise in the camera position dominates the
projection, and the uncertainty shoots to infinity.

The "sweet spot" where the uncertainty is lowest sits at the range where we
observed the chessboards.

The uncertainty we asymptotically approach at infinity is set by the [[file:tour-choreography.org][specifics
of the chessboard dance]].

See the [[file:tour-uncertainty.org][tour of mrcal]] for a simulation validating this approach of quantifying
uncertainty and for some empirical results.

