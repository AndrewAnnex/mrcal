#+TITLE: Projection uncertainty
#+OPTIONS: toc:t

After a calibration has been computed, it is essential to get a sense of how
good the calibration is (how closely it represents reality). Traditional
(non-mrcal) calibration routines rely on one metric of calibration quality: the
residual fit error. This is clearly inadequate because we can always improve
this metric by throwing away some input data, and it doesn't make sense that
using less data would make a calibration /better/.

There are two main sources of error in the calibration solve. Without these
errors, the calibration data would fit perfectly, producing a solve residual
vector that's exactly $\vec 0$. The two sources of error are:

- *Sampling error*. Our computations are based on fitting a model to
  observations of chessboard corners. These observations aren't perfect, and
  contain a sample of some noise distribution. We can [[file:formulation.org::#noise-model][characterize this
  distribution]] and we can analytically predict the effects of that noise

- *Model error*. These result when the solver's model of the world is
  insufficient to describe what is actually happening. When model errors are
  present, even the best set of parameters aren't able to completely fit the
  data. Some sources of model errors: motion blur, unsynchronized cameras,
  chessboard detector errors, too-simple (or unstable) [[file:lensmodels.org][lens models]] or chessboard
  deformation models, and so on. Since these errors are unmodeled (by
  definition), we can't analytically predict their effects. Instead we try hard
  to force these errors to zero, so that we can ignore them. We do this by using
  rich-enough models and by gathering clean data. To detect model errors we
  [[file:how-to-calibrate.org::#interpreting-results][look at the solve
  diagnostics]] and we compute [[file:tour-cross-validation.org][cross-validation diffs]].

Let's do as much as we can analytically: let's gauge the effects of sampling
error by computing a /projection uncertainty/ for a model. Since /only/ the
sampling error is evaluated:

*Any promises of a high-quality low-uncertainty calibration are valid only if
the model errors are small*.

The method to estimate the projection uncertainty is accessed via the
[[file:mrcal-python-api-reference.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]] function. Here the "uncertainty" is the
sensitivity to sampling error: the calibration-time pixel noise. This tells us
how good a calibration is (we aim for low projection uncertainties), and it can
tell us how good the downstream results are as well (by propagating projection
uncertainties through the downstream computation).

To estimate the projection uncertainty we:

1. Estimate the [[file:formulation.org::#noise-model-inputs][noise in the chessboard observations]]
2. Propagate that noise to the optimal parameters $\vec b^*$ reported by the
   calibration routine
3. Propagate the uncertainty in calibration parameters $\vec b^*$ through the
   projection function to get uncertainty in the resulting pixel coordinate $\vec
   q$

This overall approach is sound, but it implies some limitations:

- Once again, model errors are not included in this uncertainty estimate

- The choice of lens model affects the reported uncertainties. Lean models
  (those with few parameters) are less flexible than rich models, and don't fit
  general lenses as well as rich models do. This stiffness also serves to limit
  the model's response to noise in their parameters. Thus the above method will
  report less uncertainty for leaner models than rich models. So, unless we're
  /sure/ that a given lens follows some particular lens model perfectly, a
  [[file:splined-models.org][splined lens model]] (i.e. a very rich model) is recommended for truthful
  uncertainty reporting. Otherwise the reported confidence comes from the model
  itself, rather than the calibration data.

- Currently the uncertainty estimates can be computed only from a vanilla
  calibration problem: a set of stationary cameras observing a moving
  calibration object. Other formulations can be used to compute the lens
  parameters as well (structure-from-motion while also computing the lens models
  for instance), but at this time the uncertainty computations cannot handle
  those cases. It can be done, but the current method needs to be extended to do
  so.

* Estimating the input noise
We're measuring the sensitivity to the noise in the calibration-time
observations. In order to propagate this noise, we need to know what that input
noise is. The current approach is described in the [[file:formulation.org::#noise-model][optimization problem
formulation]].

* Propagating input noise to the state vector
:PROPERTIES:
:CUSTOM_ID: propagating-to-state-vector
:END:

We solved the [[file:formulation.org][least squares problem]], so we have the optimal state vector $\vec
b^*$.

We apply a perturbation to the observations $\vec q_\mathrm{ref}$, reoptimize
this slightly-perturbed least-squares problem, assuming everything is linear,
and look at what happens to the optimal state vector $\vec b^*$.

We have

\[ E \equiv \left \Vert \vec x \right \Vert ^2 \]
\[ J \equiv \frac{\partial \vec x}{\partial \vec b} \]

At the optimum $E$ is minimized, so

\[ \frac{\partial E}{\partial \vec b} \left(\vec b = \vec b^* \right) = 2 J^T \vec x^* = 0 \]

We perturb the problem:

\[ E( \vec b + \Delta \vec b, \vec q_\mathrm{ref} + \Delta \vec q_\mathrm{ref})) \approx
\left \Vert \vec x + \frac{\partial \vec x}{\partial \vec b} \Delta \vec b + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} \Delta \vec q_\mathrm{ref} \right \Vert ^2 =
\left \Vert \vec x + J \Delta \vec b + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} \Delta \vec q_\mathrm{ref} \right \Vert ^2 \]

And we reoptimize:

\[ \frac{\mathrm{d}E}{\mathrm{d}\Delta \vec b} \approx 
2 \left( \vec x + J \Delta \vec b + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \right)^T J = 0\]

We started at an optimum, so $\vec x = \vec x^*$ and $J^T \vec x^* = 0$, and thus

\[ J^T J \Delta \vec b = -J^T \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \]

As defined on the [[file:formulation.org::#noise-model][input noise page]], we have

\[ \vec x_\mathrm{observations} = W (\vec q - \vec q_\mathrm{ref}) \]

where $W$ is a diagonal matrix of weights. These are the only elements of $\vec
x$ that depend on $\vec q_\mathrm{ref}$. Let's assume the non-observation
elements of $\vec x$ are at the end, so

\[ \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} =
\left[ \begin{array}{cc} - W \\ 0 \end{array} \right] \]

and thus

\[ J^T J \Delta \vec b = J_\mathrm{observations}^T W \Delta \vec q_\mathrm{ref} \]

So if we perturb the input observation vector $q_\mathrm{ref}$ by $\Delta
q_\mathrm{ref}$, the resulting effect on the optimal parameters is $\Delta \vec
b = M \Delta \vec q_\mathrm{ref}$ where

\[ M = \left( J^T J \right)^{-1} J_\mathrm{observations}^T W \]

As usual,

\[ \mathrm{Var}(\vec b) = M \mathrm{Var}\left(\vec q_\mathrm{ref}\right) M^T \]

As stated on the [[file:formulation.org::#noise-model][input noise page]], we're assuming independent noise on all
observed pixels, with a standard deviation inversely proportional to the weight:

\[ \mathrm{Var}\left( \vec q_\mathrm{ref} \right) = \sigma^2 W^{-2} \]

so

\begin{aligned}
\mathrm{Var}\left(\vec b\right) &= \sigma^2 M W^{-2} M^T \\
&= \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T W W^{-2} W J_\mathrm{observations} \left( J^T J \right)^{-1} \\
&= \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T J_\mathrm{observations}  \left( J^T J \right)^{-1}
\end{aligned}

If we have no regularization, then $J_\mathrm{observations} = J$ and we can
simplify even further:

\[\mathrm{Var}\left(\vec b\right) = \sigma^2 \left( J^T J \right)^{-1} \]

Note that these expressions do not explicitly depend on $W$, but the weights
still have an effect, since they are a part of $J$. So if an
observation $i$ were to become less precise, $w_i$ and $x_i$ and $J_i$ would all
decrease. And as a result, $\mathrm{Var}\left(\vec b\right)$ would increase, as
expected.

* Propagating the state vector noise through projection
:PROPERTIES:
:CUSTOM_ID: propagating-through-projection
:END:
We now have the variance of the full optimization state $\vec b$, and we want to
propagate this through projection to end up with an estimate of uncertainty at
any given pixel $\vec q$.

The state vector $\vec b$ is a random variable, and we know its distribution. To
evaluate the projection uncertainty we want to project a /fixed/ point, to see
how this projection $\vec q$ moves around as the chessboards and cameras and
intrinsics shift due to the uncertainty in $\vec b$. In other words, we want to
project a point defined in the coordinate system of the camera housing, as the
origin of the mathematical camera moves around inside this housing:

[[file:figures/uncertainty.svg]]

How do we operate on points in a fixed coordinate system when all the coordinate
systems we have are floating random variables? We use the most fixed thing we
have: chessboards. As with the camera housing, the chessboards themselves are
fixed in space. We have noisy camera observations of the chessboards that
implicitly produce estimates of the fixed transformation $T_{\mathrm{cf}_i}$ for
each chessboard $i$. The explicit transformations that we /actually/ have in
$\vec b$ all relate to a floating reference coordinate system: $T_\mathrm{cr}$
and $T_\mathrm{rf}$. /That/ coordinate system doesn't have any physical meaning,
and it's useless in producing our fixed point.

Thus if we project points from a chessboard frame, we would be unaffected by the
untethered reference coordinate system. So points in a chessboard frame are
somewhat "fixed" for our purposes.

To begin, let's focus on just /one/ chessboard frame: frame 0. We want to know
the uncertainty at a pixel coordinate $\vec q$, so let's unproject and transform
$\vec q$ out to frame 0:

\[ \vec p_{\mathrm{frame}_0} = T_{\mathrm{f}_0\mathrm{r}} T_\mathrm{rc} \mathrm{unproject}\left( \vec q \right) \]

We then transform and project $\vec p_{\mathrm{frame}_0}$ back to the imager to
get $\vec q^+$. But here we take into account the uncertainties of each
transformation to get the desired projection uncertainty $\mathrm{Var}\left(\vec
q^+ - \vec q\right)$. The full data flow looks like this, with all the perturbed
quantities marked with a $+$ superscript.

# Another way to do this (using xymatrix). This works, except for the funny
# spacing
#
# \begin{equation}
# \xymatrix{
#    \vec q^+ &
#    \vec p^+_\mathrm{camera}          \ar[l]^{\mathrm{intrinsics}^+} &
#    \vec p^+_{\mathrm{reference}_0}   \ar[l]^{T^+_\mathrm{cr}} &
#    \vec p_{\mathrm{frame}_0}         \ar[l]^{T^+_{\mathrm{rf}_0}} &
#    \vec p_\mathrm{reference}         \ar[l]^{T_\mathrm{fr}} &
#    \vec p_\mathrm{camera}            \ar[l]^{T_\mathrm{rc}} &
#    \vec q                            \ar[l]^{\mathrm{intrinsics}}
# }
# \end{equation}

\[
   \vec q^+                         \xleftarrow{\mathrm{intrinsics}^+}
   \vec p^+_\mathrm{camera}         \xleftarrow{T^+_\mathrm{cr}}
   \vec p^+_{\mathrm{reference}_0}  \xleftarrow{T^+_{\mathrm{rf}_0}} \vec p_{\mathrm{frame}_0} \xleftarrow{T_\mathrm{fr}}
   \vec p_\mathrm{reference}
   \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
   \xleftarrow{\mathrm{intrinsics}}
   \vec q
\]

This works, but it depends on $\vec p_{\mathrm{frame}_0}$ being "fixed". We can
do better. We're observing more than one chessboard, and /in aggregate/ all the
chessboard frames can represent an even-more "fixed" frame. Currently we take a
very simple approach towards combinining the frames: we compute the mean of all
the $\vec p^+_\mathrm{reference}$ estimates from each frame. The full data flow
then looks like this:

\begin{aligned}
   & \swarrow                   & \vec p^+_{\mathrm{reference}_0}  & \xleftarrow{T^+_{\mathrm{rf}_0}} & \vec p_{\mathrm{frame}_0} & \nwarrow & \\
   \vec q^+                      \xleftarrow{\mathrm{intrinsics}^+}
   \vec p^+_\mathrm{camera}      \xleftarrow{T^+_\mathrm{cr}}
   \vec p^+_\mathrm{reference}
   & \xleftarrow{\mathrm{mean}} & \vec p^+_{\mathrm{reference}_1}  & \xleftarrow{T^+_{\mathrm{rf}_1}} & \vec p_{\mathrm{frame}_1} & \xleftarrow{T_\mathrm{fr}} &
   \vec p_\mathrm{reference}
   \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
   \xleftarrow{\mathrm{intrinsics}}
   \vec q \\
   & \nwarrow                   & \vec p^+_{\mathrm{reference}_2}  & \xleftarrow{T^+_{\mathrm{rf}_2}} & \vec p_{\mathrm{frame}_2} & \swarrow
\end{aligned}

This is better, but there's another issue. What is the transformation relating
the original and perturbed reference coordinate systems?

\[ T_{\mathrm{r}^+\mathrm{r}} = \mathrm{mean}_i \left( T_{\mathrm{r}^+\mathrm{f}_i} T_{\mathrm{f}_i\mathrm{r}} \right) \]

Each transformation $T$ includes a rotation matrix $R$, so the above constructs
a new rotation as a mean of multiple rotation matrices, which is aphysical: the
resulting matrix is not a valid rotation. In practice, the perturbations are
tiny, and this is sufficiently close. Extreme geometries do break it, and this
will be fixed in the future.

So to summarize, to compute the projection uncertainty at a pixel $\vec q$ we

1. Unproject $\vec q$ and transform to /each/ chessboard coordinate system to
   obtain $\vec p_{\mathrm{frame}_i}$

2. Transform and project back to $\vec q^+$, useing the mean of all the $\vec
   p_{\mathrm{reference}_i}$ and taking into account uncertainties

We have $\vec q^+\left(\vec b\right) = \mathrm{project}\left( T_\mathrm{cr} \,
\mathrm{mean}_i \left( T_{\mathrm{rf}_i} \vec p_{\mathrm{frame}_i} \right)
\right)$ where the transformations $T$ and the intrinsics used in
$\mathrm{project}()$ come directly from the optimization state vector $\vec b$. So

\[ \mathrm{Var}\left( \vec q \right) = \frac{\partial \vec q^+}{\partial \vec b} \mathrm{Var}\left( \vec b \right) \frac{\partial \vec q^+}{\partial \vec b}^T \]

We computed $\mathrm{Var}\left( \vec b \right)$ earlier, and $\frac{\partial
\vec q^+}{\partial \vec b}$ comes from the projection expression above.

The [[file:mrcal-python-api-reference.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]] function implements this logic. For the
special-case of visualizing the uncertainties, call the any of the uncertainty
visualization functions:
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty][=mrcal.show_projection_uncertainty()=]]: Visualize the uncertainty in camera projection
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty_vs_distance][=mrcal.show_projection_uncertainty_vs_distance()=]]: Visualize the uncertainty in camera projection along one observation ray

or use the [[file:mrcal-show-projection-uncertainty.html][=mrcal-show-projection-uncertainty=]] tool.

A sample uncertainty map of the splined model calibration from the [[file:tour-uncertainty.org][tour of mrcal]]
looking out to infinity:

#+begin_src sh
mrcal-show-projection-uncertainty splined.cameramodel --cbmax 1 --unset key
#+end_src
#+begin_src sh :exports none :eval no-export
# THIS IS GENERATED IN tour-uncertainty.org
#+end_src

[[file:external/figures/uncertainty/uncertainty-splined.png]]

* The effect of range
:PROPERTIES:
:CUSTOM_ID: effect-of-range
:END:
We glossed over an important detail in the above derivation. Unlike a projection
operation, an /unprojection/ is ambiguous: given some camera-coordinate-system
point $\vec p$ that projects to a pixel $\vec q$, we have $\vec q =
\mathrm{project}\left(k \vec v\right)$ /for all/ $k$. So an unprojection gives
you a direction, but no range. The direct implication of this is that we can't
ask for an "uncertainty at pixel coordinate $\vec q$". Rather we must ask about
"uncertainty at pixel coordinate $\vec q$ looking $x$ meters out".

And a surprising consequence of that is that while /projection/ is invariant to
scaling ($k \vec v$ projects to the same $\vec q$ for any $k$), the uncertainty
of projection is /not/ invariant to this scaling:

[[file:figures/projection-scale-invariance.svg]]

Let's look at the projection uncertainty at the center of the imager at
different ranges for an arbitrary model:

#+begin_src sh
mrcal-show-projection-uncertainty \
  --vs-distance-at center         \
  --set 'yrange [0:0.1]'          \
  opencv8.cameramodel
#+end_src
#+begin_src sh :exports none :eval no-export
# THIS IS GENERATED IN tour-effect-of-range.org
#+end_src

[[file:external/figures/uncertainty/uncertainty-vs-distance-at-center.svg]]

So the uncertainty grows without bound as we approach the camera. As we move
away, there's a sweet spot where we have maximum confidence. And as we move
further out still, we approach some uncertainty asymptote at infinity.
Qualitatively this is the figure I see 100% of the time, with the position of
the minimum and of the asymptote varying.

As we approach the camera, the uncertainty is unbounded because we're looking at
the projection of a fixed point into a camera whose position is uncertain. As we
get closer to the origin, the noise in the camera position dominates the
projection, and the uncertainty shoots to infinity.

The "sweet spot" where the uncertainty is lowest sits at the range where we
observed the chessboards.

The uncertainty we asymptotically approach at infinity is set by the [[file:tour-choreography.org][specifics
of the chessboard dance]].

See the [[file:tour-uncertainty.org][tour of mrcal]] for a simulation validating this approach of quantifying
uncertainty and for some empirical results.

* cross-uncertainty

** notes from the roadmap
Improved projection uncertainty quantification. The [[file:uncertainty.org][current projection
uncertainty method]], which functional, has some issues. A new approach in the
[[https://github.com/dkogan/mrcal/tree/2022-04--cross-uncertainty][=2022-04--cross-uncertainty= branch]] aims to resolve them.

The current projection uncertainty method works badly if given chessboards at
multiple different ranges from the camera. This is due to the aphysical
transform $T_{\mathrm{r}^+\mathrm{r}}$ computed as part of the [[file:uncertainty.org::#propagating-through-projection][uncertainty
computation]]. We can clearly see this in the dance study:

#+begin_src sh
./dance-study.py                          \
    --scan num_far_constant_Nframes_near  \
    --range 2,10                          \
    --method cross-reprojection--rrp-Jfp  \
    --Ncameras 1                          \
    --Nframes-near 100                    \
    --observed-pixel-uncertainty 2        \
    --ymax 4                              \
    --uncertainty-at-range-sampled-max 35 \
    ~/projects/mrcal-doc-external/2022-11-05--dtla-overpass--samyang--alpha7/3-f22-infinity/opencv8.cameramodel
#+end_src

This tells us that adding /any/ observations at 10m to the bulk set at 2m
makes the projection uncertainty /worse/. One could expect no improvement from
the far-off observations, but they shouldn't break anything. The issue is the
averaging in 3D point space. Observation noise causes the far-off geometry to
move much more than the nearby chessboards, and that far-off motion then
dominates the average. We can also see it with the much larger ellipse we get
when we add =--extra-observation-at= to

#+begin_src sh
test/test-projection-uncertainty.py \
  --fixed cam0                      \
  --model opencv4                   \
  --show-distribution               \
  --range-to-boards 4               \
  --extra-observation-at 40         \
  --do-sample                       \
  --explore
#+end_src

Some experimental fixes are implemented in
[[https://www.github.com/dkogan/mrcal/blob/master/test/test-projection-uncertainty.py][=test/test-projection-uncertainty.py=]]. For instance:

#+begin_src sh
test/test-projection-uncertainty.py \
  --fixed cam0                      \
  --model opencv4                   \
  --show-distribution               \
  --range-to-boards 4               \
  --extra-observation-at 40         \
  --do-sample                       \
  --explore                         \
  --reproject-perturbed mean-frames-using-meanq-penalize-big-shifts
#+end_src

It is important to solve this to be able to clearly say if non-closeup
observations are useful at all or not. There was quick a bit of thought and
experimentation in this area, but no conclusive solutions yet.

The solution being considered: solve for $T_{\mathrm{r}^+\mathrm{r}}$
directly. We have a solve that minimizes the reprojection error $\Sigma_i
\left\Vert\vec q_i - \mathrm{project}\left(T_\mathrm{cr_i} T_\mathrm{rf_i}
\vec p_{\mathrm{frame}_i}\right)\right\Vert^2$ and another one that looks at
perturbed quantities $\left\Vert\vec q^+ -
\mathrm{project}^+\left(T_{\mathrm{c}^+\mathrm{r}^+}
T_{\mathrm{r}^+\mathrm{f}^+} \vec p_{\mathrm{frame}}\right)\right\Vert^2$. Can
I cross these to find the $T_{\mathrm{r}^+\mathrm{r}}$ that minimizes
$\left\Vert\vec q^+ - \mathrm{project}^+\left(T_{\mathrm{c}^+\mathrm{r}^+}
T_{\mathrm{r}^+\mathrm{r}} T_\mathrm{rf} \vec
p_{\mathrm{frame}}\right)\right\Vert^2$. A diagram:

#+begin_example
ORIGINAL SOLVE                   PERTURBED SOLVE

point in                         point in
chessboard                       chessboard
frame                            frame

  |                                |
  | Trf                            | Tr+f+
  v                                v

point in                         point in
ref frame     <-- Trr+ -->       ref frame

  |                                |
  | Tcr                            | Tc+r+
  v                                v

point in                         point in
cam frame                        cam frame

  |                                |
  | project                        | project
  v                                v

pixel                            pixel
#+end_example

Some experiments along those lines are implemented in
=mrcal-show-projection-diff --same-dance= and in
=test/test-projection-uncertainty.py --reproject-perturbed ...=

When asked to compute the uncertainty of many pixels at once (such as what
[[file:mrcal-show-projection-uncertainty.html][=mrcal-show-projection-uncertainty=]] tool does), mrcal currently computes a
separate $T_{\mathrm{r}^+\mathrm{r}}$ for each pixel. But there exists only
one $T_{\mathrm{r}^+\mathrm{r}}$, and this should be computed once for all
pixels, and applied to all of them.

Currently we are able to compute projection uncertainties only when given a
vanilla calibration problem: stationary cameras are observing a moving
chessboard. We should support more cases, for instance structure-from-motion
coupled with intrinsics optimization. And computing uncertainty from a
points-only chessboard-less solve should be possible

** New notes

Reproject by explicitly computing a ref-refperturbed transformation

I have a baseline solve (parameter vector $\vec b$) and a perturbed solve (parameter
vector $\vec b^+$) obtained from perturbing the observations $\vec q_\mathrm{ref}$ and
re-optimizing. I also have an arbitrary baseline query pixel $\vec q$ and distance
$d$ from which I compute the perturbed reprojection $\vec q^+$.

I need to eventually compute $\mathrm{Var}\left(\vec q^+\right)$. I linearize
everything to get

\[
\Delta \vec q^+ \approx \frac{\mathrm{d}\vec q^+}{\mathrm{d}\vec b^+} \frac{\mathrm{d}\vec b^+}{\mathrm{d}\vec q_\mathrm{ref}}
\Delta \vec q_\mathrm{ref}
\]

Let

\[
P \equiv \frac{\mathrm{d}\vec q^+}{\mathrm{d}\vec b^+}
\]

and

\[
M \equiv \frac{\mathrm{d}\vec b^+}{\mathrm{d}\vec q_\mathrm{ref}}
\]

Then

\[
\Delta \vec q^+ \approx P M \Delta \vec q_\mathrm{ref}
\]

Then

\[
\mathrm{Var} \left( \vec q^+ \right) = P M \mathrm{Var} \left( \vec q_\mathrm{ref} \right) M^T P^T
\]

I have $M$ from the [[#propagating-to-state-vector][above uncertainty propagation logic]], so I just need $P$.

In my usual least squares solve each chessboard point produces two elements
(horizontal, vertical error) of the measurements vector:

\[
\vec x = W \left( \mathrm{project}\left(\mathrm{intrinsics}, T_\mathrm{cam,ref} \, T_\mathrm{ref,frame} \, \vec p \right) -
\vec q_\mathrm{ref} \right)
\]

In this uncertainty quantification method I optimize the "cross reprojection
error": I look at the /perturbed/ chessboard,frames,points and the /unperturbed/
camera intrinsics, extrinsics. The data flows from the top-right to the
bottom-left:

\[
\xymatrix{
p_\mathrm{board}  \ar[d]^{T_{rf}} & p^*_\mathrm{board}  \ar[d]^{T_{r^*f^*}} \\
p_\mathrm{ref}    \ar[d]^{T_{cr}} \ar@{<->}[r]^{T_{rr^*}} &
p^*_\mathrm{ref}    \ar[d]^{T_{c^*r^*}} \\
p_\mathrm{cam}                 & p^*_\mathrm{cam}
}
\]

This requires computing a ref transformation to take into account the shifting
reference frame that results when re-optimizing:

\begin{aligned}
\vec x_\mathrm{cross} &=& \\
& & + W_\mathrm{board} \mathrm{project}\left(\mathrm{intrinsics},
                  T_\mathrm{cam_ref} T_\mathrm{ref,ref^+} T_\mathrm{ref^+,frame^+} p_\mathrm{board}\right) \\
& & - W_\mathrm{board} q_\mathrm{ref}_board \\
& & + W_\mathrm{point} \mathrm{project}\left(\mathrm{intrinsics},
                  T_\mathrm{cam,ref} T_\mathrm{ref,ref^+} p^+\right) \\
& & - W_\mathrm{point} q_\mathrm{ref}_point \\
\end{aligned}

This expression includes observations of chessboards and of discrete points.

For a given perturbation of the input observations I want to compute
T_\mathrm{ref_ref^+}. So here I look at an operating point T_\mathrm{ref_ref^+} = 0.
At the operating point I have x_cross0: affected by the input
perturbation, but not any reference transform.

I reoptimize norm2(x_cross) by varying rt_ref_ref^+. Let J_cross =
dx_cross/drt_ref_ref^+. I assume everything is locally linear, as defined by
J_cross. I minimize

  E = norm2(x_cross0 + dx_cross)

I set the derivative to 0:

  0 = dE/drt_ref_ref^+ ~ (x_cross0 + dx_cross)t J_cross

-> J_cross_t x_cross0 = -J_cross_t dx_cross

Furthermore, dx_cross = J_cross drt_ref_ref^+, so

  J_cross_t x_cross0 = -J_cross_t J_cross drt_ref_ref^+

and

  drt_ref_ref^+ = -inv(J_cross_t J_cross) J_cross_t x_cross0
               = -pinv(J_cross) x_cross0

The operating point is at rt_ref_ref^+ = 0, so the shift is off 0:

  rt_ref_ref^+ = 0 + drt_ref_ref^+
              = -pinv(J_cross) x_cross0

This is good, but implies that J_cross needs to be computed directly. We can do
better.

Since everything I'm looking at is near the original solution to the main
optimization problem, I can look at EVERYTHING in the linear space defined by
the optimal measurements x and their gradient J:

  x = + x0
      + J_intrinsics     dintrinsics
      + J_extrinsics     drt_cam_ref
      + J_frames         drt_ref_frame
      + J_points         dpoints
      + J_calobject_warp dcalobject_warp

Once again, we have this expression:

  x_cross =
    [
      + W_board \mathrm{project}\left(intrinsics,
                        T_\mathrm{cam_ref} T_\mathrm{ref_ref^+} T_\mathrm{ref^+_frame^+} pboard\right)
      - W_board q_\mathrm{ref}_board

      + W_point \mathrm{project}\left(intrinsics,
                        T_\mathrm{cam_ref} T_\mathrm{ref_ref^+} p^+\right)
      - W_point q_\mathrm{ref}_point
    ]

When evaluating x_cross0, I have rt_ref_ref^+ = 0, so I have perturbed
points and rt_ref_frame and calobject_warp only:

  x_cross0 =  + x0
              + J_points         M[points]         delta_q_\mathrm{ref}
              + J_frames         M[frames]         delta_q_\mathrm{ref}
              + J_calobject_warp M[calobject_warp] delta_q_\mathrm{ref}
            = + x0
              + J[frames,points,calobject_warp] db[frames,points,calobject_warp]

When evaluating J_cross = dx_cross/drt_ref_ref^+, I can look at it in two ways:

- a rt_cam_ref shift to compose_rt(rt_cam_ref,rt_ref_ref^+).

  J_cross_e = dx_cross/drt_ref_ref^+
            = J_extrinsics drt_cam_ref^+/drt_ref_ref^+
            = J_extrinsics d(compose_rt(rt_cam_ref,rt_ref_ref^+))/drt_ref_ref^+

  For observations that have no extrinsics (the camera is defined to sit at the
  ref coord system) this formulation is not possible. Because there is no
  J_extrinsics.

- a rt_ref_frame shift to compose_rt(rt_ref_ref^+,rt_ref^+_frame^+) and/or a point
  shift to transform_rt(rt_ref_ref^+,p^+)

  rt_ref^+_frame^+ is a tiny shift off rt_ref_frame AND I'm assuming that
  everything is locally linear. So this shift is insignificant, and I use
  rt_ref_frame to compute the gradient instead. Same for p^+/p.

  J_cross_f = dx_cross/drt_ref_ref^+
            = J_frame drt_ref_frame^+/drt_ref_ref^+
            = J_frame d(compose_rt(rt_ref_ref^+,rt_ref^+_frame^+))/drt_ref_ref^+
            = J_frame d(compose_rt(rt_ref_ref^+,rt_ref_frame))/drt_ref_ref^+

  J_cross_p = dx_cross/drt_ref_ref^+
            = J_p dp^+/drt_ref_ref^+
            = J_p d(transform(rt_ref_ref^+,p^+))/drt_ref_ref^+
            = J_p d(transform(rt_ref_ref^+,p ))/drt_ref_ref^+

There's one more simplification available. From above:

  rt_ref_ref^+ = -pinv(J_cross) x_cross0
              = -inv() J_cross_t x_cross0
              = ... J_frame_t (x0 + ...)

The original optimization problem has d/dx (x0_t x0) = 0 -> Jt x0 = 0. So
J_frame_t x0 = 0 as well, and thus instead of x_cross0 we can use

  dx_cross0 = J[frames,points,calobject_warp] db[frames,points,calobject_warp]

So we have rt_ref_ref^+ = K db for some K that depends on the various J matrices
that are constant for each solve:

  K = -pinv(J_cross) J[frames,points,calobject_warp]

Now that I have rt_ref_ref^+, I can use it to compute q^+. This
can accept arbitrary q, not just those in the solve, so I actually need to
compute projections, rather than looking at a linearized space defined by J

I have without any perturbations:

  pref = T_\mathrm{ref_cam} unproject(intrinsics, q)

For a given perturbation I have

  pref^+ = T_\mathrm{ref^+_ref} pref
  pcam^+ = T_\mathrm{cam^+_ref^+} pref^+
  q^+    = project(intrinsics^+, pcam^+)

Let's linearize everything:

q^+ - q

~   dq_dpcam (pcam^+ - pcam)
  + dq_dintrinsics db[intrinsics_this]

~   dq_dpcam (+ dpcam__drt_cam_ref db[extrinsics_this]
              + dpcam__dpref       (pref^+ - pref) )
  + dq_dintrinsics db[intrinsics_this]

~   dq_dpcam (  dpcam__drt_cam_ref db[extrinsics_this]
              + dpcam__dpref dpref^+__drt_ref_ref^+ rt_ref_ref^+)
  + dq_dintrinsics db[intrinsics_this]

~   dq_dpcam (  dpcam__drt_cam_ref db[extrinsics_this]
              - dpcam__dpref dpref^+__drt_ref_ref^+ pinv(J_cross) dx_cross0)
  + dq_dintrinsics db[intrinsics]

~   dq_dpcam (  dpcam__drt_cam_ref db[extrinsics_this]
              - dpcam__dpref dpref^+__drt_ref_ref^+ pinv(J_cross) J[frames_all,points_all,calobject_warp] D Dinv db[frames_all,points_all,calobject_warp])
  + dq_dintrinsics db[intrinsics_this]

~   dq_dpcam (  dpcam__drt_cam_ref db[extrinsics_this]
              + dpcam__dpref dpref^+__drt_ref_ref^+ K Dinv db[frames_all,points_all,calobject_warp])
  + dq_dintrinsics db[intrinsics_this]

--->

dq/db[extrinsics_this]                      = dq_dpcam dpcam__drt_cam_ref
dq/db[intrinsics_this]                      = dq_dintrinsics
dq/db[frames_all,points_all,calobject_warp] = dq_dpcam dpcam__dpref dpref^+__drt_ref_ref^+ K Dinv

============================================================================

I can also go the other way: traversing the diagram above from the top-left to
bottom-right. The derivation is similar, with slightly different results. We
have

  x_cross =
    [
      + W_board project(intrinsics^+,
                        T_\mathrm{cam^+_ref^+} T_\mathrm{ref^+_ref} T_\mathrm{ref_frame} pboard)
      - W_board q_\mathrm{ref}_board^+

      + W_point project(intrinsics^+,
                        T_\mathrm{cam^+_ref^+} T_\mathrm{ref^+_ref} p)
      - W_point q_\mathrm{ref}_point^+
    ]

And the optimum is at

  rt_ref^+_ref = 0 + drt_ref^+_ref
              = -pinv(J_cross) x_cross0

And we can compute the linearized quantities near rt_ref^+_ref = identity:

  rt_cam^+_ref = compose_rt(rt_cam^+_ref^+, rt_ref^+_ref)

  x_cross0 = + x0
             + J_intrinsics M[intrinsics] delta_q_\mathrm{ref}
             + J_extrinsics M[extrinsics] delta_q_\mathrm{ref}
             - W delta_q_\mathrm{ref}
           = + x0
             + J[intrinsics,extrinsics] db[intrinsics,extrinsics]
             - W delta_q_\mathrm{ref}

For points that have no extrinsics (the camera is defined to sit at the ref
coord system) there is no J_extrinsics, and we can ignore it here. But we must
use J_cross_f below in that case.

When evaluating J_cross = dx_cross/drt_ref^+_ref, I can once again look at it in
two ways:

- a rt_cam_ref shift to compose_rt(rt_cam^+_ref^+,rt_ref^+_ref).

  rt_cam^+_ref^+ is a tiny shift off rt_cam_ref AND I'm assuming that everything
  is locally linear. So this shift is insignificant, and I use rt_cam_ref to
  compute the gradient instead

  J_cross_e = dx_cross/drt_ref^+_ref
            = J_extrinsics drt_cam^+_ref/drt_ref^+_ref
            = J_extrinsics d(compose_rt(rt_cam^+_ref^+,rt_ref^+_ref))/drt_ref^+_ref
            = J_extrinsics d(compose_rt(rt_cam_ref,  rt_ref^+_ref))/drt_ref^+_ref

  As before, for points that have no extrinsics (the camera is defined to sit at
  the ref coord system) there is no J_extrinsics, so this formulation is not
  possible here. Use J_cross_fp.


- a rt_ref_frame shift to compose_rt(rt_ref^+_ref, rt_ref_frame) and/or a point
  shift to transform_rt(rt_ref^+_ref,p)

  J_cross_f = dx_cross/drt_ref^+_ref
            = J_frame drt_ref^+_frame/drt_ref^+_ref
            = J_frame d(compose_rt(rt_ref^+_ref,rt_ref_frame))/drt_ref^+_ref

  J_cross_p = dx_cross/drt_ref^+_ref
            = J_p dp^+/drt_ref^+_ref
            = J_p d(transform(rt_ref^+_ref,p ))/drt_ref^+_ref

There's one more simplification available. From above:

  rt_ref^+_ref = -pinv(J_cross) x_cross0
              = -inv() J_cross_t x_cross0
              = ... J_frame_t (x0 + ...)

And we can simplify to

  rt_ref^+_ref = -pinv(J_cross) dx_cross0

where

  dx_cross0 = J[intrinsics,extrinsics] db[intrinsics,extrinsics] - W delta_q_\mathrm{ref}

So we have rt_ref^+_ref = K db - W delta_q_\mathrm{ref} for some K that depends on the
various J matrices that are constant for each solve:

  K = -pinv(J_cross) J[intrinsics,extrinsics]

The rest of the data flow is the same as above, except we already have
rt_ref^+_ref, so we don't need to invert the transform when applying it.
