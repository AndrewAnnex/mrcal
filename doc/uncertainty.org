#+TITLE: Projection uncertainty

After a calibration has been computed, it is important to get a sense of how
good the calibration is. Without doing that, we have no idea of the precision of
the data products that use the calibration. We have a vague sense that "more
chessboard observations are better", but that's about it.

mrcal addresses this by providing an estimate of projection uncertainty
explicitly via the [[file:mrcal-python-api.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]] function. This tells you how
good your calibration is (lower projection uncertainties are good), and it tells
you how good your downstream results are (by allowing the user to propagate the
projection uncertainties through their data pipeline).

A grand summary of the process:

1. Estimate the noise distribution on the chessboard observations input to the
   calibration routine
2. Propagate that uncertainty to the optimal parameters $\vec p$ reported by the
   calibration routine
3. Propagate the uncertainty in calibration parameters $\vec p$ through the
   projection function to get uncertainty in the resulting pixel coordinate $\vec
   q$

This overall approach is sound, but it implies some limitations:

- Only the response to chessboard observation noise is taken into account. Any
  other issues are /not/ included in the reported uncertainty. Issues such as:
  motion blur, out-of-focus images, out-of-synchronization images, unexpected
  chessboard shape. It is thus imperative that we try to minimize these issues,
  and mrcal provides [[file:index.org::How to run a calibration][tools]] to detect some of these problems.

- A consequence of the above is that the choice of lens model affects the
  reported uncertainties. Lean models (those with few parameters) are less
  flexible than rich models, and don't fit general lenses as well as rich models
  do. However, this stiffness also serves to limit the model's response to noise
  in their parameters. So the above method will report less uncertainty for
  leaner models than rich models. So, unless we're /sure/ that a given lens
  follows some particular lens model perfectly, a [[file:index.org::Splined stereographic lens model][splined lens model]] (i.e. a
  very rich model) is recommended for truthful uncertainty reporting. Otherwise
  the reported confidence comes from the model itself, rather than the
  calibration data.

- Currently the uncertainty estimates can be computed only from a vanilla
  calibration problem: a set of stationary cameras observing a moving
  calibration object. Other formulations can be used to compute the lens
  parameters as well (structure-from-motion while also computing the lens models
  for instance), but at this time the uncertainty computations cannot handle
  those cases.

* Input noise model
I solve the calibration problem using [[https://en.wikipedia.org/wiki/Ordinary_least_squares][Ordinary Least Squares]], minimizing the
discrepancies between pixel observations and their predictions. The pixel
observations are noisy, and I assume they are zero-mean, independent and
normally-distributed. Empirical evidence suggests that this is a reasonable
assumption. I treat the 2 values in each observation as two independent
measurements. Thus I minimize a cost function $E \equiv \left \Vert \vec x
\right \Vert ^2$ where $\vec x$ is the full measurement vector.

Most elements of the measurement vector $\vec x$ depend on the pixel
observations, but some don't (regularization: often needed to help convergence;
small-enough to not break anything). For the purposes of propagating noise in
the input observations, we only care about the former. For the $i$ -th observed
point the optimized parameters $\vec p$ predict an observation at a pixel
coordinate $\vec q_i$, while we actually did observe that same point at $\vec
{q_\mathrm{ref}}_i$. What is $\mathrm{Var}\left(\vec {q_\mathrm{ref}}_i\right)$?

The chessboard corner detection routine tells us how confident it was in
that observation, and we use that to weight the measurements. For convenience,
the implementation splits this into two parts:

- The baseline standard deviation of the noise $\sigma$ (referred to as the
  =observed_pixel_uncertainty=)
- The unitless scale applied to that baseline noise level

This is useful when using [[https://github.com/dkogan/mrgingham/][=mrgingham=]] to detect the chessboard corners. It
reports the resolution used in detecting each point, and from that we get the
weight $w_i$. A point detected at half-resolution has double the uncertainty at
full resolution, and I weigh it by a factor of 2 less in the optimization:
$\mathrm{Var}\left( \vec {q_\mathrm{ref}}_i \right) = \frac{\sigma^2}{w_i^2} I$
for all $i$. And $\vec x_i = w_i I (\vec{q_i} - \vec {q_\mathrm{ref}}_i)$ and
$\mathrm{Var}\left( \vec x_i\right) = \sigma^2 I$ for all $i$.

So the variance on each pixel observation is identical, and the optimal
parameter vector we compute from the least-squares optimization is the
maximum-likelihood estimate of the true solution.

The noise $\sigma$ is hard to measure (there's an [[https://github.com/dkogan/mrgingham/blob/master/mrgingham-observe-pixel-uncertainty][attempt]] in mrgingham), but
easy to loosely estimate. The current best practice is to get a conservative
eyeball estimate to produce conservative estimates of projection uncertainty.

* Propagating input noise to the state vector
We solved the least squares problem, so we have the optimal state vector $\vec p^*$. Let's find
the uncertainty at this point.

We apply a perturbation to the observations $\vec q_\mathrm{ref}$, reoptimize
this slightly-perturbed least-squares problem (assuming everything is linear)
and look what happens to the optimal state vector $\vec p$.

We have

\[ E \equiv \left \Vert \vec x \right \Vert ^2 \]
\[ J \equiv \frac{\partial \vec x}{\partial \vec p} \]
\[ \frac{\partial E}{\partial \vec p} \left(\vec p = \vec p^* \right) = 2 J^T \vec x^* = 0 \]

We perturb the problem:

\[ E( \vec p + \Delta \vec p, \vec q_\mathrm{ref} + \Delta \vec q_\mathrm{ref})) \approx \left \Vert \vec x + J \Delta \vec p + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \right \Vert ^2 \]

And we reoptimize:

\[ \frac{\mathrm{d}E}{\mathrm{d}\Delta \vec p} \approx 
2 \left( \vec x + J \Delta \vec p + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \right)^T J = 0\]

we started at an optimum, so $J^T \vec x^* = 0$, and thus

\[ J^T J \Delta \vec p = -J^T \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \]

As stated above, for reprojection errors we have

\[ \vec x_\mathrm{observations} = W (\vec q - \vec q_\mathrm{ref}) \]

where $W$ is a diagonal matrix of weights. Let's assume the non-observations
elements of $\vec x$ are at the end of $\vec x$, so

\[ \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} =
\left[ \begin{array}{cc} - W \\ 0 \end{array} \right] \]

and thus

\[ J^T J \Delta \vec p = -J_\mathrm{observations}^T W \Delta \vec q_\mathrm{ref} \]

So if we perturb the input observation vector $q_\mathrm{ref}$ by $\Delta
q_\mathrm{ref}$, the resulting effect on the optimal parameters is $\Delta \vec
p = M \Delta \vec q_\mathrm{ref}$. Where

\[ M = - \left( J^T J \right)^{-1} J_\mathrm{observations}^T W \]

So

\[ \mathrm{Var}(\vec p) = M \mathrm{Var}\left(\vec q_\mathrm{ref}\right) M^T \]

As stated before, we're assuming independent noise on all observed pixels, with
a standard deviation inversely proportional to the weight:

\[ \mathrm{Var}\left( \vec q_\mathrm{ref} \right) = \sigma^2 W^{-2} \]

so

\begin{eqnarray*}
\mathrm{Var}\left(\vec p\right) &=& \sigma^2 M W^{-2} M^T \\
&=& \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T W W^{-2} W J_\mathrm{observations} \left( J^T J \right)^{-1} \\
&=& \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T J_\mathrm{observations}  \left( J^T J \right)^{-1}
\end{eqnarray*}

If we have no regularization, and all measurements are pixel errors, then
$J_\mathrm{observations} = J$ and

\[\mathrm{Var}\left(\vec p\right) = \sigma^2 \left( J^T J \right)^{-1} \]

Note that this does not explicitly depend on $W$. However, the weights are a
part of $J$. So if observation $i$ were to become less precise,
$\mathrm{Var}\left(\vec {q_\mathrm{ref}}_i \right)$ would increase, which means
that $w_i$ and $x_i$ and $J_i$ would all decrease. And as a result,
$\mathrm{Var}\left(\vec p\right)$ would increase, as expected.

* Propagating the state vector noise through projection
I now have the variance of the full optimization state $\vec p$. This contains
the intrinsics and extrinsics of /all/ the cameras. And it contains /all/ the
poses of observed chessboards, and everything else, like the chessboard warp
terms.

How are those parameters used during the optimization? The fundamental operation
is projecting points in a "frame" coordinate system (the coordinate system of a
chessboard). Projecting a point $p_\mathrm{chessboard}$ involves several
transformations and then a projection:

\[ \vec q                     \xleftarrow{\mathrm{intrinsics}}
   \vec p_\mathrm{camera}     \xleftarrow{T_\mathrm{cr}}
   \vec p_\mathrm{reference}  \xleftarrow{T_\mathrm{rf}}
   \vec p_\mathrm{frame}
\]

Here the $\mathrm{intrinsics}$ are the lens parameters, $T_\mathrm{cr}$ is the
extrinsics transformation, and $T_\mathrm{rf}$ is the "frame" transformation.
Each is an element of the state vector $\vec p$ whose uncertainty we have.

So how can we estimate $\mathrm{Var}\left( \vec q \right)$? The simplest thing
to do is to focus just on the projection operation:

\[\vec q = \mathrm{project}\left(\vec p_\mathrm{camera}, \mathrm{intrinsics}\right)\]

We can use this expression to propagate the intrinsics uncertainties, but this
is insufficient. We want to know the projection uncertainty of points in a
/fixed/ coordinate system, a coordinate system that doesn't move due to random
shifts in the state $\vec p$. As we can see above, $\vec p_\mathrm{camera}$
depends on the extrinsics, which are a part of the state.

But what if we only have one camera, and thus we have no extrinsics (the camera
coordinate system /is/ the reference coordinate system)? This doesn't work
either. The lens intrinsics encode an implied transformation that moves the
camera coordinate system, so once again $\vec p_\mathrm{camera}$ would move in
response to our perturbation.

So how do we operate on points in a fixed coordinate system when all the
coordinate systems we have are floating random variables? We can use the poses
of the observed chessboards in aggregate: these are the most fixed thing we
have.

Let's focus on /one/ observed chessboard frame: frame 0. I want to know the
uncertainty at a pixel coordinate $\vec q$. I follow the sequence above in
reverse:

\[ \vec p_{\mathrm{frame}_0} = T_{\mathrm{f}_0\mathrm{r}} T_\mathrm{rc} \mathrm{unproject}\left( \vec q \right) \]

This is a "fixed" point. I then transform and project $\vec p_{\mathrm{frame}_0}$
back to the imager to get $\vec q^+$. But here I take into account the
uncertainties of each transformation to get the desired projection uncertainty
$\mathrm{Var}\left(\vec q^+ - \vec q\right)$. The full data flow looks like
this, with all the perturbed quantities superscripted with a $+$.

\[
   \vec q^+                      \xleftarrow{\mathrm{intrinsics}^+}
   \vec p^+_\mathrm{camera}      \xleftarrow{T^+_\mathrm{cr}}
   \vec p^+_{\mathrm{reference}_0}  \xleftarrow{T^+_{\mathrm{rf}_0}} \vec p_{\mathrm{frame}_0} \xleftarrow{T_\mathrm{fr}}
   \vec p_\mathrm{reference}
   \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
   \xleftarrow{\mathrm{intrinsics}}
   \vec q
\]

This works, but it depends on $\vec p_{\mathrm{frame}_0}$ being "fixed", which it
isn't, since $T_\mathrm{f0r}$ is in the optimization state /and/ since the
reference coordinate system that $T_\mathrm{f0r}$ relates to isn't fixed either.
However, we're observing more than one chessboard, and /together/ all the
chessboard frames can represent a mostly-fixed reference.

How do I combine all the different estimates from the different chessboard
observations? I take a very simple approach: I compute the mean of all the $\vec
p^+_\mathrm{reference}$ estimates from each frame. The full data flow looks like
this:

\begin{aligned}
   & \swarrow                   & \vec p^+_{\mathrm{reference}_0}  & \xleftarrow{T^+_{\mathrm{rf}_0}} & \vec p_{\mathrm{frame}_0} & \nwarrow & \\
   \vec q^+                      \xleftarrow{\mathrm{intrinsics}^+}
   \vec p^+_\mathrm{camera}      \xleftarrow{T^+_\mathrm{cr}}
   \vec p^+_\mathrm{reference}
   & \xleftarrow{\mathrm{mean}} & \vec p^+_{\mathrm{reference}_1}  & \xleftarrow{T^+_{\mathrm{rf}_1}} & \vec p_{\mathrm{frame}_1} & \xleftarrow{T_\mathrm{fr}} &
   \vec p_\mathrm{reference}
   \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
   \xleftarrow{\mathrm{intrinsics}}
   \vec q \\
   & \nwarrow                   & \vec p^+_{\mathrm{reference}_2}  & \xleftarrow{T^+_{\mathrm{rf}_2}} & \vec p_{\mathrm{frame}_2} & \swarrow
\end{aligned}

This is better, but has another issue. What is the transformation relating the
original and perturbed reference coordinate systems?

\[ T_{\mathrm{r}^+\mathrm{r}} = \mathrm{mean}_i \left( T_{\mathrm{r}^+\mathrm{f}_i} T_{\mathrm{f}_i\mathrm{r}} \right) \]

Each transformation $T$ includes a rotation matrix $R$, so the above constructs
a new rotation as a mean of multiple rotation matrices, which is aphysical: the
resulting matrix is not a valid rotation. In practice, the perturbations are
tiny, and this is sufficiently close. Extreme geometries do break this, and I
will tweak this approach in the future.

* Implementation details

I computed Var(p) earlier, which contains the variance of ALL the optimization
parameters together. The noise on the chessboard poses is coupled to the noise
on the extrinsics and to the noise on the intrinsics. And we can apply all these
together to propagate the uncertainty.

Let's define some variables:

- p_i: the intrinsics of a camera
- p_e: the extrinsics of that camera (T_cr)
- p_f: ALL the chessboard poses (T_fr)
- p_ief: the concatenation of p_i, p_e and p_f

I have

    dq = q0 + dq/dp_ief dp_ief

    Var(q) = dq/dp_ief Var(p_ief) (dq/dp_ief)t

    Var(p_ief) is a subset of Var(p), computed above.

    dq/dp_ief = [dq/dp_i dq/dp_e dq/dp_f]

    dq/dp_e = dq/d\vec p_\mathrm{camera} d\vec p_\mathrm{camera}/dp_e

    dq/dp_f = dq/d\vec p_\mathrm{camera} d\vec p_\mathrm{camera}/dpref dpref/dp_f / Nframes

dq/dp_i and all the constituent expressions comes directly from the project()
and transform calls above. Depending on the details of the optimization problem,
some of these may not exist. For instance, if we're looking at a camera that is
sitting at the reference coordinate system, then there is no p_e, and Var_ief is
smaller: it's just Var_if. If we somehow know the poses of the frames, then
there's no Var_f. If we want to know the uncertainty at distance=infinity, then
we ignore all the translation components of p_e and p_f.

And note that this all assumes a vanilla calibration setup: we're calibration a
number of stationary cameras by observing a moving object. If we're instead
moving the cameras, then there're multiple extrinsics vectors for each set of
intrinsics, and it's not clear what projection uncertainty even means.

Note a surprising consequence of all this: projecting k*\vec p_\mathrm{camera} in camera
coordinates always maps to the same pixel coordinate q for any non-zero scalar
k. However, the uncertainty DOES depend on k. If a calibration was computed with
lots of chessboard observations at some distance from the camera, then the
uncertainty of projections at THAT distance will be much lower than the
uncertanties of projections at any other distance. And as we get closer and
closer to the camera, the uncertainty grows to infinity as the translation
uncertainty in the extrinsics begins to dominate.

Alright, so we have Var(q). We could claim victory at that point. But it'd be
nice to convert Var(q) into a single number that describes my projection
uncertainty at q. Empirically I see that Var(dq) often describes an eccentric
ellipse, so I want to look at the length of the major axis of the 1-sigma
ellipse:

    eig (a b) --> (a-l)*(c-l)-b^2 = 0 --> l^2 - (a+c) l + ac-b^2 = 0
        (b c)

    --> l = (a+c +- sqrt( a^2+2ac+c^2 - 4ac + 4b^2)) / 2 =
          = (a+c +- sqrt( a^2-2ac+c^2 + 4b^2)) / 2 =
          = (a+c)/2 +- sqrt( (a-c)^2/4 + b^2)

So the worst-case stdev(q) is

$\sqrt{\frac{a+c}{2} + \sqrt{ \frac{\left(a-c\right)^2}{4} + b^2}}$
