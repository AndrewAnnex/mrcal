#+title: mrcal - a toolkit for manipulating cameras, projections and geometry
#+author: Dima Kogan
#+email: dima@secretsauce.net
#+language: en

#+options: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+options: html-scripts:t html-style:t html5-fancy:nil tex:t
#+html_doctype: xhtml-strict
#+html_container: div
#+description:
#+keywords:
#+html_link_home:
#+html_link_up:
#+html_mathjax:
#+HTML_HEAD: <style>pre.src {background-color: #303030; color: #e5e5e5;}</style>
#+html_head_extra:
#+subtitle:
#+infojs_opt:
#+creator: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 28.0.50 (<a href="https://orgmode.org">Org</a> mode 9.3)
#+latex_header:



* Intro
mrcal is a toolkit for working with lens models, camera geometry, images,
projections, and the various related operations such as camera calibration. This
toolkit was originally built to produce high-accuracy calibrations demanded by
long-range stereo, so it provides facilities to analyze the results and to track
down sources of error.

mrcal provides a routine to compute the "[[Model differencing][difference]]" between two models, which
can be a fundamental piece of a wide number of analyses, for instance to measure
a lens's response to temperature cycles.

mrcal provides estimates of [[Projection uncertainty][projection uncertainty]], which can be used to gauge
calibration quality, and to compute the uncertainty of any data products that
use the lens model.

A rich, [[Splined stereographic lens model][splined lens model]] is available to fit any projection function and to
provide realistic uncertainty estimates.

The core functionality is exposed from the [[C API][C API]], while higher-level routines
are available through [[Python API][Python]]. The most common workflows are available as
[[Commandline tools][commandline tools]], with no coding required.

Please see [[file:tour.org][a tour of mrcal]] for a high-level overview of the capabilities of the
toolkit.

* Conventions
** Terminology
Some terms in the documentation and sources can have ambiguous meanings, so I
explicitly define them here

- *calibration*: the procedure used to compute the lens parameters and geometry
  of a set of cameras. Usually this involves a stationary set of cameras
  observing a moving object.

- *calibration object* or *chessboard* or *board*: these are used more or less
  interchangeably. They refer to the known-geometry object observed by the
  cameras, with those observations used as input during calibration

- *pose*: a position and orientation

- *intrinsics*: the parameters describing the behavior of a lens. The pose of
  the lens does not affect the intrinsics

- *extrinsics*: the pose of a lens in respect to some fixed coordinate system

- *frames*: in the context of mrcal's optimization these refer to an array of
  poses of the observed chessboards

- *SFM*: structure from motion. This is the converse of "calibration": we
  observe a stationary scene from a moving camera to compute the geometry of the
  scene

- *state*: the vector of parameters that an optimization algorithm is free to
  move around as it searches for the optimum. mrcal generally refers to this
  vector as $\vec p$

- *measurements* or *residuals*: I use these more or less interchangeably. This
  is the vector whose norm the optimization algorithm is trying to minimize.
  mrcal generally refers to this as $\vec x$, and it contains differences
  between pixel coordinates observed by a camera, and where the state vector
  $\vec p$ predicts those observations should be. The optimization tries to
  minimize these differences by finding the $\vec p$ that minimizes $\left \Vert
  \vec x \right \Vert ^2$

- *project*: to map a point in space to a pixel coordinate where that point
  would be observed by a given camera

- *unproject*: to map a pixel coordinate back to a point in space that would
  produce an observation at that pixel. Unprojection is only unique up-to scale

- *camera model*: this is used to refer to the intrinsics and extrinsics
  together.

** Symbols
*** Geometry
- $\vec q$ is a 2-dimensional vector representing a pixel coordinate: $\left( x,y \right)$

- $\vec v$ is a 3-dimensional vector representing a /direction/ $\left( x,y,z
  \right)$ in space. $\vec v$ is only defined up-to-length. In a camera's
  coordinate system we have $\vec q = \mathrm{project}\left(\vec v \right)$

- $\vec p$ is a 3-dimensional vector representing a /point/ $\left( x,y,z
  \right)$ in space. Unlike $\vec v$, $\vec p$ has a defined range. Like $\vec
  v$ we have $\vec q = \mathrm{project}\left(\vec p \right)$

*** Optimization
The core of the mrcal calibration routine is a nonlinear least-squares
optimization

\[
\min_{\vec p} E = \min_{\vec p} \left \Vert \vec x \left( \vec p \right) \right \Vert ^2
\]

Here we have

- $\vec p$ is the vector of parameters being optimized. It's clear from context
  whether $\vec p$ refers to some point in space, or the optimization vector.

- $\vec x$ is the vector of /measurements/ describing the error of the solution
  at some hypothesis $\vec p$

- $\vec E$ is the cost function being optimized. $E \equiv \left \Vert \vec x \right \Vert ^2$

- $\vec J$ is the /jacobian/ matrix. This is the matrix $\frac{ \partial \vec x
  }{ \partial \vec p }$ 

** Camera coordinate system
mrcal uses right-handed coordinate systems. No convention is assumed for the
world coordinate system. The canonical /camera/ coordinate system has =x,y= as
with pixel coordinates in an image: =x= is to the "right" and =y= is "down". =z=
is then "forward" to complete the right-handed system of coordinates.

** Transformations
We describe transformations as mappings between a representation of a point in
one coordinate system to a representation of the /same/ point in another
coordinate system. =T_AB= is a transformation from coordinate system =B= to
coordinate system =A=. These chain together nicely, so if we know the
transformation between =A= and =B= and between =B= and =C=, we can transform a
point represented in =C= to =A=: =x_A = T_AB T_BC x_C = T_AC x_C=. And =T_AC =
T_AB T_BC=.

** Poses

Various parts of the toolkit have preferred representations of pose, and mrcal
has functions to convert between them. Available representations are:

- =Rt=: a (4,3) numpy array with a (3,3) rotation matrix concatenated with a
  (1,3) translation vector. This form is easy to work with, but there are
  implied constraints: most (4,3) numpy arrays are /not/ valid =Rt=
  transformations.

- =rt=: a (6,) numpy array with a (3,) vector representing a Rodrigues rotation
  concatenated with another (3,) vector, representing a rotation. This form
  requires more computations to deal with, but has no implied constraints: /any/
  (6,) numpy array is a valid =rt= transformation. Thus this is the form used
  inside the mrcal optimization routine.

Each of these represents a transformation =rotate(x) + t=.

Since a pose represents a transformation between two coordinate systems, the
toolkit generally refers to a pose as something like =Rt_AB=, which is an
=Rt=-represented transformation to convert a point from a representation in the
coordinate system =B= to a representation in coordinate system =A=.

A Rodrigues rotation vector =r= represents a rotation of =length(r)= radians
around an axis in the direction =r=. Converting between =R= and =r= is done via
the [[https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula][Rodrigues rotation formula]]: using the [[file:mrcal-python-api.html#-r_from_R][=mrcal.r_from_R()=]] and
[[file:mrcal-python-api.html#-R_from_r][=mrcal.R_from_r()=]] functions. For translating /poses/, not just rotations, use
[[file:mrcal-python-api.html#-rt_from_Rt][=mrcal.rt_from_Rt()=]] and [[file:mrcal-python-api.html#-Rt_from_rt][=mrcal.Rt_from_rt()=]].

There're [[file:mrcal-python-api.html#-R_from_quat][several]] [[file:mrcal-python-api.html#-quat_from_R][functions]] to work with unit quaternions as a rotation
representation, but they're lightly used, and exist only for compatibility with
other tools. mrcal does not use quaternions.

** Linear algebra
mrcal follows the usual linear algebra convention of column vectors. So applying
a rotation looks like $\vec b = R \vec a$ where both $\vec a$ and $\vec b$ are
column vectors.

However, numpy print vectors (1-dimensional objects), as /row/ vectors, so the
code treats 1-dimensional objects as transposed vectors. In the code, the above
rotation would be implemented equivalently: $\vec b^T = \vec a^T R^T$. The
[[file:mrcal-python-api.html#-rotate_point_R][=mrcal.rotate_point_R()=]] and [[file:mrcal-python-api.html#-transform_point_Rt][=mrcal.transform_point_Rt()=]] functions serve to
handle this transparently.

A similar issue is that numpy follows the linear algebra convention of indexing
with =(index_column, index_row)= and not the other way around. This runs against
the /other/ convention of referring to image dimensions as =(width, height)= and
referring to pixels as =(x,y)=. mrcal places the =x= coordinate first (as in the
latter) whenever possible, but when interacting directly with numpy, it must
place the =y= coordinate first. The choice being made is very clearly
documented, so when in doubt, do read the docs.

When computing gradients mrcal places the dependent variables in the leading
dimensions, and the independent variables in the trailing dimensions. So in the
above expressions we have $\frac{ \partial \vec b }{ \partial \vec a } = R$ and
row $i$ of $R$ represents the $\frac{ \partial b_i }{ \partial \vec a }$

** Implementation
The core of mrcal is written in C, but most of the API is currently available in
Python only. The python-wrapping is done via the [[https://github.com/dkogan/numpysane/blob/master/README-pywrap.org][=numpysane_pywrap=]] library,
which makes it fairly simple to make the Python interface /and/ provides
[[https://numpy.org/doc/stable/user/basics.broadcasting.html][broadcasting]] support. So over time more of the Python API will become translated
to C, and exposed there, as needed.

The Python layer uses [[https://numpy.org/][numpy]] and [[https://github.com/dkogan/numpysane/][=numpysane=]] heavily. All the plotting is done
with [[https://github.com/dkogan/gnuplotlib][=gnuplotlib=]]. [[https://opencv.org/][OpenCV]] is used a bit, but /only/ in the Python layer (their C
APIs are gone, and the C++ APIs are unstable). Over time the dependence on this
library will decrease even further.

* Camera model file formats

Reading/writing camera models is done in Python with the [[file:mrcal-python-api.html#cameramodel][=mrcal.cameramodel=]]
class. This class supports two different file formats:

- =.cameramodel=: the preferred format. This is a plain text representation of a
  Python =dict=. The pose is represented internally as =rt_fromref=: an =rt=
  transformation /from/ the reference coordinate system /to/ the coordinate
  system of this camera. That is the /internal/ representation: the class
  provides methods to get the transformation in any form.

- =.cahvor=: the alternative format available for compatibility with existing
  tools. If you don't need to interoperate with tools that require this format,
  there's little reason to use it. This format cannot store [[Splined stereographic lens model][splined models]] or
  the auxillary data required for the [[Projection uncertainty][uncertainty computations]].

The [[file:mrcal-python-api.html#cameramodel][=mrcal.cameramodel=]] class will intelligently pick the correct file format
based on the filename. The file format is just a way to store data: both the
CAHVOR and OpenCV lens models can be stored in either file format. The
[[file:mrcal-to-cahvor.html][=mrcal-to-cahvor=]] and [[file:mrcal-to-cameramodel.html][=mrcal-to-cameramodel=]] tools can be used to convert
between the two file formats.

The class (and its representation on disk) contains:

- The lens parameters
- The pose of the camera in space
- The =optimization_inputs=: the data used to compute the model initially. Used
  for the uncertainty computations

See the [[file:mrcal-python-api.html#cameramodel][API documentation]] for usage details. A trivial example to

- read two models from disk
- recombine into a joint model that uses the lens parameters from one model with
  geometry from the other
- write to disk

#+begin_src python
model_for_intrinsics = mrcal.cameramodel('model0.cameramodel')
model_for_extrinsics = mrcal.cameramodel('model1.cameramodel')

model_joint = mrcal.cameramodel( model_for_intrinsics )

extrinsics = model_for_extrinsics.extrinsics_rt_fromref()
model_joint.extrinsics_rt_fromref(extrinsics)

model_joint.write('model-joint.cameramodel')
#+end_src

This is the basic operation of the [[file:mrcal-graft-models.html][=mrcal-graft-models= tool]].

* Lens models
mrcal supports a wide range of lens models. The full set of supported models is
returned by the [[file:mrcal-python-api.html#-supported_lensmodels][=mrcal.supported_models()=]] function. At the time of this writing
the supported models are:

- =LENSMODEL_PINHOLE=
- =LENSMODEL_STEREOGRAPHIC=
- =LENSMODEL_SPLINED_STEREOGRAPHIC_...=
- =LENSMODEL_OPENCV4=
- =LENSMODEL_OPENCV5=
- =LENSMODEL_OPENCV8=
- =LENSMODEL_OPENCV12=
- =LENSMODEL_CAHVOR=
- =LENSMODEL_CAHVORE=

In Python, the models are represented as one of the above strings. In C, in an
enum with =MRCAL_= prepended, and the =...= placeholder stripped. The =...=
above means that this model has /configuration parameters/ that would replace
the =...= placeholder. These are specific to each kind of model, and currently
only the [[Splined stereographic lens model][splined stereographic models]] have any configuration. The number of
parameters needed to fully describe a given model can be obtained by calling
[[file:mrcal-python-api.html#-lensmodel_num_params][=mrcal.lensmodel_num_params()=]] in Python or [[file:mrcal.h.html#mrcal_lensmodel_num_params][=mrcal_lensmodel_num_params()=]] in C.
Any configuration /must/ be included.

In C, the raw type of model is represented by the [[file:mrcal.h.html#mrcal_lensmodel_type_t][=mrcal_lensmodel_type_t=]] enum.
The model type /and/ the configuration are represented by [[file:mrcal.h.html#mrcal_lensmodel_t][=mrcal_lensmodel_t=]].

The pinhole and stereographic models are very simple, and are usually used as
part of data processing pipelines rather than trying to represent real-world
lenses. The splined stereographic model is [[Splined stereographic lens model][described in great detail later]]. This
is the recommended lens model to get the most fidelity and reliable
[[Projection uncertainty][uncertainty estimates]].

The CAHVOR(E) and OpenCV lens models are supported by many other tools, so mrcal
also supporting them provides interoperability. These are much leaner than the
[[Splined stereographic lens model][splined models]], so they have many fewer parameters. Thus they need far less
computation, but they're not as good at representing arbitrary lenses, and they
provide overly-optimistic [[Projection uncertainty][uncertainty estimates]].

CAHVORE is only partially supported: lensmodel parameter gradients aren't
implemented, so it isn't currently possible to solve for a CAHVORE model. Full
support may be added in the future.

* Calibration object
This is called a "chessboard" or just "board" in some parts of the code.

When running a camera calibration, we use observations of a known-geometry
object. Usually this object is a chessboard-like grid of black and white
squares, where the corners between squares are detected, and serve as the input
features to mrcal. mrcal is a purely geometrical toolkit, so this vision problem
must be handled by another library. I recommend [[https://github.com/dkogan/mrgingham/][=mrgingham=]], but any other
source of grid observations can be used.

The specific design of the calibration object is not important, as long as it
meets the current assumptions of the tool:

- flat (with a small amount of warping tolerated)
- contains a square grid of features

Chessboards are recommended, in contrast to grids of circles, which are strongly
discouraged. Precisely extracting the center of an observed circle from a tilted
observation that is also subjected to lens distortion is very difficult. And the
resulting inaccurate detections will introduce biases into the resulting
calibrations. Analysis [[file:tour.org::Optimal choreography][here]].

mrcal assumes independent noise on each point observation, so correlated sources
of points (such as corners of an apriltag) are not appropriate sources of data
currently.

* Commandline tools
#+NAME: Commandline tools
A number of commandline tools are available for common tasks.

- [[file:mrcal-calibrate-cameras.html][=mrcal-calibrate-cameras=]]: calibrate N cameras. 
- [[file:mrcal-convert-lensmodel.html][=mrcal-convert-lensmodel=]]: fits one lens model to another
- [[file:mrcal-show-distortion-off-pinhole.html][=mrcal-show-distortion-off-pinhole=]]: visualize the deviation of a specific
  lens model from a pinhole model
- [[file:mrcal-show-splined-model-surface.html][=mrcal-show-splined-model-surface=]]: visualize the surface and knots used in
  the specification of splined models
- [[file:mrcal-show-projection-uncertainty.html][=mrcal-show-projection-uncertainty=]]: visualize the uncertainty of intrinsics
  due to noise in the calibration inputs
- [[file:mrcal-show-projection-diff.html][=mrcal-show-projection-diff=]]: visualize the difference between the
  intrinsics of a number of models
- [[file:mrcal-reproject-points.html][=mrcal-reproject-points=]]: Given two lens models and a set of points,
  maps them from one lens model to the other
- [[file:mrcal-reproject-image.html][=mrcal-reproject-image=]]: Given image(s) and lens model(s), produces a new
  set of images that observe the same scene but with the other model. Several
  flavors of functionality are included here, such as undistortion-to-pinhole,
  re-rotation, and remapping to infinity.
- [[file:mrcal-graft-models.html][=mrcal-graft-models=]]: Combines the intrinsics of one cameramodel with the
  extrinsics of another
- [[file:mrcal-to-cahvor.html][=mrcal-to-cahvor=]]: Converts a model stored in the native =.cameramodel= file
  format to the =.cahvor= format. This exists for compatibility only, and does
  not touch the data: the lens distortion may or may not use the CAHVOR
  lens model
- [[file:mrcal-to-cameramodel.html][=mrcal-to-cameramodel=]]: Converts a model stored in the legacy =.cahvor= file
  format to the =.cameramodel= format. This exists for compatibility only, and
  does not touch the data: the lens distortion may or may not use the CAHVOR
  lens model
- [[file:mrcal-show-geometry.html][=mrcal-show-geometry=]]: Shows a visual representation of the geometry
  represented by some camera models on disk, and optionally, the
  chessboard observations used to compute that geometry
- [[file:mrcal-show-valid-intrinsics-region.html][=mrcal-show-valid-intrinsics-region=]]: Visualizes the region where a model's
  intrinsics are valid
- [[file:mrcal-is-within-valid-intrinsics-region.html][=mrcal-is-within-valid-intrinsics-region=]]: Augments a vnlog of pixel
  coordinates with a column indicating whether or not each point lies within
  the valid-intrinsics region

mrcal-cull-corners

* Developer manual (APIs)
** Python API
#+NAME: Python API

The full Python API reference is available [[file:mrcal-python-api.html][here]]. Note that everything has
docstrings, so the =pydoc3= tool is effective at displaying the relevant
documentation. For convenience, all the docstrings have been extracted and
formatted into the webpage linked above.

** C API
#+NAME: C API

[[file:mrcal.h.html][=mrcal.h=]]

[[file:basic_geometry.h.html][=basic_geometry.h=]]

[[file:poseutils.h.html][=poseutils.h=]]

* How to run a calibration
#+NAME: 
talk about --seed and how that can be used to validate intrinsics

** Tutorial
If all you want to do is run a calibration, read this section first.

You need to get observations of a grid of points. This tool doesn't dictate
exactly how these observations are obtained, but the recommended way to do that
is to use mrgingham (http://github.com/dkogan/mrgingham). This documentation
assumes that's what is being done.

See the mrgingham documentation for a .pdf of a chessboard pattern. This pattern
should be printed (at some size; see below) and mounted onto a RIGID and FLAT
surface to produce the calibration object. The most useful observations are
close-ups: views that cover as much of the imager as possible. Thus you
generally a large printout of the chessboard pattern. If you're calibrating a
wide lens then this is especially true: the wider the lens, the larger an object
needs to be in order to cover the field of view.

Now that we have a calibration object, this object needs to be shown to the
camera(s) to produce the images that mrgingham will use to find the corner
coordinates, which mrcal will then use in its computations.

It is important that the images contain clear corners. If the image is badly
overexposed, the white chessboard squares will bleed into each other, the
adjoining black squares will no longer touch each other in the image, and there
would be no corner to detect. Conversely, if the image is badly underexposed,
the black squares will bleed into each other, which would also destroy the
corner. mrgingham tries to handle a variety of lighting conditions, including
varying illumination across the image, but the corners must exist in the image
in some form. A fundamental design decision in mrgingham is to only output
chessboards that we are very confident in, and a consequence of this is that
mrgingham requires the WHOLE chessboard to be visible in order to produce any
results. Thus it requires a bit of effort to produce any data at the edges and
in the corners of the imager: if even a small number of the chessboard corners
are out of bounds, mrgingham will not detect the chessboard at all. A live
preview of the calibration images being gathered is thus essential to aid the
user in obtaining good data. Another requirement due to the design of mrgingham
is that the board should be held with a flat edge parallel to the camera xz
plane (parallel to the ground, usually). mrgingham looks for vertical and
horizontal sequences of corners, but if the board is rotated diagonally, then
none of these sequences are "horizontal" or "vertical", but they're all
"diagonal", which isn't what mrgingham is looking for.

The most useful observations to gather are

- close-ups: the chessboard should fill the whole frame as much as possible

- oblique views: tilt the board forward/back and left/right. I generally tilt by
  more than 45 degrees. At a certain point the corners become indistinct and
  mrgingham starts having trouble, but depending on the lens, that point could
  come with quite a bit of tilt.

- If you are calibrating multiple cameras, and they are synchronized, you can
  calibrate them all at the same time, and obtain intrinsics AND extrinsics. In
  that case you want frames where multiple cameras see the calibration object at
  the same time. Depending on the geometry, it may be impossible to place a
  calibration object in a location where it's seen by all the cameras, AND where
  it's a close-up for all the cameras at the same time. In that case, get
  close-ups for each camera individually, and get observations common to
  multiple cameras, that aren't necessarily close-ups. The former will serve to
  define your camera intrinsics, and the latter will serve to define your
  extrinsics (geometry).

A dataset composed primarily of tilted closeups will produce good results. It is
better to have more data rather than less. mrgingham will throw away frames
where no chessboard can be found, so it is perfectly reasonable to grab too many
images with the expectation that they won't all end up being used in the
computation.

I usually aim for about 100 usable frames, but you can often get away with far
fewer. The mrcal confidence feedback (see below) will tell you if you need more
data.

Once we have gathered input images, we can run the calibration procedure:

  mrcal-calibrate-cameras
    --corners-cache corners.vnl
    -j 10
    --focal 2000
    --object-spacing 0.1
    --object-width-n 10
    --outdir /tmp
    --lensmodel LENSMODEL_OPENCV8
    --observed-pixel-uncertainty 1.0
    --explore
    'frame*-camera0.png' 'frame*-camera1.png' 'frame*-camera2.png'

You would adjust all the arguments for your specific case.

The first argument says that the chessboard corner coordinates live in a file
called "corners.vnl". If this file exists, we'll use that data. If that file
does not exist (which is what will happen the first time), mrgingham will be
invoked to compute the corners from the images, and the results will be written
to that file. So the same command is used to both compute the corners initially,
and to reuse the pre-computed corners with subsequent runs.

'-j 10' says to spread the mrgingham computation across 10 CPU cores. This
command controls mrgingham only; if 'corners.vnl' already exists, this option
does nothing.

'--focal 2000' says that the initial estimate for the camera focal lengths is
2000 pixels. This doesn't need to be precise at all, but do try to get this
roughly correct if possible. Simple geometry says that

  focal_length = imager_width / ( 2 tan (field_of_view_horizontal / 2) )

--object-spacing is the width of each square in your chessboard. This depends on
the specific chessboard object you are using. --object-width-n is the corner
count of the calibration object. Currently mrgingham more or less assumes that
this is 10.

--outdir specifies the directory where the output models will be written

--lensmodel specifies which lens model we're using for the cameras.
At this time all OpenCV lens models are supported, in addition to
LENSMODEL_CAHVOR. The CAHVOR model is there for legacy compatibility only. If
you're not going to be using these models in a system that only supports CAHVOR,
there's little reason to use it. If you use a model that is too lean
(LENSMODEL_PINHOLE or LENSMODEL_OPENCV4 maybe), the model will not fit the data,
especially at the edges; the tool will tell you this. If you use a model that is
too rich (something crazy like LENSMODEL_OPENCV12), then you will need much
more data than you normally would. Most lenses I've seen work well with
LENSMODEL_OPENCV4 or LENSMODEL_OPENCV5 or LENSMODEL_OPENCV8; wider lenses
need richer models.

'--observed-pixel-uncertainty 1.0' says that the x,y corner coordinates reported
by mrgingham are distributed normally, independently, and with the standard
deviation as given in this argument. There's a tool to compute this value
empirically, but it needs more validation. For now pick a value that seems
reasonable. 1.0 pixels or less usually makes sense.

--explore says that after the models are computed, a REPL should be open so that
the user can look at various metrics describing the output; more on this
later.

After all the options, globs describing the images are passed in. Note that
these are GLOBS, not FILENAMES. So you need to quote or escape each glob to
prevent the shell from expanding it. You want one glob per camera; in the above
example we have 3 cameras. The program will look for all files matching the
globs, and filenames with identical matched strings are assumed to have been
gathered at the same instant in time. I.e. if in the above example we found
frame003-camera0.png and frame003-camera1.png, we will assume that these two
images were time-synchronized. If your capture system doesn't have
fully-functional frame syncronization, you should run a series of monocular
calibrations. Otherwise the models won't fit well (high reprojection errors
and/or high outlier counts) and you might see a frame with systematic
reprojection errors where one supposedly-synchronized camera's observation pulls
the solution in one direction, and another camera's observation pulls it in
another.

When you run the program as given above, the tool will spend a bit of time
computing (usually 10-20 seconds is enough, but this is highly dependent on the
specific problem, the amount of data, and the computational hardware). When
finished, it will write the resulting models to disk, and open a REPL (if
--explore was given). The resulting filenames are "camera-N.cameramodel" where N
is the index of the camera, starting at 0. The models contain the intrinsics and
extrinsics, with camera-0 sitting at the reference coordinate system.

When the solve is completed, you'll see a summary such as this one:

    RMS reprojection error: 0.3 pixels
    Worst reprojection error: 4.0 pixels
    Noutliers: 7 out of 9100 total points: 0.1% of the data

The reprojection errors should look reasonable given your
--observed-pixel-uncertainty. Since any outliers will be thrown out, the
reported reprojection errors will be reasonable.

Higher outlier counts are indicative of some/all of these:

- Errors in the input data, such as incorrectly-detected chessboard corners, or
  unsynchronized cameras

- Badly-fitting lens model

A lens model that doesn't fit isn't a problem in itself. The results will
simply not be reliable everywhere in the imager, as indicated by the uncertainty
and residual metrics (see below)

With --explore you get a REPL, and a message that points out some useful
functions. Generally you want to start with

    show_residuals_observation_worst(0)

This will show you the worst-fitting chessboard observation with its observed
and predicted corners, as an error vector. The reprojection errors are given by
a colored dot. Corners thrown out as outliers will be missing their colored dot.
You want to make sure that this is reasonable. Incorrectly-detected corners will
be visible: they will be outliers or they will have a high error. The errors
should be higher towards the edge of the imager, especially with a wider lens. A
richer better-fitting model would reduce those errors. Past that, there should
be no pattern to the errors. If the camera synchronization was broken, you'll
see a bias in the error vectors, to compensate for the motion of the chessboard.

Next do this for each camera in your calibration set (icam is an index counting
up from 0):

    show_residuals_regional(icam)

Each of these will pop up 3 plots describing your distribution of errors. You
get

- a plot showing the mean reprojection error across the imager
- a plot showing the standard deviation of reprojection errors across the imager
- a plot showing the number of data points across the imager AFTER the outlier
  rejection

The intrinsics are reliable in areas that have

- a low mean error relative to --observed-pixel-uncertainty
- a standard deviation roughly similar to --observed-pixel-uncertainty
- have some data available

If you have too little data, you will be overfitting, so you'd be expalining the
signal AND the noise, and your reprojection errors will be too low. With enough
input data you'll be explaining the signal only: the noise is random and with
enough samples our model can't explain it. Another factor that controls this is
the model we're fitting. If we fit a richer model (LENSMODEL_OPENCV8 vs
LENSMODEL_OPENCV4 for instance), the extra parameters will allow us to fit the
data better, and to produce lower errors in more areas of the imager.

These are very rough guidelines; I haven't written the logic to automatically
interpret these yet. A common feature that these plots bring to light is a
poorly-fitting model at the edges of the imager. In that case you'll see higher
errors with a wider distribution towards the edge.

Finally run this:

    show_projection_uncertainty()

This will pop up a plot of projection uncertainties for each camera. The
uncertainties are shown as a color-map along with contours. These are the
expected value of projection based on noise in input corner observations. The
noise is assumed to be independent, 0-mean gaussian with a standard deviation of
--observed-pixel-uncertainty. You will see low uncertainties in the center of
the imager (this is the default focus point; a different one can be picked). As
you move away from the center, you'll see higher errors. You should decide how
much error is acceptable, and determine the usable area of the imager based on
this. These uncertainty metrics are complementary to the residual metrics
described above. If you have too little data, the residuals will be low, but the
uncertainties will be very high. The more data you gather, the lower the
uncertainties. A richer lens model lowers the residuals, but raises the
uncertainties. So with a richer model you need to get more data to get to the
same acceptable uncertainty level.

** Capture images
 - Hold board straight
 - Oblique closeups
** mrgingham
*** mrcal
 - metrics
* Theory
** optimization, weighting, least squares
** research topics
- Is my spline representation good? Can I avoid it crossing itself?
- Note that regularization causes a bias
- Intrinsics uncertainty contains a built-in extrinsics uncertainty. As we move
  the cameras around, we carry with them an uncertain transformation
- Board warping
- outlier rejection. Cook's D
- rotation compensation for the diff
- compensating for board flex
- compensating for focal-length errors
  common-mode errors do not affect yaw. differential-mode errors affect yaw very
  much
- intrinsics errors effect on yaw. I ran some simulations earlier, I think.
  Similar effect: differential errors are very significant

** Model differencing
#+NAME: Model differencing
xzz

** Splined stereographic lens model
#+NAME: Splined stereographic lens model
yyy


* Projection uncertainty
#+NAME: Projection uncertainty
** Overview
After a calibration has been computed, it is important to get a sense of how
good the calibration is. Without doing that, we have no idea of the precision of
the data products that use the calibration. We have a vague sense that "more
chessboard observations are better", but that's about it.

mrcal addresses this by providing an estimate of projection uncertainty
explicitly via the [[file:mrcal-python-api.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]] function. This tells you how
good your calibration is (lower projection uncertainties are good), and it tells
you how good your downstream results are (by allowing the user to propagate the
projection uncertainties through their data pipeline).

A grand summary of the process:

1. Estimate the noise distribution on the chessboard observations input to the
   calibration routine
2. Propagate that uncertainty to the optimal parameters $\vec p$ reported by the
   calibration routine
3. Propagate the uncertainty in calibration parameters $\vec p$ through the
   projection function to get uncertainty in the resulting pixel coordinate $\vec
   q$

This overall approach is sound, but it implies some limitations:

- Only the response to chessboard observation noise is taken into account. Any
  other issues are /not/ included in the reported uncertainty. Issues such as:
  motion blur, out-of-focus images, out-of-synchronization images, unexpected
  chessboard shape. It is thus imperative that we try to minimize these issues,
  and mrcal provides [[How to run a calibration][tools]] to detect some of these problems.

- A consequence of the above is that the choice of lens model affects the
  reported uncertainties. Lean models (those with few parameters) are less
  flexible than rich models, and don't fit general lenses as well as rich models
  do. However, this stiffness also serves to limit the model's response to noise
  in their parameters. So the above method will report less uncertainty for
  leaner models than rich models. So, unless we're /sure/ that a given lens
  follows some particular lens model perfectly, a [[Splined stereographic lens model][splined lens model]] (i.e. a
  very rich model) is recommended for truthful uncertainty reporting. Otherwise
  the reported confidence comes from the model itself, rather than the
  calibration data.

- Currently the uncertainty estimates can be computed only from a vanilla
  calibration problem: a set of stationary cameras observing a moving
  calibration object. Other formulations can be used to compute the lens
  parameters as well (structure-from-motion while also computing the lens models
  for instance), but at this time the uncertainty computations cannot handle
  those cases.

** Input noise model
I solve the calibration problem using [[https://en.wikipedia.org/wiki/Ordinary_least_squares][Ordinary Least Squares]], minimizing the
discrepancies between pixel observations and their predictions. The pixel
observations are noisy, and I assume they are zero-mean, independent and
normally-distributed. Empirical evidence suggests that this is a reasonable
assumption. I treat the 2 values in each observation as two independent
measurements. Thus I minimize a cost function $E \equiv \left \Vert \vec x
\right \Vert ^2$ where $\vec x$ is the full measurement vector.

Most elements of the measurement vector $\vec x$ depend on the pixel
observations, but some don't (regularization: often needed to help convergence;
small-enough to not break anything). For the purposes of propagating noise in
the input observations, we only care about the former. For the $i$ -th observed
point the optimized parameters $\vec p$ predict an observation at a pixel
coordinate $\vec q_i$, while we actually did observe that same point at $\vec
{q_\mathrm{ref}}_i$. What is $\mathrm{Var}\left(\vec {q_\mathrm{ref}}_i\right)$?

The chessboard corner detection routine tells us how confident it was in
that observation, and we use that to weight the measurements. For convenience,
the implementation splits this into two parts:

- The baseline standard deviation of the noise $\sigma$ (referred to as the
  =observed_pixel_uncertainty=)
- The unitless scale applied to that baseline noise level

This is useful when using [[https://github.com/dkogan/mrgingham/][=mrgingham=]] to detect the chessboard corners. It
reports the resolution used in detecting each point, and from that we get the
weight $w_i$. A point detected at half-resolution has double the uncertainty at
full resolution, and I weigh it by a factor of 2 less in the optimization:
$\mathrm{Var}\left( \vec {q_\mathrm{ref}}_i \right) = \frac{\sigma^2}{w_i^2} I$
for all $i$. And $\vec x_i = w_i I (\vec{q_i} - \vec {q_\mathrm{ref}}_i)$ and
$\mathrm{Var}\left( \vec x_i\right) = \sigma^2 I$ for all $i$.

So the variance on each pixel observation is identical, and the optimal
parameter vector we compute from the least-squares optimization is the
maximum-likelihood estimate of the true solution.

The noise $\sigma$ is hard to measure (there's an [[https://github.com/dkogan/mrgingham/blob/master/mrgingham-observe-pixel-uncertainty][attempt]] in mrgingham), but
easy to loosely estimate. The current best practice is to get a conservative
eyeball estimate to produce conservative estimates of projection uncertainty.

** Propagating input noise to the state vector
We solved the least squares problem, so we have the optimal state vector $\vec p^*$. Let's find
the uncertainty at this point.

We apply a perturbation to the observations $\vec q_\mathrm{ref}$, reoptimize
this slightly-perturbed least-squares problem (assuming everything is linear)
and look what happens to the optimal state vector $\vec p$.

We have

\[ E \equiv \left \Vert \vec x \right \Vert ^2 \]
\[ J \equiv \frac{\partial \vec x}{\partial \vec p} \]
\[ \frac{\partial E}{\partial \vec p} \left(\vec p = \vec p^* \right) = 2 J^T \vec x^* = 0 \]

We perturb the problem:

\[ E( \vec p + \Delta \vec p, \vec q_\mathrm{ref} + \Delta \vec q_\mathrm{ref})) \approx \left \Vert \vec x + J \Delta \vec p + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \right \Vert ^2 \]

And we reoptimize:

\[ \frac{\mathrm{d}E}{\mathrm{d}\Delta \vec p} \approx 
2 \left( \vec x + J \Delta \vec p + \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \right)^T J = 0\]

we started at an optimum, so $J^T \vec x^* = 0$, and thus

\[ J^T J \Delta \vec p = -J^T \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} {\Delta \vec q_\mathrm{ref}} \]

As stated above, for reprojection errors we have

\[ \vec x_\mathrm{observations} = W (\vec q - \vec q_\mathrm{ref}) \]

where $W$ is a diagonal matrix of weights. Let's assume the non-observations
elements of $\vec x$ are at the end of $\vec x$, so

\[ \frac{\partial \vec x}{\partial \vec q_\mathrm{ref}} =
\left[ \begin{array}{cc} - W \\ 0 \end{array} \right] \]

and thus

\[ J^T J \Delta \vec p = -J_\mathrm{observations}^T W \Delta \vec q_\mathrm{ref} \]

So if we perturb the input observation vector $q_\mathrm{ref}$ by $\Delta
q_\mathrm{ref}$, the resulting effect on the optimal parameters is $\Delta \vec
p = M \Delta \vec q_\mathrm{ref}$. Where

\[ M = - \left( J^T J \right)^{-1} J_\mathrm{observations}^T W \]

So

\[ \mathrm{Var}(\vec p) = M \mathrm{Var}\left(\vec q_\mathrm{ref}\right) M^T \]

As stated before, we're assuming independent noise on all observed pixels, with
a standard deviation inversely proportional to the weight:

\[ \mathrm{Var}\left( \vec q_\mathrm{ref} \right) = \sigma^2 W^{-2} \]

so

\begin{eqnarray*}
\mathrm{Var}\left(\vec p\right) &=& \sigma^2 M W^{-2} M^T \\
&=& \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T W W^{-2} W J_\mathrm{observations} \left( J^T J \right)^{-1} \\
&=& \sigma^2 \left( J^T J \right)^{-1} J_\mathrm{observations}^T J_\mathrm{observations}  \left( J^T J \right)^{-1}
\end{eqnarray*}

If we have no regularization, and all measurements are pixel errors, then
$J_\mathrm{observations} = J$ and

\[\mathrm{Var}\left(\vec p\right) = \sigma^2 \left( J^T J \right)^{-1} \]

Note that this does not explicitly depend on $W$. However, the weights are a
part of $J$. So if observation $i$ were to become less precise,
$\mathrm{Var}\left(\vec {q_\mathrm{ref}}_i \right)$ would increase, which means
that $w_i$ and $x_i$ and $J_i$ would all decrease. And as a result,
$\mathrm{Var}\left(\vec p\right)$ would increase, as expected.

** Propagating the state vector noise through projection
I now have the variance of the full optimization state $\vec p$. This contains
the intrinsics and extrinsics of /all/ the cameras. And it contains /all/ the
poses of observed chessboards, and everything else, like the chessboard warp
terms.

How are those parameters used during the optimization? The fundamental operation
is projecting points in a "frame" coordinate system (the coordinate system of a
chessboard). Projecting a point $p_\mathrm{chessboard}$ involves several
transformations and then a projection:

\[ \vec q                     \xleftarrow{\mathrm{intrinsics}}
   \vec p_\mathrm{camera}     \xleftarrow{T_\mathrm{cr}}
   \vec p_\mathrm{reference}  \xleftarrow{T_\mathrm{rf}}
   \vec p_\mathrm{frame}
\]

Here the $\mathrm{intrinsics}$ are the lens parameters, $T_\mathrm{cr}$ is the
extrinsics transformation, and $T_\mathrm{rf}$ is the "frame" transformation.
Each is an element of the state vector $\vec p$ whose uncertainty we have.

So how can we estimate $\mathrm{Var}\left( \vec q \right)$? The simplest thing
to do is to focus just on the projection operation:

\[\vec q = \mathrm{project}\left(\vec p_\mathrm{camera}, \mathrm{intrinsics}\right)\]

We can use this expression to propagate the intrinsics uncertainties, but this
is insufficient. We want to know the projection uncertainty of points in a
/fixed/ coordinate system, a coordinate system that doesn't move due to random
shifts in the state $\vec p$. As we can see above, $\vec p_\mathrm{camera}$
depends on the extrinsics, which are a part of the state.

But what if we only have one camera, and thus we have no extrinsics (the camera
coordinate system /is/ the reference coordinate system)? This doesn't work
either. The lens intrinsics encode an implied transformation that moves the
camera coordinate system, so once again $\vec p_\mathrm{camera}$ would move in
response to our perturbation.

So how do we operate on points in a fixed coordinate system when all the
coordinate systems we have are floating random variables? We can use the poses
of the observed chessboards in aggregate: these are the most fixed thing we
have.

Let's focus on /one/ observed chessboard frame: frame 0. I want to know the
uncertainty at a pixel coordinate $\vec q$. I follow the sequence above in
reverse:

\[ \vec p_{\mathrm{frame}_0} = T_{\mathrm{f}_0\mathrm{r}}} T_\mathrm{rc} \mathrm{unproject}\left( \vec q \right) \]

This is a "fixed" point. I then transform and project $\vec p_{\mathrm{frame}_0}}$
back to the imager to get $\vec q^+$. But here I take into account the
uncertainties of each transformation to get the desired projection uncertainty
$\mathrm{Var}\left(\vec q^+ - \vec q\right)$. The full data flow looks like
this, with all the perturbed quantities superscripted with a $+$.

\[
   \vec q^+                      \xleftarrow{\mathrm{intrinsics}^+}
   \vec p^+_\mathrm{camera}      \xleftarrow{T^+_\mathrm{cr}}
   \vec p^+_{\mathrm{reference}_0}}  \xleftarrow{T^+_{\mathrm{rf}_0}}} \vec p_{\mathrm{frame}_0}} \xleftarrow{T_\mathrm{fr}}
   \vec p_\mathrm{reference}
   \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
   \xleftarrow{\mathrm{intrinsics}}
   \vec q
\]

This works, but it depends on $\vec p_{\mathrm{frame}_0}}$ being "fixed", which it
isn't, since $T_\mathrm{f0r}$ is in the optimization state /and/ since the
reference coordinate system that $T_\mathrm{f0r}$ relates to isn't fixed either.
However, we're observing more than one chessboard, and /together/ all the
chessboard frames can represent a mostly-fixed reference.

How do I combine all the different estimates from the different chessboard
observations? I take a very simple approach: I compute the mean of all the $\vec
p^+_\mathrm{reference}$ estimates from each frame. The full data flow looks like
this:

\begin{aligned}
   & \swarrow                   & \vec p^+_{\mathrm{reference}_0}}  & \xleftarrow{T^+_{\mathrm{rf}_0}}} & \vec p_{\mathrm{frame}_0}} & \nwarrow & \\
   \vec q^+                      \xleftarrow{\mathrm{intrinsics}^+}
   \vec p^+_\mathrm{camera}      \xleftarrow{T^+_\mathrm{cr}}
   \vec p^+_\mathrm{reference}
   & \xleftarrow{\mathrm{mean}} & \vec p^+_{\mathrm{reference}_1}}  & \xleftarrow{T^+_{\mathrm{rf}_1}}} & \vec p_{\mathrm{frame}_1}} & \xleftarrow{T_\mathrm{fr}} &
   \vec p_\mathrm{reference}
   \xleftarrow{T_\mathrm{rc}}   \vec p_\mathrm{camera}
   \xleftarrow{\mathrm{intrinsics}}
   \vec q \\
   & \nwarrow                   & \vec p^+_{\mathrm{reference}_2}}  & \xleftarrow{T^+_{\mathrm{rf}_2}}} & \vec p_{\mathrm{frame}_2}} & \swarrow
\end{aligned}

This is better, but has another issue. What is the transformation relating the
original and perturbed reference coordinate systems?

\[ T_{\mathrm{r}^+\mathrm{r}} = \mathrm{mean}_i \left( T_{\mathrm{r}^+\mathrm{f}_i} T_{\mathrm{f}_i\mathrm{r}} \right) \]

Each transformation $T$ includes a rotation matrix $R$, so the above constructs
a new rotation as a mean of multiple rotation matrices, which is aphysical: the
resulting matrix is not a valid rotation. In practice, the perturbations are
tiny, and this is sufficiently close. Extreme geometries do break this, and I
will tweak this approach in the future.

** Implementation details

I computed Var(p) earlier, which contains the variance of ALL the optimization
parameters together. The noise on the chessboard poses is coupled to the noise
on the extrinsics and to the noise on the intrinsics. And we can apply all these
together to propagate the uncertainty.

Let's define some variables:

- p_i: the intrinsics of a camera
- p_e: the extrinsics of that camera (T_cr)
- p_f: ALL the chessboard poses (T_fr)
- p_ief: the concatenation of p_i, p_e and p_f

I have

    dq = q0 + dq/dp_ief dp_ief

    Var(q) = dq/dp_ief Var(p_ief) (dq/dp_ief)t

    Var(p_ief) is a subset of Var(p), computed above.

    dq/dp_ief = [dq/dp_i dq/dp_e dq/dp_f]

    dq/dp_e = dq/d\vec p_\mathrm{camera} d\vec p_\mathrm{camera}/dp_e

    dq/dp_f = dq/d\vec p_\mathrm{camera} d\vec p_\mathrm{camera}/dpref dpref/dp_f / Nframes

dq/dp_i and all the constituent expressions comes directly from the project()
and transform calls above. Depending on the details of the optimization problem,
some of these may not exist. For instance, if we're looking at a camera that is
sitting at the reference coordinate system, then there is no p_e, and Var_ief is
smaller: it's just Var_if. If we somehow know the poses of the frames, then
there's no Var_f. If we want to know the uncertainty at distance=infinity, then
we ignore all the translation components of p_e and p_f.

And note that this all assumes a vanilla calibration setup: we're calibration a
number of stationary cameras by observing a moving object. If we're instead
moving the cameras, then there're multiple extrinsics vectors for each set of
intrinsics, and it's not clear what projection uncertainty even means.

Note a surprising consequence of all this: projecting k*\vec p_\mathrm{camera} in camera
coordinates always maps to the same pixel coordinate q for any non-zero scalar
k. However, the uncertainty DOES depend on k. If a calibration was computed with
lots of chessboard observations at some distance from the camera, then the
uncertainty of projections at THAT distance will be much lower than the
uncertanties of projections at any other distance. And as we get closer and
closer to the camera, the uncertainty grows to infinity as the translation
uncertainty in the extrinsics begins to dominate.

Alright, so we have Var(q). We could claim victory at that point. But it'd be
nice to convert Var(q) into a single number that describes my projection
uncertainty at q. Empirically I see that Var(dq) often describes an eccentric
ellipse, so I want to look at the length of the major axis of the 1-sigma
ellipse:

    eig (a b) --> (a-l)*(c-l)-b^2 = 0 --> l^2 - (a+c) l + ac-b^2 = 0
        (b c)

    --> l = (a+c +- sqrt( a^2+2ac+c^2 - 4ac + 4b^2)) / 2 =
          = (a+c +- sqrt( a^2-2ac+c^2 + 4b^2)) / 2 =
          = (a+c)/2 +- sqrt( (a-c)^2/4 + b^2)

So the worst-case stdev(q) is

$\sqrt{\frac{a+c}{2} + \sqrt{ \frac{\left(a-c\right)^2}{4} + b^2}}$

* Visualization
- say that the plots are interactive in normal usage
* Other
** interesting stereo discoveries
- compensating for board flex
- compensating for focal-length errors
  common-mode errors do not affect yaw. differential-mode errors affect yaw very
  much
- intrinsics errors effect on yaw. I ran some simulations earlier, I think.
  Similar effect: differential errors are very significant
** things to mention in the talk and in the docs
- talk about regularization bias
- splined models shouldn't fit the core to keep things non-singular
- splined models may not be fitted into opencv8 without moving extrinsics
- say that poor uncertainty = overfitting
- say that we need to track down the source of all errors. The model we're
  optimizing should not produce any error on its own. And it shouldn't produce
  any constraints on its own. The "model" includes the lens model and the
  warping here. Thus the uncertainties are only directly usable with the splined
  models
- talk about how I'm projecting the "same world point", and how there're other
  (possibly-better) methods
- talk about how to get observed_pixel_uncertainty
- talk about how to select an appropriate splined model
- talk about --seed and how that can be used to validate intrinsics

* After-release todo
- feed uncertainties to stereo, triangulation
- compute uncertainties for multiple points at the same time to get covariance.
  Possibly could work across multiple cameras in the same solve as well
- better regularization non/crossing in splined models
- should include a study of how to calibrate long lenses. Tilted observations
  aren't as effective unless the board is GIANT
- Can we study intrinsics stability over time? In response to heating? Shaking?
- Can we use a 3-parallel calibration to quantify chromatic aberration?

* future work

- measure observed_pixel_uncertainty
- use uncertainty in triangulation, deltapose
- improve uncertainty method: faraway obervations don't make things worse
- projection_uncertainty() should be able to project multiple points at a time,
  and to report correlations in the projection
- splined models should behave more nicely at the edges
- sfm
- integrate deltapose-lite
- projection_uncertainty() should report correlated results
- can I quantify the heteroscedasticity and thus the model-nonfitting and the
  resulted expected bias? White test?
- study cubic/quadratic splines, spline density effects
- do a triangulation with explict uncertainty propagation

- Redo, show stability. Heat? Show effects?
- uncertainty questions:
  - study the effects of the spline control points density
  - are quadratic splines better? more sparse, but only c1 instead of c2
  - Can I use the heteroschedasticity metrics to say stuff about the lean
    models?

- mention sfm
- feed uncertainties to stereo, triangulation
- compute uncertainties for multiple points at the same time to get covariance.
  Possibly could work across multiple cameras in the same solve as well
- better regularization non/crossing in splined models
- should include a study of how to calibrate long lenses. Tilted observations
  aren't as effective unless the board is GIANT
- Can we study intrinsics stability over time? In response to heating? Shaking?
- Can we use a 3-parallel calibration to quantify chromatic aberration?
- Measure effect of focus, aperture

* todo for the document
The "commandline tools" link on top is generated wrong

code should no longer refer to the projection_uncertainty() docstring, but
rather refer here

should say what this toolkit isfor, other than calibration
