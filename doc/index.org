#+title: mrcal - a toolkit for manipulating cameras, projections and geometry
#+author: Dima Kogan
#+email: dima@secretsauce.net
#+language: en

* Intro
mrcal is a toolkit for working with lens models, camera geometry, images,
projections, and the various related operations such as camera calibration. This
toolkit was originally built to produce high-accuracy calibrations demanded by
long-range stereo, so it provides facilities to analyze the results and to track
down sources of error.

mrcal provides a routine to compute the "[[#Model differencing][difference]]" between two models, which
can be a fundamental piece of a wide number of analyses, for instance to measure
a lens's response to temperature cycles.

mrcal provides estimates of [[file:uncertainty.org][projection uncertainty]], which can be used to gauge
calibration quality, and to compute the uncertainty of any data products that
use the lens model.

A rich, [[#Splined stereographic lens model][splined lens model]] is available to fit any projection function and to
provide realistic uncertainty estimates.

The core functionality is exposed from the [[file:c-api.org][C API]], while higher-level routines
are available through [[#Python API][Python]]. The most common workflows are available as
[[#Commandline tools][commandline tools]], with no coding required.

Please see [[file:tour.org][a tour of mrcal]] for a high-level overview of the capabilities of the
toolkit.

* Conventions
** Terminology
Some terms in the documentation and sources can have ambiguous meanings, so I
explicitly define them here

- *calibration*: the procedure used to compute the lens parameters and geometry
  of a set of cameras. Usually this involves a stationary set of cameras
  observing a moving object.

- *calibration object* or *chessboard* or *board*: these are used more or less
  interchangeably. They refer to the known-geometry object observed by the
  cameras, with those observations used as input during calibration

- *pose*: a position and orientation

- *intrinsics*: the parameters describing the behavior of a lens. The pose of
  the lens does not affect the intrinsics

- *extrinsics*: the pose of a lens in respect to some fixed coordinate system

- *frames*: in the context of mrcal's optimization these refer to an array of
  poses of the observed chessboards

- *SFM*: structure from motion. This is the converse of "calibration": we
  observe a stationary scene from a moving camera to compute the geometry of the
  scene

- *state*: the vector of parameters that an optimization algorithm is free to
  move around as it searches for the optimum. mrcal generally refers to this
  vector as $\vec p$

- *measurements* or *residuals*: I use these more or less interchangeably. This
  is the vector whose norm the optimization algorithm is trying to minimize.
  mrcal generally refers to this as $\vec x$, and it contains differences
  between pixel coordinates observed by a camera, and where the state vector
  $\vec p$ predicts those observations should be. The optimization tries to
  minimize these differences by finding the $\vec p$ that minimizes $\left \Vert
  \vec x \right \Vert ^2$

- *project*: to map a point in space to a pixel coordinate where that point
  would be observed by a given camera

- *unproject*: to map a pixel coordinate back to a point in space that would
  produce an observation at that pixel. Unprojection is only unique up-to scale

- *camera model*: this is used to refer to the intrinsics and extrinsics
  together.

** Symbols
*** Geometry
- $\vec q$ is a 2-dimensional vector representing a pixel coordinate: $\left( x,y \right)$

- $\vec v$ is a 3-dimensional vector representing a /direction/ $\left( x,y,z
  \right)$ in space. $\vec v$ is unique only up-to-length. In a camera's
  coordinate system we have $\vec q = \mathrm{project}\left(\vec v \right)$

- $\vec p$ is a 3-dimensional vector representing a /point/ $\left( x,y,z
  \right)$ in space. Unlike $\vec v$, $\vec p$ has a defined range. Like $\vec
  v$ we have $\vec q = \mathrm{project}\left(\vec p \right)$

*** Optimization
The core of the mrcal calibration routine is a nonlinear least-squares
optimization

\[
\min_{\vec p} E = \min_{\vec p} \left \Vert \vec x \left( \vec p \right) \right \Vert ^2
\]

Here we have

- $\vec p$ is the vector of parameters being optimized. It's clear from context
  whether $\vec p$ refers to some point in space, or the optimization vector.

- $\vec x$ is the vector of /measurements/ describing the error of the solution
  at some hypothesis $\vec p$

- $\vec E$ is the cost function being optimized. $E \equiv \left \Vert \vec x \right \Vert ^2$

- $\vec J$ is the /jacobian/ matrix. This is the matrix $\frac{ \partial \vec x
  }{ \partial \vec p }$. Usually this is large and sparse.

** Camera coordinate system
mrcal uses right-handed coordinate systems. No convention is assumed for the
world coordinate system. The canonical /camera/ coordinate system has $x$ and
$y$ as with pixel coordinates in an image: $x$ is to the right and $y$ is down.
$z$ is then forward to complete the right-handed system of coordinates.

** Transformations
We describe transformations as mappings between a representation of a point in
one coordinate system to a representation of the /same/ point in another
coordinate system. =T_AB= is a transformation from coordinate system =B= to
coordinate system =A=. These chain together nicely, so if we know the
transformation between =A= and =B= and between =B= and =C=, we can transform a
point represented in =C= to =A=: =x_A = T_AB T_BC x_C = T_AC x_C=. And =T_AC =
T_AB T_BC=.

** Poses

Various parts of the toolkit have preferred representations of pose, and mrcal
has functions to convert between them. Available representations are:

- =Rt=: a (4,3) numpy array with a (3,3) rotation matrix concatenated with a
  (1,3) translation vector. This form is easy to work with, but there are
  implied constraints: most (4,3) numpy arrays are /not/ valid =Rt=
  transformations.

- =rt=: a (6,) numpy array with a (3,) vector representing a [[https://en.wikipedia.org/wiki/Axis%E2%80%93angle_representation#Rotation_vector][Rodrigues rotation]]
  concatenated with another (3,) vector, representing a rotation. This form
  requires more computations to deal with, but has no implied constraints: /any/
  (6,) numpy array is a valid =rt= transformation. Thus this is the form used
  inside the mrcal optimization routine.

Each of these represents a transformation =rotate(x) + t=.

Since a pose represents a transformation between two coordinate systems, the
toolkit generally refers to a pose as something like =Rt_AB=, which is an
=Rt=-represented transformation to convert a point from a representation in the
coordinate system =B= to a representation in coordinate system =A=.

A Rodrigues rotation vector =r= represents a rotation of =length(r)= radians
around an axis in the direction =r=. Converting between =R= and =r= is done via
the [[https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula][Rodrigues rotation formula]]: using the [[file:mrcal-python-api-reference.html#-r_from_R][=mrcal.r_from_R()=]] and
[[file:mrcal-python-api-reference.html#-R_from_r][=mrcal.R_from_r()=]] functions. For translating /poses/, not just rotations, use
[[file:mrcal-python-api-reference.html#-rt_from_Rt][=mrcal.rt_from_Rt()=]] and [[file:mrcal-python-api-reference.html#-Rt_from_rt][=mrcal.Rt_from_rt()=]].

There're [[file:mrcal-python-api-reference.html#-R_from_quat][several]] [[file:mrcal-python-api-reference.html#-quat_from_R][functions]] to work with unit quaternions as a rotation
representation, but they're lightly used, and exist only for compatibility with
other tools. mrcal does not use quaternions.

** Linear algebra
mrcal follows the usual linear algebra convention of column vectors. So applying
a rotation looks like $\vec b = R \vec a$ where both $\vec a$ and $\vec b$ are
column vectors.

However, numpy print vectors (1-dimensional objects), as /row/ vectors, so the
code treats 1-dimensional objects as transposed vectors. In the code, the above
rotation would be implemented equivalently: $\vec b^T = \vec a^T R^T$. The
[[file:mrcal-python-api-reference.html#-rotate_point_R][=mrcal.rotate_point_R()=]] and [[file:mrcal-python-api-reference.html#-transform_point_Rt][=mrcal.transform_point_Rt()=]] functions serve to
handle this transparently.

A similar issue is that numpy follows the linear algebra convention of indexing
with =(index_column, index_row)= and not the other way around. This runs against
the /other/ convention of referring to image dimensions as =(width, height)= and
referring to pixels as =(x,y)=. mrcal places the =x= coordinate first (as in the
latter) whenever possible, but when interacting directly with numpy, it must
place the =y= coordinate first. The choice being made is very clearly
documented, so when in doubt, do read the docs.

When computing gradients mrcal places the dependent variables in the leading
dimensions, and the independent variables in the trailing dimensions. So in the
above expressions we have $\frac{ \partial \vec b }{ \partial \vec a } = R$ and
row $i$ of $R$ represents the $\frac{ \partial b_i }{ \partial \vec a }$

** Implementation
The core of mrcal is written in C, but most of the API is currently available in
Python only. The python-wrapping is done via the [[https://github.com/dkogan/numpysane/blob/master/README-pywrap.org][=numpysane_pywrap=]] library,
which makes it fairly simple to make the Python interface /and/ provides
[[https://numpy.org/doc/stable/user/basics.broadcasting.html][broadcasting]] support.

The Python layer uses [[https://numpy.org/][numpy]] and [[https://github.com/dkogan/numpysane/][=numpysane=]] heavily. All the plotting is done
with [[https://github.com/dkogan/gnuplotlib][=gnuplotlib=]]. [[https://opencv.org/][OpenCV]] is used a bit, but /only/ in the Python layer (their C
APIs are gone, and the C++ APIs are unstable). Over time the dependence on this
library will decrease even further.

* Camera model file formats

Reading/writing camera models is done in Python with the [[file:mrcal-python-api-reference.html#cameramodel][=mrcal.cameramodel=]]
class. This class supports two different file formats:

- =.cameramodel=: the preferred format. This is a plain text representation of a
  Python =dict=. The pose is represented internally as =rt_fromref=: an =rt=
  transformation /from/ the reference coordinate system /to/ the coordinate
  system of this camera. That is the /internal/ representation: the class
  provides methods to get the transformation in any form.

- =.cahvor=: the alternative format available for compatibility with existing
  tools. If you don't need to interoperate with tools that require this format,
  there's little reason to use it. This format cannot store [[#Splined stereographic lens model][splined models]] or
  the auxillary data required for the [[file:uncertainty.org][uncertainty computations]].

The [[file:mrcal-python-api-reference.html#cameramodel][=mrcal.cameramodel=]] class will intelligently pick the correct file format
based on the filename. The file format is just a way to store data: both the
CAHVOR and OpenCV lens models can be stored in either file format. The
[[file:mrcal-to-cahvor.html][=mrcal-to-cahvor=]] and [[file:mrcal-to-cameramodel.html][=mrcal-to-cameramodel=]] tools can be used to convert
between the two file formats.

The class (and its representation on disk) contains:

- The lens parameters
- The pose of the camera in space
- The =optimization_inputs=: the data used to compute the model initially. Used
  for the uncertainty computations

See the [[file:mrcal-python-api-reference.html#cameramodel][API documentation]] for usage details. A trivial example to

- read two models from disk
- recombine into a joint model that uses the lens parameters from one model with
  geometry from the other
- write to disk

#+begin_src python
model_for_intrinsics = mrcal.cameramodel('model0.cameramodel')
model_for_extrinsics = mrcal.cameramodel('model1.cameramodel')

model_joint = mrcal.cameramodel( model_for_intrinsics )

extrinsics = model_for_extrinsics.extrinsics_rt_fromref()
model_joint.extrinsics_rt_fromref(extrinsics)

model_joint.write('model-joint.cameramodel')
#+end_src

This is the basic operation of the [[file:mrcal-graft-models.html][=mrcal-graft-models= tool]].

Currently there's no support for reading/writing these files in the [[file:c-api.org][C API]]. I
will write it when I need it or when somebody bugs me about it, whichever comes
first.

* Lens models
:PROPERTIES:
:CUSTOM_ID: Lens models
:END:

mrcal supports a wide range of lens models in both C and in Python. The
representation details and projection behaviors are described here.

** Representation
In Python the models are identified with a string =LENSMODEL_XXX= where the
=XXX= selects the specific model in question. For most models, this
specification describes the model fully. For some models, however, the =XXX=
selects a model /family/, and some /configuration parameters/ are required to
define the specific model in question. An example string with a configuration:
=LENSMODEL_SPLINED_STEREOGRAPHIC_order=3_Nx=30_Ny=20_fov_x_deg=170=. The
configuration parameters (=order=3=, =Nx=30= and so on) select the model, and
are /not/ subject to optimization. Currently only the [[#Splined stereographic lens model][splined stereographic
models]] have any configuration, but more will be added over time.

In C, the model family is selected with the [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h#mrcal_lensmodel_type_t][=mrcal_lensmodel_type_t=]] enum. The
elements are the same as the Python model names, but with =MRCAL_= prepended. So
in C the above splined model is of type =MRCAL_LENSMODEL_SPLINED_STEREOGRAPHIC=.
In C the type /and/ configuration are represented by the [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h##mrcal_lensmodel_t][=mrcal_lensmodel_t=]]
structure. Most routines require the configuration to be available. For
instance, the number of parameters needed to fully describe a given model can be
obtained by calling [[file:mrcal-python-api-reference.html#-lensmodel_num_params][=mrcal.lensmodel_num_params()=]] in Python or
[[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h#mrcal_lensmodel_num_params][=mrcal_lensmodel_num_params()=]] in C (requires the full [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h##mrcal_lensmodel_t][=mrcal_lensmodel_t=]]).

** Intrinsics core
Most models contain an "intrinsics core". These are 4 values that appear at the
start of the parameter list:

- $f_x$: the focal-length in the horizontal direction, in pixels
- $f_y$: the focal-length in the vertical direction, in pixels
- $c_x$: the horizontal projection center, in pixels
- $c_y$: the vertical projection center, in pixels

At this time all models contain a core, but this will change in the future.

** Models
*** =LENSMODEL_PINHOLE=
This is the basic "pinhole" model with 4 parameters (the core only). Projection
of a point $\vec p$ is defined as

\[\vec q = \left( \begin{array}{c} f_x \frac{p_x}{p_z} + c_x \\ f_y \frac{p_y}{p_z} + c_y \end{array} \right) \]

This model is defined only in front of the camera, and projects to infinity as
we approach 90 degrees off the projection axis ($p_z \rightarrow 0$). Straight
lines in space remain straight under this projection, and observations of the
same plane by two pinhole cameras define a homography. No real-world lens
follows this projection, so this exists for data processing only.

*** =LENSMODEL_STEREOGRAPHIC=
Another trivial model that exists for data processing, and not to represent real
lenses. Like the pinhole model, this has just the 4 core parameters.

To define the projection of a point $\vec p$, let's define the angle off the
projection axis:

\[ \theta \equiv \tan^{-1} \frac{\left| \vec p_{xy} \right|}{p_z} \]

then

\[ \vec u \equiv \frac{\vec p_{xy}}{\left| \vec p_{xy} \right|} 2 \tan\frac{\theta}{2} \]

and

\[\vec q = \left( \begin{array}{c} f_x u_x + c_x \\ f_y u_y + c_y \end{array} \right) \]

This model is able to project behind the camera, and has a single singularity:
directly opposite the projection axis.

Note that the pinhole model can be defined in the same way, except the pinhole
model has $\vec u \equiv \frac{\vec p_{xy}} {\left| \vec p_{xy} \right|} \tan
\theta$. And we can thus see that for long lenses (where $\theta$ is small) the
pinhole model and the stereographic model function similarly: $\tan \theta
\approx 2 \tan \frac{\theta}{2}$

*** =LENSMODEL_OPENCV4=, =LENSMODEL_OPENCV5=, =LENSMODEL_OPENCV8=, =LENSMODEL_OPENCV12=
These are simple parametric models that have the given number of "distortion"
parameters in addition to the 4 core parameters. The projection behavior is
described in the [[https://docs.opencv.org/4.5.0/d9/d0c/group__calib3d.html#details][OpenCV documentation]]. These do a reasonable (up to a point) job
in representing real-world lenses, /and/ they're compatible with many other
tools.

*** =LENSMODEL_CAHVOR=
mrcal supports =LENSMODEL_CAHVOR=, a lens model used in a number of tools used
at JPL. Primarily this support exists for compatibility. The CAHVOR model has 5
"distortion" parameters in addition to the 4 core parameters. If you want to use
this, you already know what it does.

*** =LENSMODEL_CAHVORE=
This is an extended =LENSMODEL_CAHVOR= to support wider lenses. CAHVORE is only
partially supported: the parameter gradients aren't implemented, so it isn't
currently possible to solve for a CAHVORE model. Full support may be added in
the future.

*** =LENSMODEL_SPLINED_STEREOGRAPHIC_...=
:PROPERTIES:
:CUSTOM_ID: Splined stereographic lens model
:END:

This projection function is a stereographic model with correction factors. We
compute $\vec u$ as in the [[*=LENSMODEL_STEREOGRAPHIC=][=LENSMODEL_STEREOGRAPHIC=]] projection definition
above. We then use $\vec u$ to look-up a $\Delta \vec u$ using two splined
surfaces: one for each of the two elements of

\[ \Delta \vec u \equiv
\begin{bmatrix}
\Delta u_x \left( \vec u \right) \\
\Delta u_y \left( \vec u \right)
\end{bmatrix} \]

Then we can define the rest of the projection function:

\[\vec q =
 \left( \begin{array}{c}
 f_x \left( u_x + \Delta u_x \right) + c_x \\
 f_y \left( u_y + \Delta u_y \right) + c_y
\end{array} \right) \]

The surfaces $\Delta u_x$ and $\Delta u_y$ are defined as B-splines with
evenly-spaced knots (control points) in the space of the domain $\vec u$. The
values of the knots comprise the parameters of this lens model. We're using
B-splines primarily for their local support properties: moving a knot only
affect the surface in the immediate region surrounding that knot. This makes for
a nice optimization problem.

Everything else needed to define the splined surfaces is given in the model
configuration:

- =order=: the degree of each 1D polynomial. This is either 2 (quadratic
  splines, C1 continuous) or 3 (cubic splines, C2 continuous)

- =Nx= and =Ny=: The spline density. We have a =Nx= by =Ny= grid of
  evenly-spaced control points. More control points results in more parameters
  and a more flexible model. Naturally this also increases the amount of
  calibration data we need to achieve the same projection uncertainty. The ratio
  of =Nx= to =Ny= should usually follow the ratio of the two imager dimensions,
  but this isn't required

- =fov_x_deg=: The horizontal field of view, in degrees. Controls the area where
  the spline is defined. Beyond this area the projection function will use the
  nearest spline patch. This will produce continuous, but very aphysical
  projection behavior. The user should choose a value of =fov_x_deg=
  large-enough to cover the region viewable by the given lens. The
  [[file:mrcal-show-splined-model-surface.html][=mrcal-show-splined-model-surface= tool]] can be used to verify that all the
  visible pixels lie within the area indexable by the spline. =fov_y_deg= is not
  included in the configuration: it is assumed proportional with =Ny= and =Nx=.

This splined model is far more flexible than the lean parametric models (all the
other lens models). It is thus the recommended choice to get the best fidelity
and [[file:uncertainty.org][uncertainty estimates]].

No data-driven method of choosing =Nx= or =Ny= is available at this time.
Something will be built eventually.

* Commandline tools
:PROPERTIES:
:CUSTOM_ID: Commandline tools
:END:

A number of commandline tools are available for common tasks. If you're just
using mrcal to calibrate some cameras, you may not need anything else.

- [[file:mrcal-calibrate-cameras.html][=mrcal-calibrate-cameras=]]: calibrate N cameras. This is the main tool to solve
  "calibration" problems.
- [[file:mrcal-show-projection-diff.html][=mrcal-show-projection-diff=]]: visualize the projection difference between a
  number of models
- [[file:mrcal-show-projection-uncertainty.html][=mrcal-show-projection-uncertainty=]]: visualize the projection uncertainty of a
  model
- [[file:mrcal-show-valid-intrinsics-region.html][=mrcal-show-valid-intrinsics-region=]]: Visualizes the region where a model's
  intrinsics are valid
- [[file:mrcal-is-within-valid-intrinsics-region.html][=mrcal-is-within-valid-intrinsics-region=]]: Augments a vnlog of pixel
  coordinates with a column indicating whether or not each point lies within
  the valid-intrinsics region
- [[file:mrcal-convert-lensmodel.html][=mrcal-convert-lensmodel=]]: Fits the behavior of one lens model to another
- [[file:mrcal-show-geometry.html][=mrcal-show-geometry=]]: Shows a visual representation of the geometry
  represented by some camera models on disk, and optionally, the
  chessboard observations used to compute that geometry
- [[file:mrcal-show-distortion-off-pinhole.html][=mrcal-show-distortion-off-pinhole=]]: visualize the deviation of a specific
  lens model from a pinhole model
- [[file:mrcal-show-splined-model-surface.html][=mrcal-show-splined-model-surface=]]: visualize the surface and knots used in
  the specification of splined models
- [[file:mrcal-reproject-image.html][=mrcal-reproject-image=]]: Given image(s) and lens model(s), produces a new set
  of images that observe the same scene with a different model. Several flavors
  of functionality are included here, such as undistortion-to-pinhole,
  re-rotation, and remapping to infinity.
- [[file:mrcal-reproject-points.html][=mrcal-reproject-points=]]: Given two lens models and a set of pixel coodinates,
  maps them from one lens model to the other
- [[file:mrcal-graft-models.html][=mrcal-graft-models=]]: Combines the intrinsics of one cameramodel with the
  extrinsics of another
- [[file:mrcal-to-cahvor.html][=mrcal-to-cahvor=]]: Converts a model stored in the native =.cameramodel= file
  format to the =.cahvor= format. This exists for compatibility only, and does
  not touch the data: any lens model may be used
- [[file:mrcal-to-cameramodel.html][=mrcal-to-cameramodel=]]: Converts a model stored in the =.cahvor= file format
  to the =.cameramodel= format. This exists for compatibility only, and does not
  touch the data: any lens model may be used
- [[file:mrcal-cull-corners.html][=mrcal-cull-corners=]]: Filters a corners.vnl on stdin to cut out some points

* Developer manual (APIs)
The mrcal toolkit has APIs in both C and Python. Everything that could
potentially be slow is written in C, but the higher-level logic is mostly in
Python. The Python-wrapping is done via the [[https://github.com/dkogan/numpysane/blob/master/README-pywrap.org][=numpysane_pywrap=]] library, which
makes it fairly simple to build the Python interfaces in a standard way, so over
time Python-only functionality will be translated to C, as needed (with
backwards-compatible Python wrappers replacing the Python implementations).

** Python API
:PROPERTIES:
:CUSTOM_ID: Python API
:END:

All the Python functions have complete docstrings, so the =pydoc3= tool is
effective at displaying the relevant documentation. For convenience, all the
docstrings have been extracted and formatted into the [[file:mrcal-python-api-reference.html][Python API reference]].

The available functions, by category:

*** Camera model reading/writing
The [[file:mrcal-python-api-reference.html#cameramodel][=mrcal.cameramodel=]] class provides functionality to read/write models
from/to files on disk. Both the =.cameramodel= and =.cahvor= file formats are
supported, choosing the proper one, depending on the given filename. When
reading a pipe (no filename known), both formats are tried. If writing to a
pipe, the =.cameramodel= format is chosen, unless =.cahvor= is requested via the
arguments. The available methods:

- [[file:mrcal-python-api-reference.html#cameramodel-__init__][=mrcal.cameramodel.__init__()=]]: Read a model from a file on disk, or construct
  from the data given in the arguments.
- [[file:mrcal-python-api-reference.html#cameramodel-write][=mrcal.cameramodel.write()=]]: Write out this camera-model to a file
- [[file:mrcal-python-api-reference.html#cameramodel-intrinsics][=mrcal.cameramodel.intrinsics()=]]: Get or set the intrinsics in this model
- [[file:mrcal-python-api-reference.html#cameramodel-extrinsics_rt_toref][=mrcal.cameramodel.extrinsics_rt_toref()=]]: Get or set the extrinsics in this model
- [[file:mrcal-python-api-reference.html#cameramodel-extrinsics_rt_fromref][=mrcal.cameramodel.extrinsics_rt_fromref()=]]: Get or set the extrinsics in this model
- [[file:mrcal-python-api-reference.html#cameramodel-extrinsics_Rt_toref][=mrcal.cameramodel.extrinsics_Rt_toref()=]]: Get or set the extrinsics in this model
- [[file:mrcal-python-api-reference.html#cameramodel-extrinsics_Rt_fromref][=mrcal.cameramodel.extrinsics_Rt_fromref()=]]: Get or set the extrinsics in this model
- [[file:mrcal-python-api-reference.html#cameramodel-imagersize][=mrcal.cameramodel.imagersize()=]]: Get the imagersize in this model
- [[file:mrcal-python-api-reference.html#cameramodel-valid_intrinsics_region][=mrcal.cameramodel.valid_intrinsics_region()=]]: Get or set the valid intrinsics region
- [[file:mrcal-python-api-reference.html#cameramodel-optimization_inputs][=mrcal.cameramodel.optimization_inputs()=]]: Get the original optimization
  inputs. Used for uncertainty evaluation or other analysis
- [[file:mrcal-python-api-reference.html#cameramodel-icam_intrinsics][=mrcal.cameramodel.icam_intrinsics()=]]: Get the camera index indentifying this
  camera at optimization time. Used in conjunction with
  [[file:mrcal-python-api-reference.html#cameramodel-optimization_inputs][=mrcal.cameramodel.optimization_inputs()=]]

*** Geometry
- [[file:mrcal-python-api-reference.html#-identity_R][=mrcal.identity_R()=]]: Return an identity rotation matrix
- [[file:mrcal-python-api-reference.html#-identity_r][=mrcal.identity_r()=]]: Return an identity Rodrigues rotation
- [[file:mrcal-python-api-reference.html#-identity_Rt][=mrcal.identity_Rt()=]]: Return an identity Rt transformation
- [[file:mrcal-python-api-reference.html#-identity_rt][=mrcal.identity_rt()=]]: Return an identity rt transformation
- [[file:mrcal-python-api-reference.html#-r_from_R][=mrcal.r_from_R()=]]: Compute a Rodrigues vector from a rotation matrix
- [[file:mrcal-python-api-reference.html#-R_from_r][=mrcal.R_from_r()=]]: Compute a rotation matrix from a Rodrigues vector
- [[file:mrcal-python-api-reference.html#-rt_from_Rt][=mrcal.rt_from_Rt()=]]: Compute an rt transformation from a Rt transformation
- [[file:mrcal-python-api-reference.html#-Rt_from_rt][=mrcal.Rt_from_rt()=]]: Compute an Rt transformation from a rt transformation
- [[file:mrcal-python-api-reference.html#-invert_Rt][=mrcal.invert_Rt()=]]: Invert an Rt transformation
- [[file:mrcal-python-api-reference.html#-invert_rt][=mrcal.invert_rt()=]]: Invert an rt transformation
- [[file:mrcal-python-api-reference.html#-compose_Rt][=mrcal.compose_Rt()=]]: Compose Rt transformations
- [[file:mrcal-python-api-reference.html#-compose_rt][=mrcal.compose_rt()=]]: Compose rt transformations
- [[file:mrcal-python-api-reference.html#-rotate_point_r][=mrcal.rotate_point_r()=]]: Rotate point(s) using a Rodrigues vector
- [[file:mrcal-python-api-reference.html#-rotate_point_R][=mrcal.rotate_point_R()=]]: Rotate point(s) using a rotation matrix
- [[file:mrcal-python-api-reference.html#-transform_point_rt][=mrcal.transform_point_rt()=]]: Transform point(s) using an rt transformation
- [[file:mrcal-python-api-reference.html#-transform_point_Rt][=mrcal.transform_point_Rt()=]]: Transform point(s) using an Rt transformation
- [[file:mrcal-python-api-reference.html#-R_from_quat][=mrcal.R_from_quat()=]]: Convert a rotation defined as a unit quaternion rotation to a rotation matrix
- [[file:mrcal-python-api-reference.html#-quat_from_R][=mrcal.quat_from_R()=]]: Convert a rotation defined as a rotation matrix to a unit quaternion
- [[file:mrcal-python-api-reference.html#-align_procrustes_points_Rt01][=mrcal.align_procrustes_points_Rt01()=]]: Compute a rigid transformation to align two point clouds
- [[file:mrcal-python-api-reference.html#-align_procrustes_vectors_R01][=mrcal.align_procrustes_vectors_R01()=]]: Compute a rotation to align two sets of direction vectors

*** Projections
- [[file:mrcal-python-api-reference.html#-project][=mrcal.project()=]]: Projects a set of 3D camera-frame points to the imager
- [[file:mrcal-python-api-reference.html#-unproject][=mrcal.unproject()=]]: Unprojects pixel coordinates to observation vectors
- [[file:mrcal-python-api-reference.html#-project_stereographic][=mrcal.project_stereographic()=]]: Projects a set of 3D camera-frame points using a stereographic map
- [[file:mrcal-python-api-reference.html#-unproject_stereographic][=mrcal.unproject_stereographic()=]]: Unprojects a set of 2D pixel coordinates using a stereographic map

*** Visualization
- [[file:mrcal-python-api-reference.html#-show_geometry][=mrcal.show_geometry()=]]: Visualize the world resulting from a calibration run
- [[file:mrcal-python-api-reference.html#-show_projection_diff][=mrcal.show_projection_diff()=]]: Visualize the difference in projection between N models
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty][=mrcal.show_projection_uncertainty()=]]: Visualize the uncertainty in camera projection
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty_xydist][=mrcal.show_projection_uncertainty_xydist()=]]: Visualize in 3D the uncertainty in camera projection
- [[file:mrcal-python-api-reference.html#-show_projection_uncertainty_vs_distance][=mrcal.show_projection_uncertainty_vs_distance()=]]: Visualize the uncertainty in camera projection along one observation ray
- [[file:mrcal-python-api-reference.html#-show_distortion_off_pinhole][=mrcal.show_distortion_off_pinhole()=]]: Visualize a lens's deviation from a pinhole projection
- [[file:mrcal-python-api-reference.html#-show_valid_intrinsics_region][=mrcal.show_valid_intrinsics_region()=]]: Visualize a model's valid-intrinsics region
- [[file:mrcal-python-api-reference.html#-show_splined_model_surface][=mrcal.show_splined_model_surface()=]]: Visualize the surface represented by a splined model
- [[file:mrcal-python-api-reference.html#-annotate_image__valid_intrinsics_region][=mrcal.annotate_image__valid_intrinsics_region()=]]: Annotate an image with a model's valid-intrinsics region
- [[file:mrcal-python-api-reference.html#-imagergrid_using][=mrcal.imagergrid_using()=]]: Get a 'using' expression for imager colormap plots
- [[file:mrcal-python-api-reference.html#-sample_imager][=mrcal.sample_imager()=]]: Returns regularly-sampled, gridded pixels coordinates across the imager
- [[file:mrcal-python-api-reference.html#-sample_imager_unproject][=mrcal.sample_imager_unproject()=]]: Reports 3D observation vectors that regularly sample the imager
- [[file:mrcal-python-api-reference.html#-plotoptions_state_boundaries][=mrcal.plotoptions_state_boundaries()=]]: Return the 'set' plot options for gnuplotlib to show the state boundaries
- [[file:mrcal-python-api-reference.html#-plotoptions_measurement_boundaries][=mrcal.plotoptions_measurement_boundaries()=]]: Return the 'set' plot options for gnuplotlib to show the measurement boundaries
- [[file:mrcal-python-api-reference.html#-apply_color_map][=mrcal.apply_color_map()=]]: Color-code an array

*** Calibration
These are used by routines implementing a camera calibration system. Most users
will run the [[file:mrcal-calibrate-cameras.html][=mrcal-calibrate-cameras=]] tool instead of calling these.

- [[file:mrcal-python-api-reference.html#-compute_chessboard_corners][=mrcal.compute_chessboard_corners()=]]: Compute the chessboard observations and returns them in a usable form
- [[file:mrcal-python-api-reference.html#-estimate_monocular_calobject_poses_Rt_tocam][=mrcal.estimate_monocular_calobject_poses_Rt_tocam()=]]: Estimate camera-referenced poses of the calibration object from monocular views
- [[file:mrcal-python-api-reference.html#-estimate_joint_frame_poses][=mrcal.estimate_joint_frame_poses()=]]: Estimate world-referenced poses of the calibration object
- [[file:mrcal-python-api-reference.html#-make_seed_pinhole][=mrcal.make_seed_pinhole()=]]: Compute an optimization seed for a camera calibration

*** Image transforms
- [[file:mrcal-python-api-reference.html#-scale_focal__best_pinhole_fit][=mrcal.scale_focal__best_pinhole_fit()=]]: Compute the optimal focal-length scale for reprojection to a pinhole lens
- [[file:mrcal-python-api-reference.html#-pinhole_model_for_reprojection][=mrcal.pinhole_model_for_reprojection()=]]: Generate a pinhole model suitable for reprojecting an image
- [[file:mrcal-python-api-reference.html#-image_transformation_map][=mrcal.image_transformation_map()=]]: Compute a reprojection map between two models
- [[file:mrcal-python-api-reference.html#-transform_image][=mrcal.transform_image()=]]: Transforms a given image using a given map

*** Model analysis
- [[file:mrcal-python-api-reference.html#-implied_Rt10__from_unprojections][=mrcal.implied_Rt10__from_unprojections()=]]: Compute the implied-by-the-intrinsics transformation to fit two cameras' projections
- [[file:mrcal-python-api-reference.html#-worst_direction_stdev][=mrcal.worst_direction_stdev()=]]: Compute the worst-direction standard deviation from a 2x2 covariance matrix
- [[file:mrcal-python-api-reference.html#-projection_uncertainty][=mrcal.projection_uncertainty()=]]: Compute the projection uncertainty of a camera-referenced point
- [[file:mrcal-python-api-reference.html#-projection_diff][=mrcal.projection_diff()=]]: Compute the difference in projection between N models
- [[file:mrcal-python-api-reference.html#-is_within_valid_intrinsics_region][=mrcal.is_within_valid_intrinsics_region()=]]: Which of the pixel coordinates fall within the valid-intrinsics region?

*** Stereo
- [[file:mrcal-python-api-reference.html#-stereo_rectify_prepare][=mrcal.stereo_rectify_prepare()=]]: Precompute everything needed for stereo rectification and matching
- [[file:mrcal-python-api-reference.html#-stereo_unproject][=mrcal.stereo_unproject()=]]: Unprojection in the rectified stereo system
- [[file:mrcal-python-api-reference.html#-stereo_range][=mrcal.stereo_range()=]]: Compute ranges from observed disparities

*** Synthetic data
- [[file:mrcal-python-api-reference.html#-ref_calibration_object][=mrcal.ref_calibration_object()=]]: Return the geometry of the calibration object
- [[file:mrcal-python-api-reference.html#-synthesize_board_observations][=mrcal.synthesize_board_observations()=]]: Produce synthetic chessboard observations

*** CHOLMOD interface
The mrcal solver is an optimization routine based on sparse nonlinear least
squares. The optimization loop is implemented in [[https://www.github.com/dkogan/libdogleg][=libdogleg=]], which uses the
[[https://people.engr.tamu.edu/davis/suitesparse.html][CHOLMOD solver]] to compute the [[https://en.wikipedia.org/wiki/Cholesky_decomposition][Cholesky factorization]]. With a Cholesky
factorization we can efficiently solve the linear system $J^T J \vec a = \vec b$
where the jacobian matrix $J$ is large and sparse.

CHOLMOD is a C routine, and mrcal provides a Python interface. This is used
internally for the [[file:uncertainty.org][projection uncertainty]] computations, and is convenient for
general analysis. The sparse $J$ matrix is available from the optimizer via the
[[file:mrcal-python-api-reference.html#-optimizer_callback][=mrcal.optimizer_callback()=]] function, as a [[https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html][=scipy.sparse.csr_matrix=]] sparse
array.

The factorization can be computed by instantiating a
[[file:mrcal-python-api-reference.html#CHOLMOD_factorization][=mrcal.CHOLMOD_factorization=]] class, and the linear system can then be solved by
calling [[file:mrcal-python-api-reference.html#CHOLMOD_factorization-solve_xt_JtJ_bt][=mrcal.CHOLMOD_factorization.solve_xt_JtJ_bt()=]]. See these two
docstrings for usage details and examples.

*** Optimization and measurement vectors layout
- [[file:mrcal-python-api-reference.html#-state_index_intrinsics][=mrcal.state_index_intrinsics()=]]: Return the index in the optimization vector of the intrinsics of camera i
- [[file:mrcal-python-api-reference.html#-state_index_extrinsics][=mrcal.state_index_extrinsics()=]]: Return the index in the optimization vector of the extrinsics of camera i
- [[file:mrcal-python-api-reference.html#-state_index_frames][=mrcal.state_index_frames()=]]: Return the index in the optimization vector of the pose of frame i
- [[file:mrcal-python-api-reference.html#-state_index_points][=mrcal.state_index_points()=]]: Return the index in the optimization vector of the position of point i
- [[file:mrcal-python-api-reference.html#-state_index_calobject_warp][=mrcal.state_index_calobject_warp()=]]: Return the index in the optimization vector of the calibration object warp
- [[file:mrcal-python-api-reference.html#-num_states_intrinsics][=mrcal.num_states_intrinsics()=]]: Get the number of intrinsics parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_extrinsics][=mrcal.num_states_extrinsics()=]]: Get the number of extrinsics parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_frames][=mrcal.num_states_frames()=]]: Get the number of calibration object pose parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_points][=mrcal.num_states_points()=]]: Get the number of point-position parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_calobject_warp][=mrcal.num_states_calobject_warp()=]]: Get the number of parameters in the optimization vector for the board warp
- [[file:mrcal-python-api-reference.html#-measurement_index_boards][=mrcal.measurement_index_boards()=]]: Return the measurement index of the start of a given board observation
- [[file:mrcal-python-api-reference.html#-measurement_index_points][=mrcal.measurement_index_points()=]]: Return the measurement index of the start of a given point observation
- [[file:mrcal-python-api-reference.html#-measurement_index_regularization][=mrcal.measurement_index_regularization()=]]: Return the index of the start of the regularization measurements
- [[file:mrcal-python-api-reference.html#-num_measurements_boards][=mrcal.num_measurements_boards()=]]: Return how many measurements we have from calibration object observations
- [[file:mrcal-python-api-reference.html#-num_measurements_points][=mrcal.num_measurements_points()=]]: Return how many measurements we have from point observations
- [[file:mrcal-python-api-reference.html#-num_measurements_regularization][=mrcal.num_measurements_regularization()=]]: Return how many measurements we have from regularization
- [[file:mrcal-python-api-reference.html#-num_measurements][=mrcal.num_measurements()=]]: Return how many measurements we have in the full optimization problem

*** Optimization
- [[file:mrcal-python-api-reference.html#-optimize][=mrcal.optimize()=]]: Invoke the calibration routine
- [[file:mrcal-python-api-reference.html#-optimizer_callback][=mrcal.optimizer_callback()=]]: Call the optimization callback function
- [[file:mrcal-python-api-reference.html#-pack_state][=mrcal.pack_state()=]]: Scales a state vector to the packed, unitless form used by the optimizer
- [[file:mrcal-python-api-reference.html#-unpack_state][=mrcal.unpack_state()=]]: Scales a state vector from the packed, unitless form used by the optimizer
- [[file:mrcal-python-api-reference.html#-ingest_packed_state][=mrcal.ingest_packed_state()=]]: Read a given packed state into optimization_inputs
- [[file:mrcal-python-api-reference.html#-corresponding_icam_extrinsics][=mrcal.corresponding_icam_extrinsics()=]]: Return the icam_extrinsics corresponding to a given icam_intrinsics

*** Lens models
- [[file:mrcal-python-api-reference.html#-supported_lensmodels][=mrcal.supported_lensmodels()=]]: Returns a tuple of strings for the various lens models we support
- [[file:mrcal-python-api-reference.html#-lensmodel_num_params][=mrcal.lensmodel_num_params()=]]: Get the number of lens parameters for a particular model type
- [[file:mrcal-python-api-reference.html#-lensmodel_metadata][=mrcal.lensmodel_metadata()=]]: Returns meta-information about a model
- [[file:mrcal-python-api-reference.html#-knots_for_splined_models][=mrcal.knots_for_splined_models()=]]: Return a tuple of locations of x and y spline knots

*** Miscellaneous utilities
- [[file:mrcal-python-api-reference.html#-hypothesis_corner_positions][=mrcal.hypothesis_corner_positions()=]]: Reports the 3D chessboard points observed by a camera at calibration time
- [[file:mrcal-python-api-reference.html#-polygon_difference][=mrcal.polygon_difference()=]]: Return the difference of two closed polygons
- [[file:mrcal-python-api-reference.html#-mapping_file_framenocameraindex][=mrcal.mapping_file_framenocameraindex()=]]: Parse image filenames to get the frame numbers
- [[file:mrcal-python-api-reference.html#-close_contour][=mrcal.close_contour()=]]: Close a polyline, if it isn't already closed
- [[file:mrcal-python-api-reference.html#-apply_homography][=mrcal.apply_homography()=]]: Apply a homogeneous-coordinate homography to a set of 2D points

* How to run a calibration
talk about --seed and how that can be used to validate intrinsics

** Tutorial
If all you want to do is run a calibration, read this section first.

You need to get observations of a grid of points. This tool doesn't dictate
exactly how these observations are obtained, but the recommended way to do that
is to use [[http://github.com/dkogan/mrgingham][mrgingham]]. This documentation assumes that's what is being done.

See the mrgingham documentation for a .pdf of a chessboard pattern. This pattern
should be printed (at some size; see below) and mounted onto a RIGID and FLAT
surface to produce the calibration object. The most useful observations are
close-ups: views that cover as much of the imager as possible. Thus you
generally a large printout of the chessboard pattern. If you're calibrating a
wide lens then this is especially true: the wider the lens, the larger an object
needs to be in order to cover the field of view.

Now that we have a calibration object, this object needs to be shown to the
camera(s) to produce the images that mrgingham will use to find the corner
coordinates, which mrcal will then use in its computations.

It is important that the images contain clear corners. If the image is badly
overexposed, the white chessboard squares will bleed into each other, the
adjoining black squares will no longer touch each other in the image, and there
would be no corner to detect. Conversely, if the image is badly underexposed,
the black squares will bleed into each other, which would also destroy the
corner. mrgingham tries to handle a variety of lighting conditions, including
varying illumination across the image, but the corners must exist in the image
in some form. A fundamental design decision in mrgingham is to only output
chessboards that we are very confident in, and a consequence of this is that
mrgingham requires the WHOLE chessboard to be visible in order to produce any
results. Thus it requires a bit of effort to produce any data at the edges and
in the corners of the imager: if even a small number of the chessboard corners
are out of bounds, mrgingham will not detect the chessboard at all. A live
preview of the calibration images being gathered is thus essential to aid the
user in obtaining good data. Another requirement due to the design of mrgingham
is that the board should be held with a flat edge parallel to the camera xz
plane (parallel to the ground, usually). mrgingham looks for vertical and
horizontal sequences of corners, but if the board is rotated diagonally, then
none of these sequences are "horizontal" or "vertical", but they're all
"diagonal", which isn't what mrgingham is looking for.

The most useful observations to gather are

- close-ups: the chessboard should fill the whole frame as much as possible

- oblique views: tilt the board forward/back and left/right. I generally tilt by
  more than 45 degrees. At a certain point the corners become indistinct and
  mrgingham starts having trouble, but depending on the lens, that point could
  come with quite a bit of tilt.

- If you are calibrating multiple cameras, and they are synchronized, you can
  calibrate them all at the same time, and obtain intrinsics AND extrinsics. In
  that case you want frames where multiple cameras see the calibration object at
  the same time. Depending on the geometry, it may be impossible to place a
  calibration object in a location where it's seen by all the cameras, AND where
  it's a close-up for all the cameras at the same time. In that case, get
  close-ups for each camera individually, and get observations common to
  multiple cameras, that aren't necessarily close-ups. The former will serve to
  define your camera intrinsics, and the latter will serve to define your
  extrinsics (geometry).

A dataset composed primarily of tilted closeups will produce good results. It is
better to have more data rather than less. mrgingham will throw away frames
where no chessboard can be found, so it is perfectly reasonable to grab too many
images with the expectation that they won't all end up being used in the
computation.

I usually aim for about 100 usable frames, but you can often get away with far
fewer. The mrcal confidence feedback (see below) will tell you if you need more
data.

Once we have gathered input images, we can run the calibration procedure:

#+begin_src sh
mrcal-calibrate-cameras
  --corners-cache corners.vnl
  -j 10
  --focal 2000
  --object-spacing 0.1
  --object-width-n 10
  --outdir /tmp
  --lensmodel LENSMODEL_OPENCV8
  --observed-pixel-uncertainty 1.0
  --explore
  'frame*-camera0.png' 'frame*-camera1.png' 'frame*-camera2.png'
#+end_src

You would adjust all the arguments for your specific case.

The first argument says that the chessboard corner coordinates live in a file
called =corners.vnl=. If this file exists, we'll use that data. If that file
does not exist (which is what will happen the first time), mrgingham will be
invoked to compute the corners from the images, and the results will be written
to that file. So the same command is used to both compute the corners initially,
and to reuse the pre-computed corners with subsequent runs.

=-j 10= says to spread the mrgingham computation across 10 CPU cores. This
command controls mrgingham only; if 'corners.vnl' already exists, this option
does nothing.

=--focal 2000= says that the initial estimate for the camera focal lengths is
2000 pixels. This doesn't need to be precise at all, but do try to get this
roughly correct if possible. Simple geometry says that

  focal_length = imager_width / ( 2 tan (field_of_view_horizontal / 2) )

=--object-spacing= is the width of each square in your chessboard. This depends on
the specific chessboard object you are using. --object-width-n is the corner
count of the calibration object. Currently mrgingham more or less assumes that
this is 10.

=--outdir= specifies the directory where the output models will be written

=--lensmodel= specifies which lens model we're using for the cameras. At this
time all OpenCV lens models are supported, in addition to =LENSMODEL_CAHVOR=.
The CAHVOR model is there for legacy compatibility only. If you're not going to
be using these models in a system that only supports CAHVOR, there's little
reason to use it. If you use a model that is too lean (=LENSMODEL_PINHOLE= or
=LENSMODEL_OPENCV4= maybe), the model will not fit the data, especially at the
edges; the tool will tell you this. If you use a model that is too rich
(something crazy like =LENSMODEL_OPENCV12=), then you will need more data than
you normally would. Most lenses I've seen work well with =LENSMODEL_OPENCV4= or
=LENSMODEL_OPENCV5= or =LENSMODEL_OPENCV8=; wider lenses need richer models.

=--observed-pixel-uncertainty 1.0= says that the x,y corner coordinates reported
by mrgingham are distributed normally, independently, and with the standard
deviation as given in this argument. There's a tool to compute this value
empirically, but it needs more validation. For now pick a value that seems
reasonable. 1.0 pixels or less usually makes sense.

=--explore= says that after the models are computed, a REPL should be open so that
the user can look at various metrics describing the output; more on this
later.

After all the options, globs describing the images are passed in. Note that
these are GLOBS, not FILENAMES. So you need to quote or escape each glob to
prevent the shell from expanding it. You want one glob per camera; in the above
example we have 3 cameras. The program will look for all files matching the
globs, and filenames with identical matched strings are assumed to have been
gathered at the same instant in time. I.e. if in the above example we found
frame003-camera0.png and frame003-camera1.png, we will assume that these two
images were time-synchronized. If your capture system doesn't have
fully-functional frame syncronization, you should run a series of monocular
calibrations. Otherwise the models won't fit well (high reprojection errors
and/or high outlier counts) and you might see a frame with systematic
reprojection errors where one supposedly-synchronized camera's observation pulls
the solution in one direction, and another camera's observation pulls it in
another.

When you run the program as given above, the tool will spend a bit of time
computing (usually 10-20 seconds is enough, but this is highly dependent on the
specific problem, the amount of data, and the computational hardware). When
finished, it will write the resulting models to disk, and open a REPL (if
--explore was given). The resulting filenames are =camera-N.cameramodel= where
=N= is the index of the camera, starting at 0. The models contain the intrinsics
and extrinsics, with camera-0 sitting at the reference coordinate system.

When the solve is completed, you'll see a summary such as this one:

#+begin_example
RMS reprojection error: 0.3 pixels
Worst reprojection error: 4.0 pixels
Noutliers: 7 out of 9100 total points: 0.1% of the data
#+end_example

The reprojection errors should look reasonable given your
=--observed-pixel-uncertainty=. Since any outliers will be thrown out, the
reported reprojection errors will be reasonable.

Higher outlier counts are indicative of some/all of these:

- Errors in the input data, such as incorrectly-detected chessboard corners, or
  unsynchronized cameras

- Badly-fitting lens model

A lens model that doesn't fit isn't a problem in itself. The results will
simply not be reliable everywhere in the imager, as indicated by the uncertainty
and residual metrics (see below)

With --explore you get a REPL, and a message that points out some useful
functions. Generally you want to start with

#+begin_example
show_residuals_observation_worst(0)
#+end_example

This will show you the worst-fitting chessboard observation with its observed
and predicted corners, as an error vector. The reprojection errors are given by
a colored dot. Corners thrown out as outliers will be missing their colored dot.
You want to make sure that this is reasonable. Incorrectly-detected corners will
be visible: they will be outliers or they will have a high error. The errors
should be higher towards the edge of the imager, especially with a wider lens. A
richer better-fitting model would reduce those errors. Past that, there should
be no pattern to the errors. If the camera synchronization was broken, you'll
see a bias in the error vectors, to compensate for the motion of the chessboard.

Next do this for each camera in your calibration set (icam is an index counting
up from 0):

#+begin_example
show_residuals_regional(icam)
#+end_example

Each of these will pop up 3 plots describing your distribution of errors. You
get

- a plot showing the mean reprojection error across the imager
- a plot showing the standard deviation of reprojection errors across the imager
- a plot showing the number of data points across the imager AFTER the outlier
  rejection

The intrinsics are reliable in areas that have

- a low mean error relative to =--observed-pixel-uncertainty=
- a standard deviation roughly similar to =--observed-pixel-uncertainty=
- have some data available

If you have too little data, you will be overfitting, so you'd be expalining the
signal /and/ the noise, and your reprojection errors will be too low. With
enough input data you'll be explaining the signal only: the noise is random and
with enough samples our model can't explain it. Another factor that controls
this is the model we're fitting. If we fit a richer model (=LENSMODEL_OPENCV8=
vs =LENSMODEL_OPENCV4= for instance), the extra parameters will allow us to fit
the data better, and to produce lower errors in more areas of the imager.

These are very rough guidelines; I haven't written the logic to automatically
interpret these yet. A common feature that these plots bring to light is a
poorly-fitting model at the edges of the imager. In that case you'll see higher
errors with a wider distribution towards the edge.

Finally run this:

#+begin_example
show_projection_uncertainty()
#+end_example

This will pop up a plot of projection uncertainties for each camera. The
uncertainties are shown as a color-map along with contours. These are the
expected value of projection based on noise in input corner observations. The
noise is assumed to be independent, 0-mean gaussian with a standard deviation of
--observed-pixel-uncertainty. You will see low uncertainties in the center of
the imager (this is the default focus point; a different one can be picked). As
you move away from the center, you'll see higher errors. You should decide how
much error is acceptable, and determine the usable area of the imager based on
this. These uncertainty metrics are complementary to the residual metrics
described above. If you have too little data, the residuals will be low, but the
uncertainties will be very high. The more data you gather, the lower the
uncertainties. A richer lens model lowers the residuals, but raises the
uncertainties. So with a richer model you need to get more data to get to the
same acceptable uncertainty level.

** Capture images
 - Hold board straight
 - Oblique closeups
** mrgingham
*** mrcal
 - metrics
* Model differencing
* Optimization problem formulation
:PROPERTIES:
:CUSTOM_ID: formulation
:END:

mrcal contains a solver used to compute the lens models and/or geometry in any
given problem. This is accessible via either

- =mrcal_optimize()= routine in the [[file:c-api.org][C API]]
- [[file:mrcal-python-api-reference.html#-optimize][=mrcal.optimize()=]] routine in the Python API

These are the main call in the [[file:mrcal-calibrate-cameras.html][=mrcal-calibrate-cameras=]] tool (to calibrate
cameras) and [[file:mrcal-convert-lensmodel.html][=mrcal-convert-lensmodel=]] tool (to fit a different lens model into
an existing model). The optimization routines themselves are more general than
this, and can solve more problems, such as structure-from-motion. The APIs for
the handling of discrete points are still unstable, so the structure-from-motion
functionality remains undocumented for now.

The solver moves around in the space of /state/ vectors $\vec p$, trying to
minimize the cost function $E \equiv \left \Vert \vec x \right \Vert ^2$ where
$\vec x$ is the vector of /measurements/. The solver evaluates each hypothesis
$\vec p$ by repeatedly invoking a callback function to report $\vec x$ and the
local gradients $J \equiv \frac{\partial \vec x}{\partial \vec p}$. The callback
function is available by itself via

- =mrcal_optimizer_callback()= routine in the [[file:c-api.org][C API]]
- [[file:mrcal-python-api-reference.html#-optimizer_callback][=mrcal.optimizer_callback()=]] routine in the Python API

More or less, it's trying to find the set of geometry and lens parameters to
best explain the observed pixel coordinates.

** Optimization details
The mrcal solver is an optimization routine based on sparse nonlinear least
squares. The optimization loop is implemented in [[https://www.github.com/dkogan/libdogleg][=libdogleg=]], which uses the
[[https://people.engr.tamu.edu/davis/suitesparse.html][CHOLMOD solver]] to compute the [[https://en.wikipedia.org/wiki/Cholesky_decomposition][Cholesky factorization]]. With a Cholesky
factorization we can efficiently solve the linear system $J^T J \vec a = \vec b$
where the jacobian matrix $J$ is large and sparse.

The optimization problem is posed without constraints. This is achieved by using
[[https://en.wikipedia.org/wiki/Axis%E2%80%93angle_representation#Rotation_vector][Rodrigues vectors]] to represent rotations. A different rotation representation,
such as one using unit quaternions or rotation matrices would require
constraints: not all sets of 4 numbers are a unit quaternion, and not all sets
of 9 numbers are a valid rotation matrix.

The optimization algorithm is iterative, so it isn't guaranteed to converge to
the global optimum. Thus it is imperative to pass a good *seed* (an initial
estimate of the solution) to the optimization routines. The
[[file:mrcal-calibrate-cameras.html][=mrcal-calibrate-cameras=]] tool achieves this by

1. Computing an initial estimate directly using geometry and some simplifying
   assumptions. The geometric seeding routines are available by themselves:

   - [[file:mrcal-python-api-reference.html#-estimate_monocular_calobject_poses_Rt_tocam][=mrcal.estimate_monocular_calobject_poses_Rt_tocam()=]]: Estimate camera-referenced poses of the calibration object from monocular views
   - [[file:mrcal-python-api-reference.html#-estimate_joint_frame_poses][=mrcal.estimate_joint_frame_poses()=]]: Estimate world-referenced poses of the calibration object
   - [[file:mrcal-python-api-reference.html#-make_seed_pinhole][=mrcal.make_seed_pinhole()=]]: Compute an optimization seed for a camera calibration

2. And then refining that estimate with a sequences of optimization problems
   that allow more and more of the parameters to vary. The final problem is the
   /full/ problem where all the variables are free to move. The set of variables
   we're optimizing can be selected with the [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_problem_details_t=]] structure
   passed to [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_optimize()=]] in C (or the =do_optimize_...= arguments to
   [[file:mrcal-python-api-reference.html#-optimize][=mrcal.optimize()=]] in Python).

** World geometry
There are 3 different coordinate systems in the optimization:

- *camera* coordinate system: the local coordinate system of each camera. The
  $x$ and $y$ axes are aligned with pixel coordinates in an image: $x$ is to the
  right and $y$ is down. $z$ is then forward to complete the right-handed
  system of coordinates.
- *frame* coordinate system: the local coordinate system of the chessboard. The
  chessboard is assumed mostly flat, with the grid of points lying in the $xy$
  plane. The origin is at the first point.
- *reference* coordinate system: the "world" coordinate system in the
  optimization problem. This coordinate system is the common system that ties
  everything together. Each chessboard pose is represented as a transformation
  between the local chessboard frame and the reference frame. And each camera
  pose is represented as the transformation between the local camera frame and
  the reference frame.

So the data flow to project a particular chessboard corner which sits at $\vec
p_\mathrm{frame}$ in the local chessboard coordinate system is:

\[ \vec q                     \xleftarrow{\mathrm{intrinsics}}
   \vec p_\mathrm{camera}     \xleftarrow{T_\mathrm{cr}}
   \vec p_\mathrm{reference}  \xleftarrow{T_\mathrm{rf}}
   \vec p_\mathrm{frame}
\]

where the intrinsics and the transformations $T_\mathrm{cr}$ and $T_\mathrm{rf}$ are elements of the state vector.

*** Geometric free variables
If too many transformations are left as free variables for the optimizer to
find, the system will be under-determined, and the optimization routine will
fail: complaining about a "not positive definite" (singular in this case)
Hessian.

Example: we have 1 stationary camera observing 10 chessboards. We want to be
able to uniquely represent the transformation between each chessboard and the
camera, for a total of 10 transformations. If we optimize a separate
$T_\mathrm{cr}$ for the camera and 10 separate $T_\mathrm{rf}$ for each
chessboard, we will have 11 transformations in the optimization vector. Here 11
> 10, so the system is under-determined, and the optimization will fail.

In a vanilla calibration problem, we would address this by fixing the reference
coordinate system to one of the camera or chessboard frames. The mrcal
convention is to fix the reference coordinate system to camera 0. In the above
example, this would reduce the number of transformations being optimized from 11
to 10, which would resolve the issue.

Any other method of making the optimization variables unique is valid also. For
instance, the chessboard poses might be known. In that case we don't need to
optimize any $T_\mathrm{rf}$, and solving for /all/ the $T_\mathrm{cr}$ is
valid.

*** The meaning of the /reference/ coordinate system
The reference coordinate system is a single coordinate system common to the
whole optimization problem that all the objects in the world can use to localize
themselves. This reference coordinate system does /not/ have any physical
meaning beyond that. In particular, the reference coordinate system is /not/
attached to any fixed object in the world. So if we're looking at how the
optimal geometry would respond to perturbations in the camera observations, the
reference coordinate system would move, as would the camera and chessboard
coordinate systems. Please see the [[file:uncertainty.org][projection uncertainty]] documentation for more
information.

** State vector $\vec p$
The state vector $\vec p$ is controlled by the optimization algorithm as it
searches for the optimal solution. This vector may contain

- *intrinsics*: the lens parameters of all the cameras in the optimization problem
- *extrinsics*: the poses of all the cameras in the optimization problem. These
  are specified as unconstrained =rt= transformations from some arbitrary
  "reference". coordinate system, to the camera coordinate system. These are
  represented by $T_\mathrm{cr}$ in the flow diagram above.
- *frames*: the poses of all the chessboards in the optimization problem. These
  are specified as unconstrained =rt= transformations from the local chessboard
  coordinate system to some arbitrary "reference" coordinate system. These are
  represented by $T_\mathrm{rf}$ in the flow diagram above.
- *points*: the location in the reference coordinate system of any discrete
  points being observed. A vanilla "calibration" problem wouldn't have any of
  these, but an SFM problem would have many.
- *calibration-object warp*: the deformation of the calibration object. Large
  calibration boards used for calibration of wide lenses are never flat:
  temperature and humidity effects deform the board strongly-enough to affect
  the calibration. mrcal models this deformation with two axis-aligned parabolic
  factors. If the chessboard grid spans $[-1,1]$ along the $x$ and $y$ axes,
  then I define the non-planar deformation as $z \equiv k_x (1 - x^2) + k_y (1 -
  y^2)$ with $k_x$ and $k_y$ being the two deformation factors being optimized.

The optimization problem /could/ contain all those things, but it usually
doesn't. A vanilla calibration problem (stationary cameras, moving chessboard)
has no discrete points. A structure-from-motion problem (moving cameras,
stationary world being observed) usually has no chessboards or
calibration-object warping, but many discrete points. An intrinsics-fitting
problem (such as what [[file:mrcal-convert-lensmodel.html][=mrcal-convert-lensmodel=]] solves) has no extrinsics or
frames or calibration-object warping, but multiple discrete points.

*** State vector layout
When analyzing the behavior of the optimizer it is often useful to pick out
particular elements of the full optimization vector $\vec p$. mrcal provides a
number of functions to report the index and size of the block of $\vec p$ that
contains specific data. In C:

- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_state_index_intrinsics()=]]: Return the index in the optimization vector of the intrinsics of camera i
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_state_index_extrinsics()=]]: Return the index in the optimization vector of the extrinsics of camera i
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_state_index_frames()=]]: Return the index in the optimization vector of the pose of frame i
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_state_index_points()=]]: Return the index in the optimization vector of the position of point i
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_state_index_calobject_warp()=]]: Return the index in the optimization vector of the calibration object warp

- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_states_intrinsics()=]]: Get the number of intrinsics parameters in the optimization vector
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_states_extrinsics()=]]: Get the number of extrinsics parameters in the optimization vector
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_states_frames()=]]: Get the number of calibration object pose parameters in the optimization vector
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_states_points()=]]: Get the number of point-position parameters in the optimization vector
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_states_calobject_warp()=]]: Get the number of parameters in the optimization vector for the board warp

- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_states()=]]: Get the full length of the optimization vector

And in Python:

- [[file:mrcal-python-api-reference.html#-state_index_intrinsics][=mrcal.state_index_intrinsics()=]]: Return the index in the optimization vector of the intrinsics of camera i
- [[file:mrcal-python-api-reference.html#-state_index_extrinsics][=mrcal.state_index_extrinsics()=]]: Return the index in the optimization vector of the extrinsics of camera i
- [[file:mrcal-python-api-reference.html#-state_index_frames][=mrcal.state_index_frames()=]]: Return the index in the optimization vector of the pose of frame i
- [[file:mrcal-python-api-reference.html#-state_index_points][=mrcal.state_index_points()=]]: Return the index in the optimization vector of the position of point i
- [[file:mrcal-python-api-reference.html#-state_index_calobject_warp][=mrcal.state_index_calobject_warp()=]]: Return the index in the optimization vector of the calibration object warp

- [[file:mrcal-python-api-reference.html#-num_states_intrinsics][=mrcal.num_states_intrinsics()=]]: Get the number of intrinsics parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_extrinsics][=mrcal.num_states_extrinsics()=]]: Get the number of extrinsics parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_frames][=mrcal.num_states_frames()=]]: Get the number of calibration object pose parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_points][=mrcal.num_states_points()=]]: Get the number of point-position parameters in the optimization vector
- [[file:mrcal-python-api-reference.html#-num_states_calobject_warp][=mrcal.num_states_calobject_warp()=]]: Get the number of parameters in the optimization vector for the board warp

If plotting a whole vector of state (or a vector of measurements), it is really
helpful to annotate the plot to make it clear which variables correspond to each
block of state (or measurements). mrcal provides helper functions to help with
this:

- [[file:mrcal-python-api-reference.html#-plotoptions_state_boundaries][=mrcal.plotoptions_state_boundaries()=]]: Return the 'set' plot options for gnuplotlib to show the state boundaries
- [[file:mrcal-python-api-reference.html#-plotoptions_measurement_boundaries][=mrcal.plotoptions_measurement_boundaries()=]]: Return the 'set' plot options for gnuplotlib to show the measurement boundaries

*** State vector scaling
The nonlinear least squares-solving library used by mrcal is [[https://www.github.com/dkogan/libdogleg][=libdogleg=]], which
implements [[https://en.wikipedia.org/wiki/Powell's_dog_leg_method][Powell's dogleg method]]. This is a trust-region algorithm that
represents the trust region as a ball in state space. I.e. the radius of this
trust region is the same in every direction. And /that/ means that the
optimization will work best when each state variable in $\vec p$ affects the
cost function $E$ evenly. Example of what we don't want: camera positions
measured in km, while the chessboard positions are measured in mm.

Clearly getting identical behavior from each variable is impossible, but we can
scale the elements of $\vec p$ to keep things more or less even. Thus the
=libdogleg= optimization library never sees the full state vector $\vec p$, but
the scaled vector $\vec p_\mathrm{packed}$. Similarly, it never sees the full
jacobian $J \equiv \frac{\partial \vec x}{\partial \vec p}$, but rather
$J_\mathrm{packed} \equiv \frac{\partial \vec x}{\partial \vec
p_\mathrm{packed}}$. This means that the optimization callback functions /also/
report the packed state. These are

- =mrcal_optimizer_callback()= routine in the [[file:c-api.org][C API]]
- [[file:mrcal-python-api-reference.html#-optimizer_callback][=mrcal.optimizer_callback()=]] routine in the Python API

To pack or unpack an array of state, mrcal provides some routines. In C:

- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_pack_solver_state_vector()=]]: Scales a state vector to the packed, unitless form used by the optimizer
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_unpack_solver_state_vector()=]]: Scales a state vector from the packed, unitless form used by the optimizer

And in Python:

- [[file:mrcal-python-api-reference.html#-pack_state][=mrcal.pack_state()=]]: Scales a state vector to the packed, unitless form used by the optimizer
- [[file:mrcal-python-api-reference.html#-unpack_state][=mrcal.unpack_state()=]]: Scales a state vector from the packed, unitless form used by the optimizer

** Measurement vector $\vec x$
Given a hypothesis state vector $\vec p$ mrcal computes a vector of errors, or
/measurements/ $\vec x$. The optimization algorithm searches the space of
hypotheses $\vec p$, trying to minimize $E \equiv \left \Vert \vec x \right \Vert^2$.

We know where each point was observed in reality, and we know where the state
vector $\vec p$ predicts each one would have been observed. So we can construct
a vector of errors $\vec q_\mathrm{err} \equiv \vec q_\mathrm{predicted}\left(
\vec p \right) - \vec q_\mathrm{ref}$.

For each observation in $\vec q_\mathrm{ref}$ the chessboard corner detection
routine tells us how confident it was in that observation, from which we can get
a vector of weights $\vec w$: less confident observations are weighed less in
the optimization. See the [[file:uncertainty.org][projection uncertainty]] documentation for the noise
analysis. Let $W \equiv \mathrm{diag}\left( \vec w \right)$. Then I can define

\[ \vec x_\mathrm{observations} \equiv W q_\mathrm{err} = W \left( \vec
q_\mathrm{predicted}\left( \vec p \right) - \vec q_\mathrm{ref} \right) \]

This is the bulk of the measurement vector.

*** Regularization
:PROPERTIES:
:CUSTOM_ID: Regularization
:END:

In addition to $\vec x_\mathrm{observations}$ the measurement vector contains
[[https://en.wikipedia.org/wiki/Regularization_(mathematics)][/regularization/]] terms. These are mostly-insignificant terms that are meant to
improve the convergence of the solver. These are aphysical, and cause a bias in
the solution. mrcal is careful to keep these small-enough to not break anything
noticeably. The behavior of these terms is likely to change in the future, so I
don't want to do document these in detail; please consult the sources for
detail. Currently the logic is at the end of the [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=optimizer_callback()=]] function
in =mrcal.c=.

It is possible to control whether a solve does/does not include regularization
terms with the =do_apply_regularization= bit in [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_problem_details_t=]] or the
=do_apply_regularization= key in the call to [[file:mrcal-python-api-reference.html#-optimize][=mrcal.optimize()=]].

*** Measurement vector layout
When analyzing the behavior of the optimizer it is often useful to pick out
particular elements of the full measurement vector $\vec x$. mrcal provides a
number of functions to report the index and size of the block of $\vec x$ that
contains specific data. In C:

- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_measurement_index_boards()=]]: Return the measurement index of the start of a given board observation
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_measurement_index_points()=]]: Return the measurement index of the start of a given point observation
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_measurement_index_regularization()=]]: Return the index of the start of the regularization measurements
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_measurements_boards()=]]: Return how many measurements we have from calibration object observations
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_measurements_points()=]]: Return how many measurements we have from point observations
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_num_measurements_regularization()=]]: Return how many measurements we have from regularization
- [[https://github.jpl.nasa.gov/maritime-robotics/mrcal/blob/master/mrcal.h][=mrcal_measurements()=]]: Return how many measurements we have in the full optimization problem

And in Python:

- [[file:mrcal-python-api-reference.html#-measurement_index_boards][=mrcal.measurement_index_boards()=]]: Return the measurement index of the start of a given board observation
- [[file:mrcal-python-api-reference.html#-measurement_index_points][=mrcal.measurement_index_points()=]]: Return the measurement index of the start of a given point observation
- [[file:mrcal-python-api-reference.html#-measurement_index_regularization][=mrcal.measurement_index_regularization()=]]: Return the index of the start of the regularization measurements
- [[file:mrcal-python-api-reference.html#-num_measurements_boards][=mrcal.num_measurements_boards()=]]: Return how many measurements we have from calibration object observations
- [[file:mrcal-python-api-reference.html#-num_measurements_points][=mrcal.num_measurements_points()=]]: Return how many measurements we have from point observations
- [[file:mrcal-python-api-reference.html#-num_measurements_regularization][=mrcal.num_measurements_regularization()=]]: Return how many measurements we have from regularization
- [[file:mrcal-python-api-reference.html#-num_measurements][=mrcal.num_measurements()=]]: Return how many measurements we have in the full optimization problem

* Calibration object
:PROPERTIES:
:CUSTOM_ID: Calibration object
:END:

This is called a "chessboard" or just "board" in some parts of the code.

When running a camera calibration, we use observations of a known-geometry
object. Usually this object is a chessboard-like grid of black and white
squares, where the corners between squares are detected, and serve as the input
features to mrcal. mrcal is a purely geometrical toolkit, so this vision problem
must be handled by another library. I recommend [[https://github.com/dkogan/mrgingham/][=mrgingham=]], but any other
source of grid observations can be used.

The specific design of the calibration object is not important, as long as it
meets the current assumptions of the tool:

- flat (with a small amount of warping tolerated)
- contains a square grid of features

Chessboards are recommended, in contrast to grids of circles, which are strongly
discouraged. Precisely extracting the center of an observed circle from a tilted
observation that is also subjected to lens distortion is very difficult. And the
resulting inaccurate detections will introduce biases into the resulting
calibrations. Analysis [[file:tour.org::Optimal choreography][here]].

mrcal assumes independent noise on each point observation, so correlated sources
of points (such as corners of an apriltag) are not appropriate sources of data
currently.


* research topics
- Is my spline representation good? Can I avoid it crossing itself?
- Note that regularization causes a bias
- Intrinsics uncertainty contains a built-in extrinsics uncertainty. As we move
  the cameras around, we carry with them an uncertain transformation
- Board warping
- outlier rejection. Cook's D
- rotation compensation for the diff
- compensating for board flex
- compensating for focal-length errors
  common-mode errors do not affect yaw. differential-mode errors affect yaw very
  much
- intrinsics errors effect on yaw. I ran some simulations earlier, I think.
  Similar effect: differential errors are very significant
- talk about regularization bias
- splined models shouldn't fit the core to keep things non-singular
- splined models may not be fitted into opencv8 without moving extrinsics
- say that poor uncertainty = overfitting
- say that we need to track down the source of all errors. The model we're
  optimizing should not produce any error on its own. And it shouldn't produce
  any constraints on its own. The "model" includes the lens model and the
  warping here. Thus the uncertainties are only directly usable with the splined
  models
- talk about how I'm projecting the "same world point", and how there're other
  (possibly-better) methods
- talk about how to get observed_pixel_uncertainty
- talk about how to select an appropriate splined model
- talk about --seed and how that can be used to validate intrinsics

* Model differencing
:PROPERTIES:
:CUSTOM_ID: Model differencing
:END:

* Visualization
- say that the plots are interactive in normal usage
* After-release todo
- feed uncertainties to stereo, triangulation
- compute uncertainties for multiple points at the same time to get covariance.
  Possibly could work across multiple cameras in the same solve as well
- better regularization non/crossing in splined models
- should include a study of how to calibrate long lenses. Tilted observations
  aren't as effective unless the board is GIANT
- Can we study intrinsics stability over time? In response to heating? Shaking?
- Can we use a 3-parallel calibration to quantify chromatic aberration?

* future work

- measure observed_pixel_uncertainty
- use uncertainty in triangulation, deltapose
- improve uncertainty method: faraway obervations don't make things worse
- projection_uncertainty() should be able to project multiple points at a time,
  and to report correlations in the projection
- splined models should behave more nicely at the edges
- sfm
- integrate deltapose-lite
- projection_uncertainty() should report correlated results
- can I quantify the heteroscedasticity and thus the model-nonfitting and the
  resulted expected bias? White test?
- study cubic/quadratic splines, spline density effects
- do a triangulation with explict uncertainty propagation

- Redo, show stability. Heat? Show effects?
- uncertainty questions:
  - study the effects of the spline control points density
  - are quadratic splines better? more sparse, but only c1 instead of c2
  - Can I use the heteroschedasticity metrics to say stuff about the lean
    models?

- mention sfm
- feed uncertainties to stereo, triangulation
- compute uncertainties for multiple points at the same time to get covariance.
  Possibly could work across multiple cameras in the same solve as well
- better regularization non/crossing in splined models
- should include a study of how to calibrate long lenses. Tilted observations
  aren't as effective unless the board is GIANT
- Can we study intrinsics stability over time? In response to heating? Shaking?
- Can we use a 3-parallel calibration to quantify chromatic aberration?
- Measure effect of focus, aperture

* todo for the document
The "commandline tools" link on top is generated wrong

code should no longer refer to the projection_uncertainty() docstring, but
rather refer here

should say what this toolkit isfor, other than calibration
