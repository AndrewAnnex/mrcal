#+title: mrcal - camera calibrations and more!

@@html:<b>@@
Note: this is a work-in-progress documentation for a work-in-progress upcoming
mrcal 2.0 release. See the [[./docs-1.0][mrcal 1.0.1 documentation]] for the current stable
version.
@@html:</b>@@

mrcal is a toolkit (originating at NASA/JPL) for working with lens models,
camera geometry, images, projections, and the various related operations such as
camera calibration. Any task that produces or consumes camera models can utilize
this toolkit. It was originally built to generate the high-accuracy calibrations
demanded by long-range stereo, so it provides facilities to analyze calibration
accuracy and to propagate and report uncertainties.

* Why mrcal?
In essense: because all other tools are terrible if you care about accuracy.
They make basic questions like "how much data should I gather for a
calibration?" and "how good is this calibration I just computed?" and "how
different are these two models?" unanswerable.

Big assumptions and obliviousness to sources of error are commonplace. This is
clearly seen from the core documentation on the subject. [[https://en.wikipedia.org/wiki/Camera_resectioning][The wikipedia article
on camera calibration]] only talks about fitting a pinhole model to lenses, even
though no real lenses follow this model (telephoto lenses do somewhat; wider
lenses don't at all). [[https://www.opencv.org][OpenCV]] is a dominant library in this area; its [[https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html][calibration
tutorial]] cheerfully says that

#+begin_example
Re-projection error gives a good estimation of just how exact the found
parameters are. The closer the re-projection error is to zero, the more accurate
the parameters we found are.
#+end_example

This statement is trivially proven false: throw away most of your calibration
data, and your reprojection error becomes very low. But we can all agree that a
calibration computed from less data is actually worse. Right?

All the various assumptions and hacks in the existing tooling are fine as long
as you don't need a whole lot of accuracy out of your models.

mrcal is meant to address these shortcomings. This toolkit tries hard to allow
the user to produce calibrations that are as good as possible. It provides lots
of visualization capabilities to evaluate various properties of a model
solution. And it provides several powerful analysis methods, such as
quantification of projection and triangulation uncertainty. And model
differencing. And functions are provided to avoid making a pinhole-camera
assumption where it is not appropriate, such as with wide-angle stereo.

* Documentation index
Please see [[file:tour.org][a tour of mrcal]] for a high-level overview of the capabilities of the
toolkit.

First, the tools should be [[file:install.org][built or installed]].

Before using the tools, it is helpful to read about the [[file:conventions.org][terminology and
conventions]] used in the sources and documentation.

At the core of a calibration routine is an optimization problem. Details about
its [[file:formulation.org][formulation]] are useful to be able to interpret the results.

A lens can be represented by any of a number of [[file:lensmodels.org][lens models]].

A [[file:how-to-calibrate.org][how-to-calibrate-some-cameras]] page describes details about how to accomplish
this very common task.

After running a calibration, the camera models are written to [[file:cameramodels.org][files on disk]].

We can then use these files with a number of [[file:commandline-tools.org][command-line tools]]. In particular,
we can [[file:differencing.org][compare the projection behaviors of different models]]. And we can [[file:uncertainty.org][compute
the projection uncertainties]] of a model.

If we need to do something more than what the pre-made tools can do, there're
two sets programmatic interfaces available:

- [[file:c-api.org][The C API to provide a set of core functionality]]
- [[file:python-api.org][The Python API to do that and a whole lot more]]

* Citing
To cite this work in a publication, use this bibtex stanza:

#+begin_example
@misc{mrcal,
  author = "Dima Kogan",
  title = "mrcal",
  howpublished = "\url{http://mrcal.secretsauce.net}",
}
#+end_example

* Dev communication
For now let's use the [[https://github.com/dkogan/mrcal/issues][github issue tracker]] for bug reporting and for
communication in general. At some point I will probably set up a mailing list as
well.

* Author
Dima Kogan =dima@secretsauce.net=

* License and copyright
Copyright (c) 2017-2020 California Institute of Technology ("Caltech"). U.S.
Government sponsorship acknowledged. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

There's some included external code in the sources, with their own [[file:copyrights.org][copyrights
and licenses]].

* future work                                                      :noexport:
** uncertainty/noise computations
*** Noted in uncertainty.org
- measure observed_pixel_uncertainty
- improve uncertainty method: faraway obervations don't make things worse
- projection_uncertainty() should be able to project multiple points at a time,
  and to report correlations in the projection. Should work with multiple
  cameras somehow (could calibration more than one camera at the same time)
*** Not noted in uncertainty.org
- use uncertainty in triangulation, deltapose, stereo
- can I quantify the heteroscedasticity and thus the model-nonfitting and the
  resulted expected bias? White test?
- do a triangulation with explict uncertainty propagation
- uncertainty math currently does a separate mean-frames for each q we ask
  about. Thus we're effectively computing a different implied tranform each
  time. We should have a single one for ALL q
- regions without chessboards (like in the talk): why do we see high
  uncertainty? That's what I /want/, but I don't think it should be working: the
  spline is dominated by the regularization terms there, so the lens model is
  purely stereographic. Oh... am I seeing /just/ the noise in the chessboard
  pose? I can't rely on that
** splined models
*** noted in lensmodels.org
- splined models should behave more nicely at the edges
- better regularization scheme for the non-splined models. Can I do better than
  L2? Surely I can
- better regularization scheme for the splined models. I should pull not towards
  0 but towards the mean. I had an implementation in
  c8f9918023142d7ee463821661dc5bcc8f770b51 that I reverted because any planar
  splined surface would have "perfect" regularization, and that was breaking
  things (crazy focal lengths would be picked). But now that I'm locking down
  the intrinsics core when optimizing splined models, this isn't a problem anymore

#+begin_example
Notes from sources:

splined regularization should penalize dqx/dvx<0. It should be >0 everywhere.
The splined representation COULD flip that around, however, and I should fight
that. This would make the function non-reversible uniquely, and unproject()
could have trouble

  q = (u + deltau(u)) * f + c
  dqx/dpx ~ (d(ux + deltaux(u))/dpx) =
          = dux/dpx + ddeltaux(u)/du du/dpx
  u = xy / (mag_p + z) * 2, so
  dqx/dpx ~ ((mag_p + z) - x^2/mag_p)/(mag_p + z)^2 +
            ddeltaux(u)/du ((mag_p + z) I - outer(xy,xy)/mag_p)/(mag_p + z)^2
  I care about the sign only, so
  dqx/dpx ~ (mag_p + z) - x^2/mag_p +
#+end_example

- study cubic/quadratic splines, spline density effects
** diff
*** noted in lensmodels.org
- projection_diff(): weighting should be better. Should I do outlier rejection?
  Should I use the hoaky valid-intrinsics region to cut down the fit set? Should
  I optimize actual reprojection error?
** stuff to add
- better sfm support
- integrate deltapose-lite (lindstrom-optimized points) into mrcal
- better outlier rejection. cook's D
- outlier rejection for points AND board observations
** stuff to study
- Redo, show stability. Heat? Show effects?
- Can we study intrinsics stability over time? In response to heating? Shaking?
- Can we use a 3-parallel calibration to quantify chromatic aberration?
- Measure effect of focus, aperture

** warnings in mrcal.c
[[file:~/jpl/mrcal/mrcal.c::// WARNING: if I could assume that dq_dintrinsics_pool_double!=NULL then I wouldnt need to copy the context][something about being efficient and not copying stuff]]

[[file:~/jpl/mrcal/mrcal.c::// WARNING: This should go away. For some reason it makes unproject() converge better, and it makes the tests pass. But it's not even right!][=mrcal_unproject_internal()=]] is seeding the optimization in a 100% wrong way
that, for some reason, works better than if I fix the bug. Fixing the bug makes
the tests fail

[[file:~/jpl/mrcal/mrcal.c::// WARNING: sparsify this. This is potentially a BIG thing on the stack][not putting the full optimization state on the stack]]

[[file:~/jpl/mrcal/mrcal.c::// WARNING: "compute size(dq_dintrinsics_pool_double) correctly and maybe bounds-check"][Again: don't put the full intrinsics on the stack]]

mrcal_optimize(): merge =packed_state= and =p_packed_final=. And =packed_state=
is a big stack thing, which is scary

Hook up the =// optimizer_callback(packed_state, NULL, NULL, &ctx);= calls.
These are supposed to do diagnostics only, or something. Look at what deltapose
is doing.

* todo for the document                                            :noexport:
stereo. Try opencv rectification

something somewhere should describe the optimizer_callback()

Somewhere talk about these:
  - [[file:mrcal-python-api-reference.html#-ingest_packed_state][=mrcal.ingest_packed_state()=]]: Read a given packed state into optimization_inputs
  - [[file:mrcal-python-api-reference.html#-corresponding_icam_extrinsics][=mrcal.corresponding_icam_extrinsics()=]]: Return the icam_extrinsics corresponding to a given icam_intrinsics
    talk about this next to optimization_inputs()

talk about --seed and how that can be used to validate intrinsics

add punchline note at the end of the tour
