#+TITLE: Model differencing


yyy

this is a copy of the thing from the tour. fix it




We just used the same chessboard observations to compute the intrinsics of a
lens in two different ways:

- Using a lean opencv8 lens model
- Using a rich splined-stereographic lens model

And we saw evidence that the splined model will do a better job of representing
reality. Can we quantify that? How different are the two models? Let's compute
it. Given a pixel $\vec q_0$ we can

1. Unproject it to a point $\vec p$ using one lens model
2. Project $\vec p$ back to pixel coords $\vec q_1$ using the /other/ lens model
3. Report the reprojection difference $\vec q_1 - \vec q_0$ as the diff at this
   pixel location

[[./diff-notransform.svg]]

This is a very common thing to want to do, so mrcal provides a tool to do it.
Let's compare the two models:

#+begin_src sh
mrcal-show-projection-diff --radius 0 --cbmax 200 --unset key data/board/opencv8.cameramodel data/board/splined.cameramodel
#+end_src

[[./diff-radius0-heatmap-splined-opencv8.png]]

Well that's strange. The reported differences really do have units of /pixels/.
Are the two models /that/ different? And is the best-aligned area really where
this plot indicates? If we ask for the vector field of differences instead of a
heat map, we get a hint about what's going on:

#+begin_src sh
mrcal-show-projection-diff --radius 0 --cbmax 200 --unset key --vectorfield --vectorscale 5 --gridn 30 20 data/board/opencv8.cameramodel data/board/splined.cameramodel
#+end_src

[[./diff-radius0-vectorfield-splined-opencv8.png]]

This is a /very/ regular pattern. What does it mean?

The answer is rooted in the location and orientation of the camera coordinate
system in respect to the physical lens and camera. The optimization algorithm
only knows where the chessboard corners were observed. It does /not/ know where
anything is, and it has to compute all geometry. And as always, fitted results
differ from reality somewhat. In this case, the origin of the camera coordinate
system and the orientation of this coordinate system are noisy quantities that
will vary from solve to solve. There exists some transformation between the
camera coordinate system from the solution and the coordinate system defined by
the physical lens and camera body. And this transformation is different each
time we run a solve. It is important to note that *this implied transformation
is built-in to the intrinsics*. Even if we're not explicitly optimizing the
camera pose (which is the case with these monocular solves), this implied
transformation is still something that exists and moves around in response to
noise. Rich models like the splined stereographic models are able to encode a
wider range of implied transformations, but even the simplest models have some
transform that must be compensated for.

Looking at the vectorfield above, it looks like we need to move one of the
cameras up and to the left, and then we need to rotate that camera. We can
automate this by adding a critical missing step to the procedure above between
steps 1 and 2:

- Transform $\vec p$ from the coordinate system of one camera to the coordinate
  system of the other camera

[[./diff-yestransform.svg]]

But we don't know anything about the physical coordinate system of either
camera. How can we compute this transformation? We do a fit. The "right"
transformation will transform $\vec v$ in such a way that the reported
mismatches in $\vec q$ will be minimized. There are many details here, but we
don't need to know them to run the tool. Previously we passed =--radius 0= to
bypass the fit. Let's leave out that option to get the usable diff:

#+begin_src sh
mrcal-show-projection-diff --unset key data/board/opencv8.cameramodel data/board/splined.cameramodel
#+end_src

[[./diff-splined-opencv8.png]]

/Much/ better. As expected, the two models agree relatively well in the center,
and the error grows as we move towards the edges. If we used a leaner model,
such as opencv4, this effect would be more pronounced. Do note that since we do
a fit, there's some ambiguity in the details. We choose where we get the data
for our implied transformation fit. We can decide that we really care about low
differences right at the center at the expense of a worse fit further out, or we
can decide that a reasonable-but-not-great fit is acceptable as long as it
covers a wide area. For instance, focusing at the center gives us a lower diff
there:

#+begin_src sh
mrcal-show-projection-diff --radius 500 --unset key data/board/opencv8.cameramodel data/board/splined.cameramodel
#+end_src

[[./diff-radius500-splined-opencv8.png]]

Both are valid.

This differencing method is very powerful. We can use it to, for instance

- evaluate the variability of different lenses
- quantify intrinsics drift due to mechanical or thermal stresses, or anything else
- test different solution methods
- as the core of a cross-validation scheme

Many of these analyses immediately raise a question: how much of a difference do
I expect to get from random noise, and how much is attributable to whatever I'm
measuring?

Furthermore, how do we decide which data to use for the fit of the implied
transformation? Here I was careful to get chessboard images everywhere in the
imager, but what if there was occlusion in the space, so I was only able to get
images in one area? In this case we would want to use only the data in that area
for the fit of the implied transformation (because we won't expect the data in
other areas to fit). But what to do if we don't know where that area is?

These questions can be answered conclusively by quantifying the projection
uncertainty, so let's talk about that now.

